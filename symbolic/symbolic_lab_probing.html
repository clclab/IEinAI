
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Investigating how Transformers learn propositional logic &#8212; Interpretability &amp; Explainability in AI — Workshop materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'symbolic/symbolic_lab_probing';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Interpretability & Explainability in AI — Workshop materials - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Interpretability & Explainability in AI — Workshop materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to the IEinAI Workshop Materials
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Workshop Materials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../week1/week1_intro.html">Week 1</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../week1/w1a_IEinAi_probing_student_version.html">1.1: Introduction to Posthoc Interpretability</a></li>


<li class="toctree-l2"><a class="reference internal" href="../week1/w1b_IEinAI_attribution_student_version.html">1.2: Feature Attribution for Language Models — Approaches and Evaluation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mechinterp/mechinterp_intro.html">Mechanistic Interpretability</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../speech-perception/speech-perception_intro.html">Speech Perception</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../speech-perception/speech_perception_lab1_probing.html">Workshop on Speech Perception, Part 1: Probing acoustic, phonemic and orthographic information in Wav2Vec2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../speech-perception/speech_perception_lab2_CKA.html">Workshop on Speech Perception, Part 2: Comparing Audio Transformer representations with Centered Kernel Alignment</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/clclab/IEinAI/blob/main/book/symbolic/symbolic_lab_probing.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/clclab/IEinAI" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/clclab/IEinAI/issues/new?title=Issue%20on%20page%20%2Fsymbolic/symbolic_lab_probing.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/symbolic/symbolic_lab_probing.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Investigating how Transformers learn propositional logic</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tothink-dataset-creation">🧠 ToThink: dataset creation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-dataset">Load dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-inspection-of-the-data">First inspection of the data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pen-and-paper-1">📝 Pen and Paper 1</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-model">Load the model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterate-over-data-to-get-hidden-states-or-load-them-from-a-file">Iterate over data to get hidden states (or load them from a file)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collect-information-about-the-dataset-and-model-outputs">Collect information about the dataset and model outputs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pen-and-paper-2">📝 Pen and Paper 2</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probing-for-truth">Probing for Truth</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probe-task-construction">Probe task construction</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-a-probetask-object">Example of a probetask object</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probe-training">Probe training</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-1-implement-the-training-loop-of-the-probe">☑️ ToDo 1: Implement the training loop of the probe</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-variable-probes-per-layer">Training variable probes per layer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-2-probe-variable-nodes-for-all-layers">☑️ ToDo 2 : probe variable nodes for all layers.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tosubmit-1-visualize-your-probing-results-per-layer">✍️ ToSubmit 1: Visualize your probing results per layer.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tothink">🧠 ToThink:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probing-other-types-of-nodes">Probing other types of nodes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-3-run-probes-for-non-variable-operator-nodes">☑️  ToDo 3: Run probes for non-variable/operator nodes.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tosubmit-2-add-probing-for-your-operator-node-probe-to-your-visualization-for-tosubmit-1">✍️ ToSubmit 2: Add probing for your operator-node probe to your visualization for ToSubmit 1.</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-probes-generalize">Do probes generalize?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-4-train-probes-on-one-subset-of-node-types-then-evaluate-them-on-another-subset-as-well-as-the-original-test-set">☑️ ToDo 4: train probes on one subset of node_types, then evaluate them on another subset as well as the original test set.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tosubmit-3-include-your-findings-in-the-report">✍️ ToSubmit 3: Include your findings in the report.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logical-equivalences-through-the-layers">Logical equivalences through the layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-our-own-small-dataset">Creating our own small dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-5-add-at-least-4-more-inputs-to-the-mini-dataset-above-and-compare-the-similarity-matrix-through-the-layers">☑️ ToDo 5: Add at least 4 more inputs to the mini-dataset above and compare the similarity matrix through the layers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">🧠 ToThink:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tosubmit-4-include-the-plot-above-with-a-subset-of-your-non-equivalent-sentences">✍️ ToSubmit 4: Include the plot above with (a subset of your) (non)-equivalent sentences.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#end-of-notebook">✅ <font color="green"> End of notebook! </font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-models-trained-on-systematically-different-data">Optional: Models trained on systematically different data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-questions-ideas-for-projects">Future questions / Ideas for projects</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="investigating-how-transformers-learn-propositional-logic">
<h1>Investigating how Transformers learn propositional logic<a class="headerlink" href="#investigating-how-transformers-learn-propositional-logic" title="Link to this heading">#</a></h1>
<p>Author: Anna Langedijk</p>
<p>With credits to: Jaap Jumelet, Jelle Zuidema</p>
<p>Part of the Logic &amp; Deep Learning workshop for the course Interpretability &amp; Explainability in AI 2023.</p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Link to this heading">#</a></h2>
<p>There are several ways researchers have combined the ultimately symbolic domain of Logic with that of neural networks. Neurosymbolic models are one research direction. For instance, in the Logic Tensor Networks paper you have seen one way of incorporating logic.  Another example is <a class="reference external" href="https://arxiv.org/abs/1802.03685">NeuroSAT</a>: an end-to-end SAT solver that, instead of adding logic explicitly, uses a graph neural network to represent its logical inputs. Input formulas are always in conjunctive normal form (CNF): relevant clauses and literals are connected by edges in the input graph to the network.</p>
<p>However, many models used today are trained end-to-end on flat input strings. They must rely on their training data to learn any type of logic and reasoning. These models have no explicit reasoning abilities, and yet they display behaviour that requires them to perform some form of reasoning, such as Question Answering and Natural Language Understanding. The question arises whether it is even possible to learn the rules of logic from data alone.</p>
<p>To test the reasoning abilities of certain Transformer models, or Transformers/neural models in general, there exist several benchmarks. In the domain of natural language, there exist several Natural Language Inference (<a class="reference external" href="https://aclanthology.org/S14-2001/">NLI</a>) datasets. These datasets usually focus on classification tasks, for instance: Given two sentences, do these sentences entail one another or not? In the formal domain, similar datasets exist (e.g. <a class="reference external" href="https://www.deepmind.com/publications/can-neural-networks-understand-logical-entailment">this paper</a>), where the task is still to predict (non)entailment, but inputs and outputs are instead given in a formal logical form.</p>
<p>A recent paper <a class="reference external" href="https://arxiv.org/abs/2003.04218">Teaching Temporal Logic to Neural Networks</a> by Hahn et al. instead explores a different objective: that of generating correct solutions given an input formula in some logic. They do this using a generic encoder-decoder setup. Instead of a specific binary output, the model is trained to generate “explanations” (in some sense) for the formula, namely a possible world/trace for the input. Their main experiments focus on generating traces for Temporal Logic formulas, but in a second experiment the authors claim that these encoder-decoder models can also learn the semantics of Propositional Logic.</p>
<p>In this notebook we will focus on the models trained on propositional logic trained on input-output pairs generated by a symbolic solver.
An example of such an input-output pair is:</p>
<p>In: <code class="docutils literal notranslate"><span class="pre">(a</span> <span class="pre">xor</span> <span class="pre">b)</span> <span class="pre">&amp;</span> <span class="pre">c</span></code></p>
<p>Out: <code class="docutils literal notranslate"><span class="pre">a=False,</span> <span class="pre">b=True,</span> <span class="pre">c=True</span></code></p>
<p>A model trained on 800000 of such pairs can achieve a high (93%) accuracy in predicting logically valid assignments, not only emulating the outputs provided by the symbolic solver, but also generating alternative valid possible worlds/assignments.</p>
<p>But, while the task itself is interpretable (it is unambiguous, and there are no hidden assumptions as is the case with many tasks based on natural data), it is still a challenge to understand <em>how</em> the model solves this task, and if it truly has knowledge about the semantics of logic or if it is exploiting irrelevant patterns in the data.</p>
<p>In this notebook, you will probe the hidden states of the encoder of this model for propositional logical ‘truth’, to see what kind of information is encoded when the encoder processes a sentence, and whether that information is encoded the same way for different types of tokens through its layers.</p>
<section id="tothink-dataset-creation">
<h3>🧠 ToThink: dataset creation<a class="headerlink" href="#tothink-dataset-creation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What are other ways this dataset differs from classification tasks? Can you see downsides to this dataset?</p></li>
</ul>
</section>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<p>First, import some prerequisites.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">confusion_matrix</span>
</pre></div>
</div>
</div>
</div>
<p>GPU shouldn’t be necessary, although it will be faster if you want to (re)calculate the hidden states.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">COLAB</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">INSTALL</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">COLAB</span><span class="p">:</span>
    <span class="c1"># Mount to drive so we can access our own files</span>
    <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
    <span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">INSTALL</span><span class="p">:</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>--user<span class="w"> </span>torch<span class="w"> </span>sklearn<span class="w"> </span>matplotlib<span class="w"> </span>seaborn<span class="w"> </span>pandas
</pre></div>
</div>
</div>
</div>
<p>Download and import the code that can load the dataset and model architecture.
The model implementation is available here: <a class="reference external" href="https://github.com/annaproxy/transformer_logic_compact">github.com/annaproxy/transformer_logic_compact</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">COLAB</span><span class="p">:</span>
    <span class="c1"># Point this to the correct location</span>
    <span class="o">%</span><span class="k">cd</span> /content/drive/My Drive/IEAI_Notebooks
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&quot;transformer_logic_compact&quot;</span><span class="p">):</span>
        <span class="o">%</span><span class="k">cd</span> transformer_logic_compact
        <span class="o">!</span>git<span class="w"> </span>pull<span class="w"> </span>https://github.com/annaproxy/transformer_logic_compact
        <span class="o">%</span><span class="k">cd</span> ..
    <span class="k">else</span><span class="p">:</span>
        <span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/annaproxy/transformer_logic_compact
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;transformer_logic_compact&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">interfaces</span> <span class="kn">import</span> <span class="n">ModelInterface</span><span class="p">,</span> <span class="n">TransformerEncDecInterface</span>
<span class="kn">from</span> <span class="nn">models</span> <span class="kn">import</span> <span class="n">TransformerEncDecModel</span>
<span class="kn">from</span> <span class="nn">layers.transformer</span> <span class="kn">import</span> <span class="n">Transformer</span>
<span class="kn">from</span> <span class="nn">helpers.collater</span> <span class="kn">import</span> <span class="n">VarLengthCollate</span>
<span class="kn">from</span> <span class="nn">logicdatasets</span> <span class="kn">import</span> <span class="n">LogicDataSet</span>
<span class="kn">from</span> <span class="nn">helpers</span> <span class="kn">import</span> <span class="n">move_to_device</span>
</pre></div>
</div>
</div>
</div>
<p>Download the pre-provided data (validation set, information about its subtrees, pretrained models) and put it in the data directory:
<a class="reference external" href="https://drive.google.com/drive/folders/1TE0loQ76bcBoYKxXDjUsyfCWiPsBHSv9">https://drive.google.com/drive/folders/1TE0loQ76bcBoYKxXDjUsyfCWiPsBHSv9</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">DATA_FOLDER</span> <span class="o">=</span> <span class="s1">&#39;transformer_logic_compact/data&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">COLAB</span><span class="p">:</span>
    <span class="o">!</span>gdown<span class="w"> </span>--folder<span class="w"> </span>1TE0loQ76bcBoYKxXDjUsyfCWiPsBHSv9<span class="w"> </span>-O<span class="w"> </span><span class="nv">$DATA_FOLDER</span>
</pre></div>
</div>
</div>
</div>
<section id="load-dataset">
<h3>Load dataset<a class="headerlink" href="#load-dataset" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The files val.src and val.tgt will be loaded</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">LogicDataSet</span><span class="p">(</span><span class="n">data_path</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">DATA_FOLDER</span><span class="si">}</span><span class="s1">/val&#39;</span><span class="p">,</span> <span class="n">vocab_path</span><span class="o">=</span><span class="s1">&#39;transformer_logic_compact/vocabulary&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="first-inspection-of-the-data">
<h3>First inspection of the data<a class="headerlink" href="#first-inspection-of-the-data" title="Link to this heading">#</a></h3>
<p>The input data is provided to the model in <a class="reference external" href="https://en.wikipedia.org/wiki/Polish_notation">Polish prefix notation</a>. Instead of writing operators with infix, e.g. <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">|</span> <span class="pre">b</span></code>, the operator is provided as a prefix: <code class="docutils literal notranslate"><span class="pre">|</span> <span class="pre">a</span> <span class="pre">b</span></code>. This avoids the use of parentheses, which would only add to the length of the input of the model.</p>
<p>The following input means <code class="docutils literal notranslate"><span class="pre">(b</span> <span class="pre">|</span> <span class="pre">d)</span> <span class="pre">xor</span> <span class="pre">(c)</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;String input:&quot;</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">input_ids_to_text</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">22623</span><span class="p">][</span><span class="s1">&#39;in&#39;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p>The outputs are provided as a simple array of maximum length 10, for example: <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">0</span> <span class="pre">b</span> <span class="pre">1</span> <span class="pre">c</span> <span class="pre">1</span> <span class="pre">d</span> <span class="pre">1</span> <span class="pre">e</span> <span class="pre">0</span></code> corresponds to the model {a=False, b=True, c=True,d=True, e=False}. The outputs are always in alphabetical order.</p>
<section id="pen-and-paper-1">
<h4>📝 Pen and Paper 1<a class="headerlink" href="#pen-and-paper-1" title="Link to this heading">#</a></h4>
<p>Think of a possible valid assignment to this sentence. Then, use the ‘out’ key of the dataset object to print the gold label. Did you generate the same output with your internal logic model, or are there multiple possible outputs?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;String output:&quot;</span><span class="p">,</span> <span class="s2">&quot;Todo: dataset output goes here.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="load-the-model">
<h3>Load the model<a class="headerlink" href="#load-the-model" title="Link to this heading">#</a></h3>
<p>The Transformers that we use are relatively small: the encoder and decoder have 6 layers. The state sizes of the encoder and decoder are 128 and 64, respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For now, we work with one model seed</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;model_seed2&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_model_from_name</span><span class="p">(</span><span class="n">model_name</span><span class="p">):</span>
    <span class="c1"># Initialize model architecture</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncDecModel</span><span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">in_vocabulary</span><span class="p">),</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">out_vocabulary</span><span class="p">),</span>
                <span class="n">state_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                <span class="n">state_size_decoder</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                <span class="n">nhead</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
                <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
                <span class="n">ff_multiplier</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                <span class="n">ff_multiplier_decoder</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                <span class="n">transformer</span><span class="o">=</span><span class="n">Transformer</span><span class="p">,</span>
                <span class="n">tied_embedding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">scale_mode</span><span class="o">=</span><span class="s2">&quot;opennmt&quot;</span>
            <span class="p">)</span>
    <span class="c1"># Load from state dict</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">DATA_FOLDER</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">.pth&quot;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

    <span class="c1"># Model interface helps with some overhead such as adding &lt;BOS&gt; and &lt;EOS&gt;.</span>
    <span class="c1"># You should call this if you want to do a forward pass.</span>
    <span class="n">model_interface</span> <span class="o">=</span> <span class="n">TransformerEncDecInterface</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model_interface</span>
<span class="n">model_interface</span> <span class="o">=</span> <span class="n">load_model_from_name</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uncomment to inspect the model architecture in more detail</span>
<span class="c1">#print(model)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="iterate-over-data-to-get-hidden-states-or-load-them-from-a-file">
<h3>Iterate over data to get hidden states (or load them from a file)<a class="headerlink" href="#iterate-over-data-to-get-hidden-states-or-load-them-from-a-file" title="Link to this heading">#</a></h3>
<p>I’ve provided a portion (~26%) of the hidden states in a separate drive: <a class="reference external" href="https://drive.google.com/drive/folders/1oSmpcYy_zi-bNA81ODf001-M80B44FMc">https://drive.google.com/drive/folders/1oSmpcYy_zi-bNA81ODf001-M80B44FMc</a>
This should be enough to run your probing experiment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="n">VarLengthCollate</span><span class="p">(</span><span class="n">batch_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_all</span><span class="p">(</span><span class="n">model_interface</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">max_it</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">calculate_outputs</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
               <span class="n">calculate_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forwards max_it batches through the model_interface, collecting its hidden states.&quot;&quot;&quot;</span>
    <span class="n">stacked_hidden</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># The dataset has size 100_000 entries, when not setting MAX_IT, these tensors will take up about 7G (!)</span>
    <span class="c1"># It may take a while to run (3-6 minutes)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">it</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">max_it</span><span class="p">)):</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">move_to_device</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">)</span>
            <span class="n">result</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">model_interface</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">collect_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">current_batch_size</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;in&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">calculate_outputs</span><span class="p">:</span>
                <span class="n">digits</span> <span class="o">=</span> <span class="n">model_interface</span><span class="o">.</span><span class="n">decode_outputs</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
                <span class="n">output_strs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset</span><span class="o">.</span><span class="n">sample_to_text</span><span class="p">(</span><span class="n">digits</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">current_batch_size</span><span class="p">)]</span>
                <span class="n">outputs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">output_strs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">calculate_hidden</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">hidden</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">layer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stacked_hidden</span><span class="p">:</span>
                        <span class="n">stacked_hidden</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># This is a bit slow, .cat creates a new tensor</span>
                        <span class="c1"># Also fun: batch size must be large enough to include a sentence of the max length every time</span>
                        <span class="n">stacked_hidden</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">stacked_hidden</span><span class="p">[</span><span class="n">layer</span><span class="p">],</span> <span class="n">hidden</span><span class="p">[</span><span class="n">layer</span><span class="p">]],</span> <span class="n">dim</span> <span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">it</span> <span class="o">&gt;</span> <span class="n">max_it</span><span class="p">:</span>
                <span class="k">break</span>
    <span class="k">return</span> <span class="n">stacked_hidden</span><span class="p">,</span> <span class="n">outputs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_hidden_and_outputs</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_interface</span><span class="p">,</span> <span class="n">load_from_file</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">calculate_outputs</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">MAX_IT</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                           <span class="n">calculate_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">stacked_hidden</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">load_from_file</span><span class="p">:</span>
        <span class="n">stacked_hidden</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">forward_all</span><span class="p">(</span><span class="n">model_interface</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">max_it</span><span class="o">=</span><span class="n">MAX_IT</span><span class="p">,</span> <span class="n">calculate_outputs</span><span class="o">=</span><span class="n">calculate_outputs</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">stacked_hidden</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">DATA_FOLDER</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s1">_hiddenstates.pth&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">calculate_outputs</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">DATA_FOLDER</span><span class="si">}</span><span class="s1">/outputs_</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s1">.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="o">+</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">calculate_hidden</span><span class="p">:</span>
            <span class="n">file_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">DATA_FOLDER</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s1">_hiddenstates.pth&#39;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
                <span class="c1"># Ugly, but don&#39;t want to download the whole folder and blast you with 10GB of hidden states</span>
                <span class="c1"># gdown doesnt provide an option to pass file_path within a folder</span>
                <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s1">&#39;model_seed2&#39;</span><span class="p">:</span>
                    <span class="o">!</span>gdown<span class="w"> </span>1Ln-mIQ1YVQpHhePqRBXdgyv9w-tzldy7<span class="w"> </span>-O<span class="w"> </span><span class="nv">$file_path</span>
                <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s1">&#39;model_without_not_xor_seed2&#39;</span><span class="p">:</span>
                    <span class="o">!</span>gdown<span class="w"> </span>1l-pmncpeJ0jNdNhJHLoMR1wj_IHJXbGH<span class="w"> </span>-O<span class="w"> </span><span class="nv">$file_path</span>
                <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s1">&#39;model_without_and_xor_seed1&#39;</span><span class="p">:</span>
                    <span class="o">!</span>gdown<span class="w"> </span>1dQjHw6eZEAgsIV2wB1-BQHdLeMOIwAoj<span class="w"> </span>-O<span class="w"> </span><span class="nv">$file_path</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No file found for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">stacked_hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">DATA_FOLDER</span><span class="si">}</span><span class="s1">/outputs_</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s1">.txt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>
    <span class="k">return</span> <span class="n">stacked_hidden</span><span class="p">,</span> <span class="n">outputs</span>

<span class="c1"># Set load_from_file to false if you want to calculate new states</span>
<span class="c1"># Downloading may take a while (~3gb)</span>
<span class="n">stacked_hidden</span><span class="p">,</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">load_hidden_and_outputs</span><span class="p">(</span><span class="s1">&#39;model_seed2&#39;</span><span class="p">,</span> <span class="n">model_interface</span><span class="p">,</span> <span class="n">load_from_file</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stacked hidden keys&quot;</span><span class="p">,</span><span class="n">stacked_hidden</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stacked hidden shape for layer 3&quot;</span><span class="p">,</span><span class="n">stacked_hidden</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># N x max_sentence_length x hidden_state_size</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="collect-information-about-the-dataset-and-model-outputs">
<h3>Collect information about the dataset and model outputs<a class="headerlink" href="#collect-information-about-the-dataset-and-model-outputs" title="Link to this heading">#</a></h3>
<p>For the 100000 sentences in this validation set, I have calculated some additional information:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">possible_worlds.txt</span></code> contains all the (partial) possible worlds in which a sentence can be true. This allows us to calculate whether any model prediction was correct, not just when the model output exactly matches the gold label.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">subformulas_valuation.txt</span></code> contains information about whether different tokens in the logical formula can be true/can be false. For instance, if the variable <code class="docutils literal notranslate"><span class="pre">a</span></code> is set to <code class="docutils literal notranslate"><span class="pre">1</span></code> in every possible world, it must always be true, and thus has the label <code class="docutils literal notranslate"><span class="pre">1</span></code>. If it is always false, it will have the label <code class="docutils literal notranslate"><span class="pre">0</span></code>. If it depends on the truth value of the rest of sentence (as is usually the case), it is set to <code class="docutils literal notranslate"><span class="pre">2</span></code>. You can find a more concrete example below.</p></li>
</ul>
<p>Let’s load this information into a pandas dataframe.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_df</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">,</span> <span class="n">cutoff</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">DATA_FOLDER</span><span class="si">}</span><span class="s1">/possible_worlds.txt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">possible_worlds</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">DATA_FOLDER</span><span class="si">}</span><span class="s1">/subformulas_valuation.txt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">valuations</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;sentence_idxs&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="n">CUTOFF</span><span class="p">),</span>
        <span class="s1">&#39;inputs&#39;</span><span class="p">:[</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">in_lines</span><span class="p">[:</span><span class="n">cutoff</span><span class="p">]],</span>
        <span class="s1">&#39;gold_outputs&#39;</span><span class="p">:[</span><span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">out_lines</span><span class="p">[:</span><span class="n">cutoff</span><span class="p">]],</span>
        <span class="s1">&#39;model_outputs&#39;</span><span class="p">:</span><span class="n">model_outputs</span><span class="p">[:</span><span class="n">cutoff</span><span class="p">],</span>
        <span class="s1">&#39;possible_worlds&#39;</span><span class="p">:</span><span class="n">possible_worlds</span><span class="p">[:</span><span class="n">cutoff</span><span class="p">],</span>
        <span class="s1">&#39;subformula_valuations&#39;</span><span class="p">:</span><span class="n">valuations</span><span class="p">[:</span><span class="n">cutoff</span><span class="p">]</span>  
    <span class="p">})</span>

    <span class="n">df</span><span class="o">.</span><span class="n">possible_worlds</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">possible_worlds</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">))</span>
    <span class="c1"># An exact match is when the model output is exactly equal to the gold outputs</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;exact_match&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;model_outputs&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;gold_outputs&#39;</span><span class="p">]</span>
    <span class="c1"># The model can be correct without exactly matching the gold outputs</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;semantically_correct&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;model_outputs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="ow">in</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;possible_worlds&#39;</span><span class="p">]</span> <span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Handy statistic so we can look at short formulas as examples</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;input_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span>
    <span class="k">return</span> <span class="n">df</span>

<span class="c1"># How many hidden states we have saved</span>
<span class="n">CUTOFF</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stacked_hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">calculate_df</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">CUTOFF</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can verify that the model has a high accuracy on the task, even when it does not exactly match the generated ‘gold’ output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;exact_match&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;semantically_correct&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Here are some examples of correct outputs that are not an exact match of the gold label, but where it still outputs a valid possible world.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="o">.</span><span class="n">exact_match</span> <span class="o">&amp;</span> <span class="n">df</span><span class="o">.</span><span class="n">semantically_correct</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;input_size&#39;</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The two columns that we added contain information about all possible outputs for the input sentence.
The <code class="docutils literal notranslate"><span class="pre">possible_worlds</span></code> column contains all possible (possibly partial) worlds for this sentence.
The <code class="docutils literal notranslate"><span class="pre">subformula_valuations</span></code> column contains information about which subtrees can or cannot be true in the sentence. The string has the same length as the input sentence.</p>
<p>When a subformula must always be true, it is mapped to <code class="docutils literal notranslate"><span class="pre">1</span></code>. When a subformula must always be false, it is mapped to <code class="docutils literal notranslate"><span class="pre">0</span></code>. Otherwise, it is mapped to <code class="docutils literal notranslate"><span class="pre">2</span></code>. Most subformulas have the value <code class="docutils literal notranslate"><span class="pre">2</span></code>, their truth values are contingent on the truth values of other subformulas.</p>
<p>Take for instance the sentence:
<code class="docutils literal notranslate"><span class="pre">|</span> <span class="pre">d</span> <span class="pre">&amp;</span> <span class="pre">a</span> <span class="pre">d</span></code> (standard notation: <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">|</span> <span class="pre">(a</span> <span class="pre">&amp;</span> <span class="pre">d)</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">15113</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The partial output here is <code class="docutils literal notranslate"><span class="pre">d</span> <span class="pre">1</span></code>, but the model would have also been correct if it had outputted <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">0</span> <span class="pre">d</span> <span class="pre">1</span></code>, since that is also a possible world.</p>
<p>The first subtree of the sentence is the root, so this will always be <code class="docutils literal notranslate"><span class="pre">1</span></code>
For instance, <code class="docutils literal notranslate"><span class="pre">d</span></code> must always be true, which is why the second and last character in <code class="docutils literal notranslate"><span class="pre">subformula_valuations</span></code> are also <code class="docutils literal notranslate"><span class="pre">1</span></code>.
Since the value of <code class="docutils literal notranslate"><span class="pre">a</span></code> can be either true or false (verify this by looking at the possible worlds), its <code class="docutils literal notranslate"><span class="pre">subformula_valuation</span></code> is <code class="docutils literal notranslate"><span class="pre">2</span></code>.
This is also the case for <code class="docutils literal notranslate"><span class="pre">&amp;</span></code>, since we can choose to make either <code class="docutils literal notranslate"><span class="pre">d</span></code> true, or <code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">a</span> <span class="pre">d</span></code>.</p>
<section id="pen-and-paper-2">
<h4>📝 Pen and Paper 2<a class="headerlink" href="#pen-and-paper-2" title="Link to this heading">#</a></h4>
<p>Take the following sentence. It has two possible worlds: calculate the possible worlds.
Then, calculate what the <code class="docutils literal notranslate"><span class="pre">subformula_valuations</span></code> column looks like for this sentence. Which nodes in the tree should always be true, and which should always be false? Check your answers by printing the entire row.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">15612</span><span class="p">][[</span><span class="s1">&#39;inputs&#39;</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p>Since the model is forced to output just one world, it would be interesting to see if there is some information in the model about all these subtrees: has the model internalized when a subtree must be true or false, helping it along to a correct answer?</p>
</section>
</section>
</section>
<section id="probing-for-truth">
<h2>Probing for Truth<a class="headerlink" href="#probing-for-truth" title="Link to this heading">#</a></h2>
<section id="probe-task-construction">
<h3>Probe task construction<a class="headerlink" href="#probe-task-construction" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">ProbeTask</span></code> class will help you with extracting the correct hidden states and labels. You don’t have to fully understand the code. There is an example of how to use this code below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="c1"># Pandas also uses np random state by default</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># if you are using GPU</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>


<span class="k">class</span> <span class="nc">ProbeDataSet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">ys</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ys</span> <span class="o">=</span> <span class="n">ys</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ys</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ys</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

    
<span class="k">class</span> <span class="nc">ProbeTask</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize a &#39;ProbeTask&#39; object.</span>
<span class="sd">        hidden: a dictionary of tensors of shape N x max_input_sentence_length x hidden_size.</span>
<span class="sd">                The dictionary key is the layer in the encoder.</span>
<span class="sd">        df: A pandas dataframe of N (or less) rows with relevant information about. </span>
<span class="sd">            This will be used for sampling and for looking up indices of states in `hidden`.</span>
<span class="sd">            Thus, the values in column `sentence_idxs` should correspond to the indices in the `hidden` obejct.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">create_datapoints_from_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_no</span><span class="p">,</span> <span class="n">sentence_idx</span><span class="p">,</span> <span class="n">word_idxs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform a sentence into 1 or more datapoints. </span>
<span class="sd">        layer_no: int</span>
<span class="sd">        sentence_idx: int</span>
<span class="sd">        word_idxs: [int] 0-based array of word indices. The BOS tag will be taken care of by self.get_hidden.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">flat_idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hidden</span><span class="p">(</span><span class="n">layer_no</span><span class="p">,</span> <span class="p">[(</span><span class="n">sentence_idx</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">word_idxs</span><span class="p">))])</span>
        <span class="k">return</span> <span class="n">state</span>
        
    <span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">layer_no</span><span class="p">,</span>
        <span class="n">node_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span><span class="s2">&quot;b&quot;</span><span class="p">,</span><span class="s2">&quot;c&quot;</span><span class="p">,</span><span class="s2">&quot;d&quot;</span><span class="p">,</span><span class="s2">&quot;e&quot;</span><span class="p">],</span>
        <span class="n">output_types</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">sents</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
        <span class="n">max_tokens_per_sent</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a dataset given the subformula_valuations column.</span>
<span class="sd">        The dataset is balanced w.r.t. both the node types and the possible &#39;truth values&#39; 0, 1 and 2.</span>
<span class="sd">        </span>
<span class="sd">        layer_no: Which layer to sample the hidden states from</span>
<span class="sd">        node_types: Which node types to sample. For instance: all variables. It can also be a singleton list, eg [&quot;xor&quot;].</span>
<span class="sd">        sents: amount of sentences to sample per node_type per valuation.</span>
<span class="sd">        max_tokens_per_sent: how many nodes we are allowed to take from the same sentence. Allowing this makes your dataset bigger.</span>
<span class="sd">        </span>
<span class="sd">        Returns: a probedataset of size max (sents * max_tokens_per_sent * len(node_types) * 3)</span>
<span class="sd">        In practice, it is shorter, since not all sentences contain max_tokens_per_sent relevant tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">idxs_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">node_type</span> <span class="ow">in</span> <span class="n">node_types</span><span class="p">:</span>
            <span class="c1"># Iterate over all possible </span>
            <span class="k">for</span> <span class="n">val_id</span> <span class="ow">in</span> <span class="n">output_types</span><span class="p">:</span>
                <span class="c1"># Be sure to balance node types when a list is passed</span>
                <span class="n">idxs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_all_idx</span><span class="p">(</span>
                    <span class="n">sents</span><span class="p">,</span> <span class="n">max_tokens_per_sent</span><span class="p">,</span> <span class="p">[</span><span class="n">node_type</span><span class="p">],</span> <span class="nb">str</span><span class="p">(</span><span class="n">val_id</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">states</span><span class="p">,</span> <span class="n">flat_idxs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hidden</span><span class="p">(</span><span class="n">layer_no</span><span class="p">,</span> <span class="n">idxs</span><span class="p">)</span>
                <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
                <span class="n">ys</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">val_id</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">states</span><span class="p">))</span>
                <span class="n">idxs_</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">flat_idxs</span><span class="p">)</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ProbeDataSet</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">),</span> <span class="n">idxs_</span>
    
    <span class="k">def</span> <span class="nf">get_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_no</span><span class="p">,</span> <span class="n">idx_list</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        idx_list is a list of (int, [int]) indicating the sentence position and the word position array</span>
<span class="sd">        This function takes care of the BOS token by adding + 1 to the provided idxs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">[</span><span class="n">layer_no</span><span class="p">]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">idxs_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word_idxs</span> <span class="ow">in</span> <span class="n">idx_list</span><span class="p">:</span>
            <span class="c1"># +1 for BOS tag</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">states</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">word_idxs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">idxs_</span><span class="o">.</span><span class="n">extend</span><span class="p">([(</span><span class="n">idx</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">word_idx</span> <span class="ow">in</span> <span class="n">word_idxs</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">idxs_</span>
    
    
    <span class="k">def</span> <span class="nf">get_all_idx</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sent_amt</span><span class="p">,</span> <span class="c1"># How many sentences to sample</span>
        <span class="n">per_sent_max_amt</span><span class="p">,</span> <span class="c1"># How many nodes are allowed in the same sentence</span>
        <span class="n">token_identities</span><span class="p">,</span>  <span class="c1"># abcde xor ! &amp; , etc...</span>
        <span class="n">value</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>  <span class="c1"># &#39;0&#39;, &#39;1&#39; or &#39;2&#39;</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This method uses the subformula_truths column to fetch relevant pairs of (sentence_idx, word_idx).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Filter rows where a token of one of the provided token_identities has the desired value</span>
        <span class="c1"># E.g. get only those indices where there exists some `a` with value `1`.</span>
        <span class="c1"># Since the root node token *always* has value 1, we start at the second token.</span>
        <span class="k">def</span> <span class="nf">get_idxs</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">idxs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">truth</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="n">start</span><span class="p">:],</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;subformula_valuations&quot;</span><span class="p">][</span><span class="n">start</span><span class="p">:])</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="n">truth</span> <span class="o">==</span> <span class="n">value</span> <span class="ow">and</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_identities</span><span class="p">:</span>
                    <span class="n">idxs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">start</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">idxs</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;_condition&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">get_idxs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;_condition&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">len</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Sample from all possible sentences</span>
        <span class="n">sents</span> <span class="o">=</span> <span class="n">sents</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="n">sent_amt</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sents</span><span class="p">)),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># For each sentence, sample from all possible positions in the sentence</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">sents</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
            <span class="n">sent_idx</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;sentence_idxs&quot;</span><span class="p">]</span>
            <span class="n">word_idxs</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">&quot;_condition&quot;</span><span class="p">]</span>
            <span class="n">word_idxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
                <span class="n">word_idxs</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="n">per_sent_max_amt</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_idxs</span><span class="p">)),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">sent_idx</span><span class="p">,</span> <span class="n">word_idxs</span><span class="p">))</span>
        <span class="c1"># Remove column just in case</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;_condition&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>
<span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tool for printing the label distribution&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="example-of-a-probetask-object">
<h4>Example of a probetask object<a class="headerlink" href="#example-of-a-probetask-object" title="Link to this heading">#</a></h4>
<p>The probetask <code class="docutils literal notranslate"><span class="pre">create_dataset</span></code> method samples from the subset of hidden states that are above tokens of a certain type. You can specify which types of nodes to collect with the <code class="docutils literal notranslate"><span class="pre">node_types</span></code> parameter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a probetask object to sample from</span>
<span class="n">probetask_test</span> <span class="o">=</span> <span class="n">ProbeTask</span><span class="p">(</span><span class="n">stacked_hidden</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sample hidden states from layer 3 for 100 sentences per node_type per output_type</span>
<span class="c1"># This may take a few seconds</span>
<span class="n">probe_dataset_test</span><span class="p">,</span> <span class="n">idxs</span> <span class="o">=</span> <span class="n">probetask_test</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">(</span><span class="n">layer_no</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sents</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">node_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span><span class="s2">&quot;b&quot;</span><span class="p">,</span><span class="s2">&quot;c&quot;</span><span class="p">,</span><span class="s2">&quot;d&quot;</span><span class="p">,</span><span class="s2">&quot;e&quot;</span><span class="p">],</span>
                                                        <span class="n">output_types</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">max_tokens_per_sent</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the output and verify it using the dataframe</span>
<span class="n">hidden_state</span><span class="p">,</span> <span class="n">prediction_label</span> <span class="o">=</span> <span class="n">probe_dataset_test</span><span class="o">.</span><span class="n">hidden</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">probe_dataset_test</span><span class="o">.</span><span class="n">ys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sentence_idx</span><span class="p">,</span> <span class="n">word_idx</span> <span class="o">=</span> <span class="n">idxs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dataset size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">probe_dataset_test</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hidden state size&quot;</span><span class="p">,</span> <span class="n">probe_dataset_test</span><span class="o">.</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sentence idx&quot;</span><span class="p">,</span> <span class="n">sentence_idx</span><span class="p">,</span> <span class="s2">&quot;word idx&quot;</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Token at position?&quot;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">sentence_idx</span><span class="p">]</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="n">word_idx</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Valuation at position?&quot;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">sentence_idx</span><span class="p">]</span><span class="o">.</span><span class="n">subformula_valuations</span><span class="p">[</span><span class="n">word_idx</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction label?&quot;</span><span class="p">,</span> <span class="n">prediction_label</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label distribution&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">get_stats</span><span class="p">(</span><span class="n">probe_dataset_test</span><span class="o">.</span><span class="n">ys</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="probe-training">
<h3>Probe training<a class="headerlink" href="#probe-training" title="Link to this heading">#</a></h3>
<p>Now that we know how to collect data for the probe, let’s set up code to construct, train and evaluate a probing model.</p>
<p>Instead of using a LogisticRegression from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>, I have provided a simple pytorch module called <code class="docutils literal notranslate"><span class="pre">Probe</span></code>, which is essentially just a wrapper around <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code>.</p>
<section id="todo-1-implement-the-training-loop-of-the-probe">
<h4>☑️ ToDo 1: Implement the training loop of the probe<a class="headerlink" href="#todo-1-implement-the-training-loop-of-the-probe" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Probe</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A very simple linear model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">train_probe</span><span class="p">(</span><span class="n">probe</span><span class="p">,</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">probe</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">probe</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">loss_function</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># TODO: implement a simple training loop given the dataloader, optimizer, probe and loss function</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="n">probe</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">probe</span>
    
<span class="k">def</span> <span class="nf">eval_probe</span><span class="p">(</span><span class="n">probe</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">):</span>
    <span class="n">probe</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">ys_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">val_dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">))</span>
    <span class="n">ys_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">val_dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">))</span>
    
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">val_dataloader</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">probe</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ys_pred</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)]</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">ys_true</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)]</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">i</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">ys_pred</span><span class="p">,</span>
            <span class="s2">&quot;true&quot;</span><span class="p">:</span> <span class="n">ys_true</span><span class="p">,</span>
            <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">ys_true</span><span class="p">,</span> <span class="n">ys_pred</span><span class="p">),</span>
            <span class="s2">&quot;f1_macro&quot;</span><span class="p">:</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">ys_true</span><span class="p">,</span> <span class="n">ys_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">),</span>
            <span class="s2">&quot;confusion_matrix&quot;</span><span class="p">:</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ys_true</span><span class="p">,</span> <span class="n">ys_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="s1">&#39;true&#39;</span><span class="p">)</span>
    <span class="p">}</span>

<span class="k">def</span> <span class="nf">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">.8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Splits a ProbeDataSet into two datasets&quot;&quot;&quot;</span>
    <span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">*</span> <span class="n">train_size</span><span class="p">)</span>
    <span class="n">test_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">test_size</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">split_dataset_into_dataloaders</span><span class="p">(</span><span class="n">probe_dataset</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">.8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Splits a ProbeDataSet into two dataloaders&quot;&quot;&quot;</span>
    <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="n">probe_dataset</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="n">train_size</span><span class="p">)</span>
    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span> <span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="training-variable-probes-per-layer">
<h3>Training variable probes per layer<a class="headerlink" href="#training-variable-probes-per-layer" title="Link to this heading">#</a></h3>
<p>We must first make some decisions about what data to use.:</p>
<ol class="arabic simple">
<li><p>To remove noise, it is probably a good idea to only include datapoints where the model was correct.</p>
<ul class="simple">
<li><p>To think: what would happen if you included incorrect datapoints to the probe trainer? Could this also have benefits?</p></li>
</ul>
</li>
<li><p>As a first experiment, we can try to probe nodes that are variables. If a variable <strong>must</strong> be true, the model <strong>must also output</strong> that it is true: this information should therefore be kept track of for all sentences where the model is correct.</p></li>
<li><p>Finally, we can choose which layer to probe.</p></li>
</ol>
<p>For evaluation of the probes: note that this is a 3-way classification task, so random accuracy is 33%. Some classes may have a higher accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Only include that part of the df where the model was correct</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">ProbeTask</span><span class="p">(</span><span class="n">stacked_hidden</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">semantically_correct</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example code for training a probe:</span>
<span class="c1"># Let&#39;s start with at least 1000 examples per node_type per truth type.</span>
<span class="c1"># Print the distribution of labels to make sure the data is balanced</span>
<span class="n">layer_no</span> <span class="o">=</span> <span class="mi">4</span>
<span class="c1"># Set seed for sampling data / training probe</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">probe_dataset</span><span class="p">,</span> <span class="n">idxs</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">(</span><span class="n">layer_no</span><span class="o">=</span><span class="n">layer_no</span><span class="p">,</span> <span class="n">sents</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">node_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span><span class="s2">&quot;b&quot;</span><span class="p">,</span><span class="s2">&quot;c&quot;</span><span class="p">,</span><span class="s2">&quot;d&quot;</span><span class="p">,</span><span class="s2">&quot;e&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label distribution&quot;</span><span class="p">,</span> <span class="n">get_stats</span><span class="p">(</span><span class="n">probe_dataset</span><span class="o">.</span><span class="n">ys</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total dataset size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">probe_dataset</span><span class="p">))</span>
<span class="n">train_dataloader</span><span class="p">,</span> <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">split_dataset_into_dataloaders</span><span class="p">(</span><span class="n">probe_dataset</span><span class="p">)</span>
<span class="n">probe</span> <span class="o">=</span> <span class="n">train_probe</span><span class="p">(</span><span class="n">Probe</span><span class="p">(),</span> <span class="n">train_dataloader</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">eval_probe</span><span class="p">(</span><span class="n">probe</span><span class="p">,</span> <span class="n">val_dataloader</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="todo-2-probe-variable-nodes-for-all-layers">
<h4>☑️ ToDo 2 : probe variable nodes for all layers.<a class="headerlink" href="#todo-2-probe-variable-nodes-for-all-layers" title="Link to this heading">#</a></h4>
<p>Which layer gives the best performance and why do you think this is?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ToDo: train and evaluate variable-node-probes for each of the six layers</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="tosubmit-1-visualize-your-probing-results-per-layer">
<h4>✍️ ToSubmit 1: Visualize your probing results per layer.<a class="headerlink" href="#tosubmit-1-visualize-your-probing-results-per-layer" title="Link to this heading">#</a></h4>
<p>Give a brief description of the pattern you see, and why you might be seeing this.</p>
<p>(It is enough to do this in the caption of the figure)</p>
</section>
<section id="tothink">
<h4>🧠 ToThink:<a class="headerlink" href="#tothink" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Given your results per layer, what are the limitations of probing for these <code class="docutils literal notranslate"><span class="pre">subformula_valuations</span></code> respective to the <strong>entire</strong> sentence?</p></li>
</ul>
</section>
</section>
<section id="probing-other-types-of-nodes">
<h3>Probing other types of nodes<a class="headerlink" href="#probing-other-types-of-nodes" title="Link to this heading">#</a></h3>
<p>We have now only extracted hidden states of nodes at the positions of variables.</p>
<p>However, other positions have the same property of always true/always false/contingent, as we have seen in exercise Pen &amp; Paper 2 above.
These nodes of other types can thus also be probed in the same way.</p>
<section id="todo-3-run-probes-for-non-variable-operator-nodes">
<h4>☑️  ToDo 3: Run probes for non-variable/operator nodes.<a class="headerlink" href="#todo-3-run-probes-for-non-variable-operator-nodes" title="Link to this heading">#</a></h4>
<p>(I recommend training on <strong>exactly one type of node</strong> at the time, see next section. Be sure to increase the <code class="docutils literal notranslate"><span class="pre">sents</span></code> parameter to get a bigger dataset for just a single node type, but also pay attention to the label distribution and keep it balanced!)</p>
<ul class="simple">
<li><p>Is the performance better or worse in general?</p></li>
<li><p>Is the pattern the same throughout the layers as with the variable nodes?</p></li>
</ul>
</section>
<section id="tosubmit-2-add-probing-for-your-operator-node-probe-to-your-visualization-for-tosubmit-1">
<h4>✍️ ToSubmit 2: Add probing for your operator-node probe to your visualization for ToSubmit 1.<a class="headerlink" href="#tosubmit-2-add-probing-for-your-operator-node-probe-to-your-visualization-for-tosubmit-1" title="Link to this heading">#</a></h4>
<p>Describe the pattern for these operator probes, and compare these results to your results in ToSubmit 1. Is there a difference, and what might be the reason for this difference?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer_no</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># 3000 sents gives a mostly balanced label distribution</span>
<span class="n">probe_dataset</span><span class="p">,</span> <span class="n">idxs</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">(</span><span class="n">layer_no</span><span class="o">=</span><span class="n">layer_no</span><span class="p">,</span> <span class="n">sents</span> <span class="o">=</span> <span class="mi">3000</span><span class="p">,</span> <span class="n">node_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&amp;&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Label distribution&quot;</span><span class="p">,</span><span class="n">get_stats</span><span class="p">(</span><span class="n">probe_dataset</span><span class="o">.</span><span class="n">ys</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total dataset size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">probe_dataset</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="do-probes-generalize">
<h3>Do probes generalize?<a class="headerlink" href="#do-probes-generalize" title="Link to this heading">#</a></h3>
<p>During our previous experiment we have assumed that data is encoded similarly for every node type in our dataset (every variable).
However, it could be that our model has learned solutions that do not treat different variable types the same.</p>
<section id="todo-4-train-probes-on-one-subset-of-node-types-then-evaluate-them-on-another-subset-as-well-as-the-original-test-set">
<h4>☑️ ToDo 4: train probes on one subset of node_types, then evaluate them on another subset as well as the original test set.<a class="headerlink" href="#todo-4-train-probes-on-one-subset-of-node-types-then-evaluate-them-on-another-subset-as-well-as-the-original-test-set" title="Link to this heading">#</a></h4>
<p>E.g. train only on variable <code class="docutils literal notranslate"><span class="pre">a</span></code> (or <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>) and evaluate on all other variables. How does performance compare when testing on <code class="docutils literal notranslate"><span class="pre">a</span></code> versus other variables?
Try multiple different subsets. Which nodes generalize to which other nodes?</p>
<p>For this experiment, you can stick with a single layer (the best one from the previous exercise), or try multiple layers.</p>
</section>
<section id="tosubmit-3-include-your-findings-in-the-report">
<h4>✍️ ToSubmit 3: Include your findings in the report.<a class="headerlink" href="#tosubmit-3-include-your-findings-in-the-report" title="Link to this heading">#</a></h4>
<p>Since you need to try multiple different subsets of train/test nodes, it is recommended to visualize this in a heatmap (similar to the cross-layer probing experiments from week 1).</p>
<p>Briefly reflect on these results. Which probes generalize, which ones do not? Might this tell us anything about the way this Transformer is representing propositional logic?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example code to generate a dataset that is of another type of node:</span>
<span class="c1"># probe_dataset_b_only, idxs = p.create_dataset(layer_no=5, sents = 5000, node_types=[&quot;b&quot;])</span>
<span class="c1"># dataloader_b , _ = split_dataset_into_dataloaders(probe_dataset_b_only, 1.0) # Do not split</span>
<span class="c1"># Now, you can pass dataloader_b to eval_probe with a probe that was trained on different data</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="logical-equivalences-through-the-layers">
<h2>Logical equivalences through the layers<a class="headerlink" href="#logical-equivalences-through-the-layers" title="Link to this heading">#</a></h2>
<p>If the model truly understood the semantics of propositional logic, it should “know about” logical equivalences. For instance, ‘! xor’ is equal in meaning to ‘&lt;-&gt;’.</p>
<p>We may be able to see this in the behaviour of the model, by comparing its outputs for sentences that are logically equivalent. Two sentences are logically equivalent if they have exactly the same possible worlds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert the full (non partial) possible worlds to a string so pandas can easily find all unique values</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;fullworlds&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;possible_worlds&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">z</span> <span class="k">for</span> <span class="n">z</span> <span class="ow">in</span> <span class="n">x</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">==</span><span class="nb">max</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span><span class="n">x</span><span class="p">))))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># What are the maximum occuring equivalent sentences?</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;fullworlds&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The model outputs (but also the gold outputs) are not always the same for equivalent sentences</span>
<span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;fullworlds&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;b0c0 b0c1 b1c1&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We see that the model does not always predict the same output for a logically equivalent sentence. Similarly, since there are only a relatively small number of possible outputs, the model will predict the same output for logically inequivalent sentences, as well.</p>
<p>Maybe we can gain more insight into whether the model knows about logical equivalence by looking at its hidden representations.</p>
<section id="creating-our-own-small-dataset">
<h3>Creating our own small dataset<a class="headerlink" href="#creating-our-own-small-dataset" title="Link to this heading">#</a></h3>
<p>The validation set contains mostly very long sentences, and not many sentences that are logically equivalent.</p>
<p>Instead of sampling from the validation set, we can create our own small dataset to further inspect its behaviour on hand-constructed (non)equivalent sentences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Some pairs of logical equivalences for the input data</span>
<span class="c1"># Student ToDo: add more inputs</span>
<span class="n">mini_data_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;! &lt;-&gt; a b&quot;</span><span class="p">,</span> <span class="s2">&quot;xor a b&quot;</span><span class="p">]</span>
<span class="c1"># Corresponding correct outputs for completeness. Will not affect outputs</span>
<span class="n">mini_data_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a 1 b 0&quot;</span><span class="p">,</span> <span class="s2">&quot;a 1 b 0&quot;</span><span class="p">]</span>
<span class="n">mini_dataset</span> <span class="o">=</span> <span class="n">LogicDataSet</span><span class="p">(</span>
    <span class="n">vocab_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;transformer_logic_compact/vocabulary&quot;</span><span class="p">,</span>
    <span class="n">data_inputs</span><span class="o">=</span><span class="p">(</span><span class="n">mini_data_inputs</span><span class="p">,</span> <span class="n">mini_data_outputs</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Wrap the mini dataset in a dataloader and perform a forward pass</span>
<span class="n">mini_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">mini_dataset</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">VarLengthCollate</span><span class="p">(</span><span class="n">batch_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="n">mini_states</span><span class="p">,</span> <span class="n">mini_outputs</span> <span class="o">=</span> <span class="n">forward_all</span><span class="p">(</span><span class="n">model_interface</span><span class="p">,</span> <span class="n">mini_dataloader</span><span class="p">,</span> <span class="n">max_it</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mini_outputs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The root node is at position 1.</span>
<span class="n">position_to_compare</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="p">[</span><span class="n">ax</span> <span class="k">for</span> <span class="n">ax_array</span> <span class="ow">in</span> <span class="n">axs</span> <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">ax_array</span><span class="p">]</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">layer_no</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span><span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)):</span>
    <span class="n">cosines</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">pairwise</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">mini_states</span><span class="p">[</span><span class="n">layer_no</span><span class="p">][:,</span><span class="n">position_to_compare</span><span class="p">,:])</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cosines</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_data_inputs</span><span class="p">))</span><span class="o">+</span><span class="mf">0.5</span> <span class="p">,</span><span class="n">mini_data_inputs</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_data_inputs</span><span class="p">))</span><span class="o">+</span><span class="mf">0.5</span> <span class="p">,</span><span class="n">mini_data_inputs</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Layer </span><span class="si">{</span><span class="n">layer_no</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="todo-5-add-at-least-4-more-inputs-to-the-mini-dataset-above-and-compare-the-similarity-matrix-through-the-layers">
<h4>☑️ ToDo 5: Add at least 4 more inputs to the mini-dataset above and compare the similarity matrix through the layers<a class="headerlink" href="#todo-5-add-at-least-4-more-inputs-to-the-mini-dataset-above-and-compare-the-similarity-matrix-through-the-layers" title="Link to this heading">#</a></h4>
<p>Be sure to include equivalent inputs as well as non-equivalent inputs.</p>
<p>It might be interesting to also include inputs that are ‘almost equivalent’!</p>
</section>
<section id="id1">
<h4>🧠 ToThink:<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>I set <code class="docutils literal notranslate"><span class="pre">position_to_compare</span></code> to 1, meaning we compare the root node. Which side effects could this have? Can we see these effects through the layers?
Could we also compare different positions?</p>
</section>
<section id="tosubmit-4-include-the-plot-above-with-a-subset-of-your-non-equivalent-sentences">
<h4>✍️ ToSubmit 4: Include the plot above with (a subset of your) (non)-equivalent sentences.<a class="headerlink" href="#tosubmit-4-include-the-plot-above-with-a-subset-of-your-non-equivalent-sentences" title="Link to this heading">#</a></h4>
<p>Reflect on at least these two questions:</p>
<ul class="simple">
<li><p>What does similarity mean through the layers?</p></li>
<li><p>Which (non)equivalent sentences are more similar? Briefly describe at least two patterns you see.</p></li>
</ul>
<p>Note: Feel free to change the plotting code, or include multiple similarity plots, if this makes things clearer.</p>
</section>
</section>
</section>
<section id="end-of-notebook">
<h2>✅ <font color=green> End of notebook! </font><a class="headerlink" href="#end-of-notebook" title="Link to this heading">#</a></h2>
<p>You have reached the end of the notebook - there are no more official ToDo’s, but I have provided one more section below for interested students.</p>
<p>I have spent a long time with these models during my thesis, so if you have (practical, existential, philosophical) questions about any of these experiments, you can always ask me (Anna) questions about it (for instance by <a class="reference external" href="mailto:annalangedijk&#37;&#52;&#48;gmail&#46;com">emailing me</a>).</p>
</section>
<section id="optional-models-trained-on-systematically-different-data">
<h2>Optional: Models trained on systematically different data<a class="headerlink" href="#optional-models-trained-on-systematically-different-data" title="Link to this heading">#</a></h2>
<p>I have also provided a model that is trained without the pattern <code class="docutils literal notranslate"><span class="pre">!</span> <span class="pre">xor</span></code>. Any <code class="docutils literal notranslate"><span class="pre">!</span> <span class="pre">xor</span></code> is replaced with <code class="docutils literal notranslate"><span class="pre">&lt;-&gt;</span></code> during training, so the amount of training data stays (almost) equal.</p>
<p>The model performs with equal performance on most sentences, including sentences with <code class="docutils literal notranslate"><span class="pre">xor</span></code> and ``&lt;-&gt;<code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">but</span> <span class="pre">when</span> </code>! xor<code class="docutils literal notranslate"><span class="pre">is</span> <span class="pre">present,</span> <span class="pre">the</span> <span class="pre">performance</span> <span class="pre">drops.</span> <span class="pre">The</span> <span class="pre">model</span> <span class="pre">seems</span> <span class="pre">to</span> <span class="pre">not</span> <span class="pre">be</span> <span class="pre">able</span> <span class="pre">to</span> <span class="pre">combine</span></code>!<code class="docutils literal notranslate"><span class="pre">with</span></code>xor`.</p>
<p>Additionally, I have provided another model. This one is training without another pattern, namely <code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">xor</span></code>. Not only does <code class="docutils literal notranslate"><span class="pre">&amp;</span> <span class="pre">xor</span></code> never appear (<code class="docutils literal notranslate"><span class="pre">xor</span></code> being the left subtree of the binary <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> node), but I have also filtered any occurences where <code class="docutils literal notranslate"><span class="pre">xor</span></code> is the right subtree of the <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> node, meaning the model has never seen a node <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> with a child <code class="docutils literal notranslate"><span class="pre">xor</span></code>, similar to the unary operator <code class="docutils literal notranslate"><span class="pre">!</span></code> in the previous model not having seen <code class="docutils literal notranslate"><span class="pre">xor</span></code> as a child.</p>
<p>The model performs with almost equal performance to the original model, including on sentences containing the left-out pattern.</p>
<p>How can we understand what this model is missing?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply the models to exactly the same validation set </span>
<span class="n">_</span><span class="p">,</span> <span class="n">model_outputs_without_not_xor</span> <span class="o">=</span>  \
    <span class="n">load_hidden_and_outputs</span><span class="p">(</span><span class="s1">&#39;model_without_not_xor_seed2&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">load_from_file</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">calculate_outputs</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">MAX_IT</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                           <span class="n">calculate_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">model_outputs_without_and_xor</span> <span class="o">=</span>  \
    <span class="n">load_hidden_and_outputs</span><span class="p">(</span><span class="s1">&#39;model_without_and_xor_seed1&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">load_from_file</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">calculate_outputs</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">MAX_IT</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                           <span class="n">calculate_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_without_not_xor</span> <span class="o">=</span> <span class="n">calculate_df</span><span class="p">(</span><span class="n">model_outputs_without_not_xor</span><span class="p">,</span> <span class="n">CUTOFF</span><span class="p">)</span>
<span class="n">df_without_and_xor</span> <span class="o">=</span> <span class="n">calculate_df</span><span class="p">(</span><span class="n">model_outputs_without_and_xor</span><span class="p">,</span> <span class="n">CUTOFF</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_without_and_xor</span><span class="p">[</span><span class="s1">&#39;semantically_correct&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_without_not_xor</span><span class="p">[</span><span class="s1">&#39;semantically_correct&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_without_not_xor</span><span class="p">[</span><span class="o">~</span><span class="n">df_without_not_xor</span><span class="o">.</span><span class="n">semantically_correct</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;input_size&#39;</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_without_and_xor</span><span class="p">[</span><span class="n">df_without_and_xor</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="s1">&#39;&amp; xor&#39;</span><span class="p">)]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;input_size&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="future-questions-ideas-for-projects">
<h2>Future questions / Ideas for projects<a class="headerlink" href="#future-questions-ideas-for-projects" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Are there differences in outputs/probing results/hidden state results for the models trained on systematically different data?</p></li>
<li><p>Can we probe for something else?</p></li>
<li><p>Can we compare this model to a (neuro)symbolic solver, or find an algorithm that the model is using internally?</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "thesis2022"
        },
        kernelOptions: {
            name: "thesis2022",
            path: "./symbolic"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'thesis2022'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tothink-dataset-creation">🧠 ToThink: dataset creation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-dataset">Load dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-inspection-of-the-data">First inspection of the data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pen-and-paper-1">📝 Pen and Paper 1</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-model">Load the model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterate-over-data-to-get-hidden-states-or-load-them-from-a-file">Iterate over data to get hidden states (or load them from a file)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collect-information-about-the-dataset-and-model-outputs">Collect information about the dataset and model outputs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pen-and-paper-2">📝 Pen and Paper 2</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probing-for-truth">Probing for Truth</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probe-task-construction">Probe task construction</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-a-probetask-object">Example of a probetask object</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probe-training">Probe training</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-1-implement-the-training-loop-of-the-probe">☑️ ToDo 1: Implement the training loop of the probe</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-variable-probes-per-layer">Training variable probes per layer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-2-probe-variable-nodes-for-all-layers">☑️ ToDo 2 : probe variable nodes for all layers.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tosubmit-1-visualize-your-probing-results-per-layer">✍️ ToSubmit 1: Visualize your probing results per layer.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tothink">🧠 ToThink:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probing-other-types-of-nodes">Probing other types of nodes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-3-run-probes-for-non-variable-operator-nodes">☑️  ToDo 3: Run probes for non-variable/operator nodes.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tosubmit-2-add-probing-for-your-operator-node-probe-to-your-visualization-for-tosubmit-1">✍️ ToSubmit 2: Add probing for your operator-node probe to your visualization for ToSubmit 1.</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#do-probes-generalize">Do probes generalize?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-4-train-probes-on-one-subset-of-node-types-then-evaluate-them-on-another-subset-as-well-as-the-original-test-set">☑️ ToDo 4: train probes on one subset of node_types, then evaluate them on another subset as well as the original test set.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tosubmit-3-include-your-findings-in-the-report">✍️ ToSubmit 3: Include your findings in the report.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logical-equivalences-through-the-layers">Logical equivalences through the layers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-our-own-small-dataset">Creating our own small dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#todo-5-add-at-least-4-more-inputs-to-the-mini-dataset-above-and-compare-the-similarity-matrix-through-the-layers">☑️ ToDo 5: Add at least 4 more inputs to the mini-dataset above and compare the similarity matrix through the layers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">🧠 ToThink:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tosubmit-4-include-the-plot-above-with-a-subset-of-your-non-equivalent-sentences">✍️ ToSubmit 4: Include the plot above with (a subset of your) (non)-equivalent sentences.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#end-of-notebook">✅ <font color="green"> End of notebook! </font></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-models-trained-on-systematically-different-data">Optional: Models trained on systematically different data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-questions-ideas-for-projects">Future questions / Ideas for projects</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div>
<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png"></a>
</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>