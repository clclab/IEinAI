# Mechanistic Interpretability

Welcome to the Mechanistic Interpretability workshop! There are three papers for this course: one main paper, which everyone reads, and two papers that will be presented. For week 2, these are the following:

1. Main Paper: [Investigating Gender Bias in Language Models Using Causal Mediation Analysis](https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf)
2. Presentation 1: [Towards Automated Circuit Discovery for Mechanistic Interpretability](https://arxiv.org/abs/2304.14997). This presentation will be on the circuits framework, and how to find circuits. Hopefully, you should also be able to skim / include results from [this paper](https://arxiv.org/abs/2310.10348) and [this paper](https://arxiv.org/abs/2403.17806) (don't bother reading Section 5 or beyond). The idea would be to first introduce circuitsâ€”what are they, and why are they useful? Then, set the three papers in dialogue, to explain how circuit-finding methods have recently evolved.
3. Presentation 2: [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features), or [Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://openreview.net/forum?id=F76bwRSLeK); whichever you prefer. This presentatation will be on sparse autoencoders, and the many attempts to understand the features that (language) models rely on to produce their behavior. In addition to the main paper, which describes sparse autoencoders, please look at the first two pages of [this paper](https://arxiv.org/abs/2104.07143), which talks about issues in neuron-level interpretation, and [this paper](https://transformer-circuits.pub/2022/toy_model/index.html) which tries to identify the roots of the aforementioned issue. These two papers form the build-up to sparse autoencoders (very useful for the related work / scientific context part of your presentation)!

The presenters of either paper are also welcome to talk about [this paper](https://arxiv.org/abs/2403.19647), which bridges this week's two topics.

The notebook is available via [this link](https://colab.research.google.com/drive/1YSjHs1nYBEqhpg6SXyxb0tBIdZvuFxHO?usp=sharing).
