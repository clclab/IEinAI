{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa1daab-b8df-41d2-be04-84cc0e1105e0",
   "metadata": {},
   "source": [
    "# Investigating how Transformers learn propositional logic\n",
    "\n",
    "Author: Anna Langedijk\n",
    "\n",
    "With credits to: Jaap Jumelet, Jelle Zuidema\n",
    "\n",
    "Part of the Logic & Deep Learning workshop for the course Interpretability & Explainability in AI 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe893ad-62dc-4cc8-9a4f-45d9101df702",
   "metadata": {
    "id": "4fe893ad-62dc-4cc8-9a4f-45d9101df702",
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "There are several ways researchers have combined the ultimately symbolic domain of Logic with that of neural networks. Neurosymbolic models are one research direction. For instance, in the Logic Tensor Networks paper you have seen one way of incorporating logic.  Another example is [NeuroSAT](https://arxiv.org/abs/1802.03685): an end-to-end SAT solver that, instead of adding logic explicitly, uses a graph neural network to represent its logical inputs. Input formulas are always in conjunctive normal form (CNF): relevant clauses and literals are connected by edges in the input graph to the network.\n",
    "\n",
    "However, many models used today are trained end-to-end on flat input strings. They must rely on their training data to learn any type of logic and reasoning. These models have no explicit reasoning abilities, and yet they display behaviour that requires them to perform some form of reasoning, such as Question Answering and Natural Language Understanding. The question arises whether it is even possible to learn the rules of logic from data alone.\n",
    "\n",
    "To test the reasoning abilities of certain Transformer models, or Transformers/neural models in general, there exist several benchmarks. In the domain of natural language, there exist several Natural Language Inference ([NLI](https://aclanthology.org/S14-2001/)) datasets. These datasets usually focus on classification tasks, for instance: Given two sentences, do these sentences entail one another or not? In the formal domain, similar datasets exist (e.g. [this paper](https://www.deepmind.com/publications/can-neural-networks-understand-logical-entailment)), where the task is still to predict (non)entailment, but inputs and outputs are instead given in a formal logical form.\n",
    "\n",
    "A recent paper [Teaching Temporal Logic to Neural Networks](https://arxiv.org/abs/2003.04218) by Hahn et al. instead explores a different objective: that of generating correct solutions given an input formula in some logic. They do this using a generic encoder-decoder setup. Instead of a specific binary output, the model is trained to generate \"explanations\" (in some sense) for the formula, namely a possible world/trace for the input. Their main experiments focus on generating traces for Temporal Logic formulas, but in a second experiment the authors claim that these encoder-decoder models can also learn the semantics of Propositional Logic. \n",
    "\n",
    "\n",
    "In this notebook we will focus on the models trained on propositional logic trained on input-output pairs generated by a symbolic solver. \n",
    "An example of such an input-output pair is:\n",
    "\n",
    "\n",
    "In: `(a xor b) & c`\n",
    "\n",
    "Out: `a=False, b=True, c=True`\n",
    "\n",
    "\n",
    "A model trained on 800000 of such pairs can achieve a high (93%) accuracy in predicting logically valid assignments, not only emulating the outputs provided by the symbolic solver, but also generating alternative valid possible worlds/assignments. \n",
    "\n",
    "But, while the task itself is interpretable (it is unambiguous, and there are no hidden assumptions as is the case with many tasks based on natural data), it is still a challenge to understand _how_ the model solves this task, and if it truly has knowledge about the semantics of logic or if it is exploiting irrelevant patterns in the data.\n",
    "\n",
    "In this notebook, you will probe the hidden states of the encoder of this model for propositional logical 'truth', to see what kind of information is encoded when the encoder processes a sentence, and whether that information is encoded the same way for different types of tokens through its layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9266ab9-9f72-4e26-b580-4e810ebea43d",
   "metadata": {},
   "source": [
    "#### 🧠 ToThink: dataset creation\n",
    "- What are other ways this dataset differs from classification tasks? Can you see downsides to this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46500a1-11dd-457a-8781-eb4bdb0d1d41",
   "metadata": {
    "id": "d46500a1-11dd-457a-8781-eb4bdb0d1d41"
   },
   "source": [
    "## Setup\n",
    "First, import some prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7369ad6-1376-4cbc-950a-8ddb3b29ed7d",
   "metadata": {
    "id": "e7369ad6-1376-4cbc-950a-8ddb3b29ed7d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe4bba-f44f-4cd4-b08d-3fe1167e6500",
   "metadata": {},
   "source": [
    "GPU shouldn't be necessary, although it will be faster if you want to (re)calculate the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27348063-3250-4b35-9fd1-7e34f835cb0e",
   "metadata": {
    "id": "27348063-3250-4b35-9fd1-7e34f835cb0e"
   },
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "INSTALL = False\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08592f9e-1788-4c98-81f5-36cd805e30a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08592f9e-1788-4c98-81f5-36cd805e30a1",
    "outputId": "a70f198a-6cac-4959-ff21-0f5926c254c3"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # Mount to drive so we can access our own files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0bd72-1b32-4176-9f2b-8606af001fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INSTALL:\n",
    "    !pip install --user torch sklearn matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b64665-72e7-4cb6-9f84-a0554f8b681c",
   "metadata": {
    "id": "17b64665-72e7-4cb6-9f84-a0554f8b681c"
   },
   "source": [
    "Download and import the code that can load the dataset and model architecture. \n",
    "The model implementation is available here: [github.com/annaproxy/transformer_logic_compact](https://github.com/annaproxy/transformer_logic_compact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aadb32-3f2a-4a08-970c-baf9add38f2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0aadb32-3f2a-4a08-970c-baf9add38f2c",
    "outputId": "fd1bf374-3f4d-47f0-e860-d4ff0e7ab096"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # Point this to the correct location\n",
    "    %cd /content/drive/My Drive/IEAI_Notebooks\n",
    "    if os.path.exists(\"transformer_logic_compact\"):\n",
    "        %cd transformer_logic_compact\n",
    "        !git pull https://github.com/annaproxy/transformer_logic_compact\n",
    "        %cd ..\n",
    "    else:\n",
    "        !git clone https://github.com/annaproxy/transformer_logic_compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d0312-3e14-4f35-a084-78ec1d7a0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"transformer_logic_compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe3c5e-bcdc-478c-8157-4179131ed9d8",
   "metadata": {
    "id": "8dfe3c5e-bcdc-478c-8157-4179131ed9d8"
   },
   "outputs": [],
   "source": [
    "from interfaces import ModelInterface, TransformerEncDecInterface\n",
    "from models import TransformerEncDecModel\n",
    "from layers.transformer import Transformer\n",
    "from helpers.collater import VarLengthCollate\n",
    "from logicdatasets import LogicDataSet\n",
    "from helpers import move_to_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83202e-2d2c-4bcc-812a-906b39c858ed",
   "metadata": {
    "id": "1e83202e-2d2c-4bcc-812a-906b39c858ed"
   },
   "source": [
    "Download the pre-provided data (validation set, information about its subtrees, pretrained models) and put it in the data directory:\n",
    "https://drive.google.com/drive/folders/1TE0loQ76bcBoYKxXDjUsyfCWiPsBHSv9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arJ-NOcsakMo",
   "metadata": {
    "id": "arJ-NOcsakMo"
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'transformer_logic_compact/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0f979-8d46-4035-aab5-47a3d29f6a3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4c0f979-8d46-4035-aab5-47a3d29f6a3b",
    "outputId": "940e68fd-7e1e-4fee-df30-1f8751354b9f"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !gdown --folder 1TE0loQ76bcBoYKxXDjUsyfCWiPsBHSv9 -O $DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2b7a0-6c9b-4ece-a363-911361d949a9",
   "metadata": {
    "id": "13e2b7a0-6c9b-4ece-a363-911361d949a9"
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4278f-43ee-4baa-acb4-338573c1cd13",
   "metadata": {
    "id": "8cd4278f-43ee-4baa-acb4-338573c1cd13"
   },
   "outputs": [],
   "source": [
    "# The files val.src and val.tgt will be loaded\n",
    "dataset = LogicDataSet(data_path=f'{DATA_FOLDER}/val', vocab_path='transformer_logic_compact/vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fe61b-5f88-4fba-8d8f-2708d9cd9cdc",
   "metadata": {
    "id": "033fe61b-5f88-4fba-8d8f-2708d9cd9cdc"
   },
   "source": [
    "### First inspection of the data\n",
    "The input data is provided to the model in [Polish prefix notation](https://en.wikipedia.org/wiki/Polish_notation). Instead of writing operators with infix, e.g. `a | b`, the operator is provided as a prefix: `| a b`. This avoids the use of parentheses, which would only add to the length of the input of the model. \n",
    "\n",
    "The following input means ``(b | d) xor (c)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7b4c1-5c03-4cc6-9975-40b5679ec3e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18f7b4c1-5c03-4cc6-9975-40b5679ec3e2",
    "outputId": "e402507d-b2b1-45ac-e964-5a8aeb4d9548"
   },
   "outputs": [],
   "source": [
    "print(\"String input:\", dataset.input_ids_to_text(dataset[22623]['in']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15899758-b379-4766-a0f5-384b0cbf4655",
   "metadata": {
    "id": "15899758-b379-4766-a0f5-384b0cbf4655"
   },
   "source": [
    "The outputs are provided as a simple array of maximum length 10, for example: `a 0 b 1 c 1 d 1 e 0` corresponds to the model {a=False, b=True, c=True,d=True, e=False}. The outputs are always in alphabetical order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e4b20-352d-4b56-901d-11bf2cae8c18",
   "metadata": {},
   "source": [
    "#### 📝 Pen and Paper 1\n",
    "Think of a possible valid assignment to this sentence. Then, use the 'out' key of the dataset object to print the gold label. Did you generate the same output with your internal logic model, or are there multiple possible outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c193f7-2874-40aa-8f4f-1989e0d21963",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54c193f7-2874-40aa-8f4f-1989e0d21963",
    "outputId": "a188b077-b51c-48c5-ac51-da2698b59aaf"
   },
   "outputs": [],
   "source": [
    "print(\"String output:\", \"Todo: dataset output goes here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c713e8f-6aa4-4cf9-8a24-8de5dc3f5cc4",
   "metadata": {
    "id": "9c713e8f-6aa4-4cf9-8a24-8de5dc3f5cc4"
   },
   "source": [
    "### Load the model\n",
    "The Transformers that we use are relatively small: the encoder and decoder have 6 layers. The state sizes of the encoder and decoder are 128 and 64, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0a0fd-47a1-4c6d-8222-68c83f6197a2",
   "metadata": {
    "id": "29a0a0fd-47a1-4c6d-8222-68c83f6197a2"
   },
   "outputs": [],
   "source": [
    "# For now, we work with one model seed\n",
    "model_name = 'model_seed2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2f0f3-6330-4c2a-b18e-aff8a0833924",
   "metadata": {
    "id": "70d2f0f3-6330-4c2a-b18e-aff8a0833924"
   },
   "outputs": [],
   "source": [
    "def load_model_from_name(model_name):\n",
    "    # Initialize model architecture\n",
    "    model = TransformerEncDecModel(\n",
    "                len(dataset.in_vocabulary),\n",
    "                len(dataset.out_vocabulary),\n",
    "                state_size=128,\n",
    "                state_size_decoder=64,\n",
    "                nhead=4,\n",
    "                num_encoder_layers=6,\n",
    "                num_decoder_layers=6,\n",
    "                ff_multiplier=4,\n",
    "                ff_multiplier_decoder=8,\n",
    "                transformer=Transformer,\n",
    "                tied_embedding=True,\n",
    "                scale_mode=\"opennmt\"\n",
    "            )\n",
    "    # Load from state dict\n",
    "    model.load_state_dict(torch.load(f\"{DATA_FOLDER}/{model_name}.pth\", map_location=torch.device(DEVICE)))\n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Model interface helps with some overhead such as adding <BOS> and <EOS>.\n",
    "    # You should call this if you want to do a forward pass.\n",
    "    model_interface = TransformerEncDecInterface(model)\n",
    "    return model_interface\n",
    "model_interface = load_model_from_name(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34979916-116f-4314-af6e-fb99d8b89bc4",
   "metadata": {
    "id": "34979916-116f-4314-af6e-fb99d8b89bc4"
   },
   "outputs": [],
   "source": [
    "# Uncomment to inspect the model architecture in more detail\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c72259-1784-49db-9b95-db55b386447b",
   "metadata": {
    "id": "09c72259-1784-49db-9b95-db55b386447b",
    "tags": []
   },
   "source": [
    "### Iterate over data to get hidden states (or load them from a file)\n",
    "I've provided a portion (~26%) of the hidden states in a separate drive: https://drive.google.com/drive/folders/1oSmpcYy_zi-bNA81ODf001-M80B44FMc \n",
    "This should be enough to run your probing experiment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9a724-ea91-4a24-8f24-45ec50355c3e",
   "metadata": {
    "id": "4ec9a724-ea91-4a24-8f24-45ec50355c3e"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=256,\n",
    "            collate_fn=VarLengthCollate(batch_dim=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256370e2-698e-49bc-b7ec-a6785a50d064",
   "metadata": {
    "id": "256370e2-698e-49bc-b7ec-a6785a50d064"
   },
   "outputs": [],
   "source": [
    "def forward_all(model_interface, dataloader, max_it=100, calculate_outputs = True,\n",
    "               calculate_hidden=True):\n",
    "    \"\"\"Forwards max_it batches through the model_interface, collecting its hidden states.\"\"\"\n",
    "    stacked_hidden = {}\n",
    "    outputs = []\n",
    "    # The dataset has size 100_000 entries, when not setting MAX_IT, these tensors will take up about 7G (!)\n",
    "    # It may take a while to run (3-6 minutes)\n",
    "    with torch.no_grad():\n",
    "        for it, d in enumerate(tqdm(dataloader, total=max_it)):\n",
    "            d = move_to_device(d, DEVICE)\n",
    "            result, hidden = model_interface(d, collect_hidden=True)\n",
    "\n",
    "            current_batch_size = d['in'].shape[-1]\n",
    "            if calculate_outputs:\n",
    "                digits = model_interface.decode_outputs(result)\n",
    "                output_strs = [dataset.sample_to_text(digits, i) for i in range(current_batch_size)]\n",
    "                outputs.extend(output_strs)\n",
    "            if calculate_hidden:\n",
    "                for layer in hidden:\n",
    "                    if layer not in stacked_hidden:\n",
    "                        stacked_hidden[layer] = hidden[layer]\n",
    "                    else:\n",
    "                        # This is a bit slow, .cat creates a new tensor\n",
    "                        # Also fun: batch size must be large enough to include a sentence of the max length every time\n",
    "                        stacked_hidden[layer] = torch.cat([stacked_hidden[layer], hidden[layer]], dim =0)\n",
    "            if it > max_it:\n",
    "                break\n",
    "    return stacked_hidden, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1f85d-f358-4b96-82b3-0c8f12dd1e1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ea1f85d-f358-4b96-82b3-0c8f12dd1e1e",
    "outputId": "77707016-e14c-4408-ed22-fa0683b49db8"
   },
   "outputs": [],
   "source": [
    "def load_hidden_and_outputs(model_name, model_interface, load_from_file=True, calculate_outputs = True, MAX_IT=100,\n",
    "                           calculate_hidden=True):\n",
    "    stacked_hidden = None\n",
    "    if not load_from_file:\n",
    "        stacked_hidden, outputs = forward_all(model_interface, dataloader, max_it=MAX_IT, calculate_outputs=calculate_outputs)\n",
    "        torch.save(stacked_hidden, f'{DATA_FOLDER}/{model_name}_hiddenstates.pth')\n",
    "        if calculate_outputs:\n",
    "            with open(f'{DATA_FOLDER}/outputs_{model_name}.txt', 'w') as f:\n",
    "                for line in outputs:\n",
    "                    f.write(line+'\\n')\n",
    "\n",
    "    else:\n",
    "        if calculate_hidden:\n",
    "            file_path = f'{DATA_FOLDER}/{model_name}_hiddenstates.pth'\n",
    "            if not os.path.exists(file_path):\n",
    "                # Ugly, but don't want to download the whole folder and blast you with 10GB of hidden states\n",
    "                # gdown doesnt provide an option to pass file_path within a folder\n",
    "                if model_name == 'model_seed2':\n",
    "                    !gdown 1Ln-mIQ1YVQpHhePqRBXdgyv9w-tzldy7 -O $file_path\n",
    "                elif model_name == 'model_without_not_xor_seed2':\n",
    "                    !gdown 1l-pmncpeJ0jNdNhJHLoMR1wj_IHJXbGH -O $file_path\n",
    "                elif model_name == 'model_without_and_xor_seed1':\n",
    "                    !gdown 1dQjHw6eZEAgsIV2wB1-BQHdLeMOIwAoj -O $file_path\n",
    "                else:\n",
    "                    print(f\"No file found for {model_name}\")\n",
    "            stacked_hidden = torch.load(file_path)\n",
    "        with open(f'{DATA_FOLDER}/outputs_{model_name}.txt') as f:\n",
    "            outputs = [l.strip() for l in f.readlines()]\n",
    "    return stacked_hidden, outputs\n",
    "\n",
    "# Set load_from_file to false if you want to calculate new states\n",
    "# Downloading may take a while (~3gb)\n",
    "stacked_hidden, outputs = load_hidden_and_outputs('model_seed2', model_interface, load_from_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73778cc-3c77-46ea-a7cf-a2d427449fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stacked hidden keys\",stacked_hidden.keys())\n",
    "print(\"Stacked hidden shape for layer 3\",stacked_hidden[3].shape) # N x max_sentence_length x hidden_state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd2d99-467f-4c24-ba04-3d3ca8349eef",
   "metadata": {
    "id": "21cd2d99-467f-4c24-ba04-3d3ca8349eef"
   },
   "source": [
    "### Collect information about the dataset and model outputs\n",
    "\n",
    "For the 100000 sentences in this validation set, I have calculated some additional information:\n",
    "- `possible_worlds.txt` contains all the (partial) possible worlds in which a sentence can be true. This allows us to calculate whether any model prediction was correct, not just when the model output exactly matches the gold label. \n",
    "- `subformulas_valuation.txt` contains information about whether different tokens in the logical formula can be true/can be false. For instance, if the variable `a` is set to `1` in every possible world, it must always be true, and thus has the label `1`. If it is always false, it will have the label `0`. If it depends on the truth value of the rest of sentence (as is usually the case), it is set to `2`. You can find a more concrete example below.\n",
    "\n",
    "Let's load this information into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0590146-a855-4f3f-9ae1-78022e859e7f",
   "metadata": {
    "id": "e0590146-a855-4f3f-9ae1-78022e859e7f"
   },
   "outputs": [],
   "source": [
    "def calculate_df(model_outputs, cutoff):\n",
    "    with open(f'{DATA_FOLDER}/possible_worlds.txt') as f:\n",
    "        possible_worlds = [l.strip() for l in f.readlines()]\n",
    "\n",
    "    with open(f'{DATA_FOLDER}/subformulas_valuation.txt') as f:\n",
    "        valuations = [l.strip() for l in f.readlines()]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'sentence_idxs':range(CUTOFF),\n",
    "        'inputs':[s.strip() for s in dataset.in_lines[:cutoff]],\n",
    "        'gold_outputs':[s.strip() for s in dataset.out_lines[:cutoff]],\n",
    "        'model_outputs':model_outputs[:cutoff],\n",
    "        'possible_worlds':possible_worlds[:cutoff],\n",
    "        'subformula_valuations':valuations[:cutoff]  \n",
    "    })\n",
    "\n",
    "    df.possible_worlds = df.possible_worlds.apply(lambda x:x.split(','))\n",
    "    # An exact match is when the model output is exactly equal to the gold outputs\n",
    "    df['exact_match'] = df['model_outputs'] == df['gold_outputs']\n",
    "    # The model can be correct without exactly matching the gold outputs\n",
    "    df['semantically_correct'] = df.apply(lambda row:row['model_outputs'].replace(' ','')in row['possible_worlds'] ,axis=1)\n",
    "\n",
    "    # Handy statistic so we can look at short formulas as examples\n",
    "    df['input_size'] = df['inputs'].apply(lambda x:len(x.split()))\n",
    "    return df\n",
    "\n",
    "# How many hidden states we have saved\n",
    "CUTOFF = len(stacked_hidden[0])\n",
    "df = calculate_df(outputs, CUTOFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa666b0-155f-46a7-9d15-bd797972a92f",
   "metadata": {
    "id": "1fa666b0-155f-46a7-9d15-bd797972a92f"
   },
   "source": [
    "We can verify that the model has a high accuracy on the task, even when it does not exactly match the generated 'gold' output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81761ab-7150-45cc-9cbe-45b18869b413",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a81761ab-7150-45cc-9cbe-45b18869b413",
    "outputId": "39840c34-d6fc-46f0-ff65-af76df1dbca2"
   },
   "outputs": [],
   "source": [
    "print(df['exact_match'].mean())\n",
    "print(df['semantically_correct'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48334f8-0310-461b-a28d-786d3207227a",
   "metadata": {
    "id": "b48334f8-0310-461b-a28d-786d3207227a"
   },
   "source": [
    "Here are some examples of correct outputs that are not an exact match of the gold label, but where it still outputs a valid possible world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0b42e-2804-4357-974e-e30a48ab1ce8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "46c0b42e-2804-4357-974e-e30a48ab1ce8",
    "outputId": "218becc4-15ca-4684-c6d6-6b6e39301695"
   },
   "outputs": [],
   "source": [
    "df[~df.exact_match & df.semantically_correct].sort_values(by='input_size')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2eb899-35b5-4965-94c5-2a116f162279",
   "metadata": {
    "id": "ad2eb899-35b5-4965-94c5-2a116f162279"
   },
   "source": [
    "The two columns that we added contain information about all possible outputs for the input sentence. \n",
    "The `possible_worlds` column contains all possible (possibly partial) worlds for this sentence.\n",
    "The `subformula_valuations` column contains information about which subtrees can or cannot be true in the sentence. The string has the same length as the input sentence. \n",
    "\n",
    "When a subformula must always be true, it is mapped to `1`. When a subformula must always be false, it is mapped to `0`. Otherwise, it is mapped to `2`. Most subformulas have the value `2`, their truth values are contingent on the truth values of other subformulas.\n",
    "\n",
    "\n",
    "Take for instance the sentence:\n",
    "`| d & a d` (standard notation: `d | (a & d)`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb23ae-904b-4429-b139-3df101b90a1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31cb23ae-904b-4429-b139-3df101b90a1c",
    "outputId": "181daed8-e567-4135-a9fb-86782802bb7b"
   },
   "outputs": [],
   "source": [
    "df.iloc[15113]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2f04e-8731-4d92-8126-f668ef9baa27",
   "metadata": {
    "id": "23d2f04e-8731-4d92-8126-f668ef9baa27"
   },
   "source": [
    "The partial output here is `d 1`, but the model would have also been correct if it had outputted `a 0 d 1`, since that is also a possible world.\n",
    "\n",
    "The first subtree of the sentence is the root, so this will always be `1`\n",
    "For instance, `d` must always be true, which is why the second and last character in `subformula_valuations` are also `1`.\n",
    "Since the value of `a` can be either true or false (verify this by looking at the possible worlds), its `subformula_valuation` is `2`.\n",
    "This is also the case for `&`, since we can choose to make either `d` true, or `& a d`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc0b0f-13d6-409f-9860-7a47699f2658",
   "metadata": {},
   "source": [
    "#### 📝 Pen and Paper 2\n",
    "Take the following sentence. It has two possible worlds: calculate the possible worlds.\n",
    "Then, calculate what the `subformula_valuations` column looks like for this sentence. Which nodes in the tree should always be true, and which should always be false? Check your answers by printing the entire row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81b6b3-55b0-44d8-954e-6ed5a27f1c66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d81b6b3-55b0-44d8-954e-6ed5a27f1c66",
    "outputId": "a0ad225d-e2a9-4663-dc3b-b75cb1a10d27"
   },
   "outputs": [],
   "source": [
    "df.iloc[15612][['inputs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce29fbc-b9c7-44bd-bed8-bdb88d5842eb",
   "metadata": {
    "id": "2ce29fbc-b9c7-44bd-bed8-bdb88d5842eb"
   },
   "source": [
    "Since the model is forced to output just one world, it would be interesting to see if there is some information in the model about all these subtrees: has the model internalized when a subtree must be true or false, helping it along to a correct answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1994a13-7844-484a-b931-62f1c1f74223",
   "metadata": {
    "id": "c1994a13-7844-484a-b931-62f1c1f74223"
   },
   "source": [
    "## Probing for Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e47220-07ab-4770-a428-b8ff71c5c8af",
   "metadata": {
    "id": "57e47220-07ab-4770-a428-b8ff71c5c8af"
   },
   "source": [
    "### Probe task construction\n",
    "\n",
    "The `ProbeTask` class will help you with extracting the correct hidden states and labels. You don't have to fully understand the code. There is an example of how to use this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf318e-01e3-4ebf-8afd-8511d779ad29",
   "metadata": {
    "id": "06bf318e-01e3-4ebf-8afd-8511d779ad29"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 1000)\n",
    "    # Pandas also uses np random state by default\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # if you are using GPU\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class ProbeDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, hidden, ys):\n",
    "        self.hidden = hidden\n",
    "        self.ys = ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.hidden[idx], self.ys[idx]\n",
    "\n",
    "    \n",
    "class ProbeTask:\n",
    "    def __init__(self, hidden, df):\n",
    "        \"\"\"\n",
    "        Initialize a 'ProbeTask' object.\n",
    "        hidden: a dictionary of tensors of shape N x max_input_sentence_length x hidden_size.\n",
    "                The dictionary key is the layer in the encoder.\n",
    "        df: A pandas dataframe of N (or less) rows with relevant information about. \n",
    "            This will be used for sampling and for looking up indices of states in `hidden`.\n",
    "            Thus, the values in column `sentence_idxs` should correspond to the indices in the `hidden` obejct.\n",
    "        \"\"\"\n",
    "        self.hidden = hidden\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def create_datapoints_from_sentence(self, layer_no, sentence_idx, word_idxs):\n",
    "        \"\"\"\n",
    "        Transform a sentence into 1 or more datapoints. \n",
    "        layer_no: int\n",
    "        sentence_idx: int\n",
    "        word_idxs: [int] 0-based array of word indices. The BOS tag will be taken care of by self.get_hidden.\n",
    "        \"\"\"\n",
    "        state, flat_idx = self.get_hidden(layer_no, [(sentence_idx, np.array(word_idxs))])\n",
    "        return state\n",
    "        \n",
    "    def create_dataset(\n",
    "        self,\n",
    "        layer_no,\n",
    "        node_types=[\"a\",\"b\",\"c\",\"d\",\"e\"],\n",
    "        output_types=[0,1,2],\n",
    "        sents=2000,\n",
    "        max_tokens_per_sent=3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a dataset given the subformula_valuations column.\n",
    "        The dataset is balanced w.r.t. both the node types and the possible 'truth values' 0, 1 and 2.\n",
    "        \n",
    "        layer_no: Which layer to sample the hidden states from\n",
    "        node_types: Which node types to sample. For instance: all variables. It can also be a singleton list, eg [\"xor\"].\n",
    "        sents: amount of sentences to sample per node_type per valuation.\n",
    "        max_tokens_per_sent: how many nodes we are allowed to take from the same sentence. Allowing this makes your dataset bigger.\n",
    "        \n",
    "        Returns: a probedataset of size max (sents * max_tokens_per_sent * len(node_types) * 3)\n",
    "        In practice, it is shorter, since not all sentences contain max_tokens_per_sent relevant tokens.\n",
    "        \"\"\"\n",
    "        ys = []\n",
    "        xs = []\n",
    "        idxs_ = []\n",
    "        for node_type in node_types:\n",
    "            # Iterate over all possible \n",
    "            for val_id in output_types:\n",
    "                # Be sure to balance node types when a list is passed\n",
    "                idxs = self.get_all_idx(\n",
    "                    sents, max_tokens_per_sent, [node_type], str(val_id)\n",
    "                )\n",
    "                states, flat_idxs = self.get_hidden(layer_no, idxs)\n",
    "                xs.append(states)\n",
    "                ys.extend([val_id] * len(states))\n",
    "                idxs_.extend(flat_idxs)\n",
    "        xs = torch.cat(xs)\n",
    "        ys = torch.tensor(ys, dtype=torch.int64)\n",
    "        return ProbeDataSet(xs, ys), idxs_\n",
    "    \n",
    "    def get_hidden(self, layer_no, idx_list):\n",
    "        \"\"\"\n",
    "        idx_list is a list of (int, [int]) indicating the sentence position and the word position array\n",
    "        This function takes care of the BOS token by adding + 1 to the provided idxs.\n",
    "        \"\"\"\n",
    "        states = self.hidden[layer_no]\n",
    "        result = []\n",
    "        idxs_ = []\n",
    "        for idx, word_idxs in idx_list:\n",
    "            # +1 for BOS tag\n",
    "            result.append(states[idx][word_idxs + 1])\n",
    "            idxs_.extend([(idx, word_idx) for word_idx in word_idxs])\n",
    "        return torch.cat(result, dim=0), idxs_\n",
    "    \n",
    "    \n",
    "    def get_all_idx(\n",
    "        self,\n",
    "        sent_amt, # How many sentences to sample\n",
    "        per_sent_max_amt, # How many nodes are allowed in the same sentence\n",
    "        token_identities,  # abcde xor ! & , etc...\n",
    "        value: str,  # '0', '1' or '2'\n",
    "    ):\n",
    "        \"\"\"This method uses the subformula_truths column to fetch relevant pairs of (sentence_idx, word_idx).\n",
    "        \"\"\"\n",
    "        # Filter rows where a token of one of the provided token_identities has the desired value\n",
    "        # E.g. get only those indices where there exists some `a` with value `1`.\n",
    "        # Since the root node token *always* has value 1, we start at the second token.\n",
    "        def get_idxs(row, start=1):\n",
    "            idxs = []\n",
    "            for i, (token, truth) in enumerate(\n",
    "                zip(row[\"inputs\"].split()[start:], row[\"subformula_valuations\"][start:])\n",
    "            ):\n",
    "                if truth == value and token in token_identities:\n",
    "                    idxs.append(i + start)\n",
    "            return idxs\n",
    "\n",
    "        self.df[\"_condition\"] = self.df.apply(get_idxs, axis=1)\n",
    "        sents = self.df[self.df[\"_condition\"].apply(len) > 0]\n",
    "        \n",
    "        # Sample from all possible sentences\n",
    "        sents = sents.sample(n=min(sent_amt, len(sents)), replace=False)\n",
    "\n",
    "        result = []\n",
    "        # For each sentence, sample from all possible positions in the sentence\n",
    "        for i, row in sents.iterrows():\n",
    "            sent_idx = row[\"sentence_idxs\"]\n",
    "            word_idxs = row[\"_condition\"]\n",
    "            word_idxs = np.random.choice(\n",
    "                word_idxs, size=min(per_sent_max_amt, len(word_idxs)), replace=False\n",
    "            )\n",
    "            result.append((sent_idx, word_idxs))\n",
    "        # Remove column just in case\n",
    "        self.df = self.df.drop(\"_condition\", axis=1)\n",
    "        return result\n",
    "def get_stats(array):\n",
    "    \"\"\"Tool for printing the label distribution\"\"\"\n",
    "    return np.unique(array, return_counts=True)[1] / len(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b06bf9b-9fc3-405e-b566-381605b3d1de",
   "metadata": {},
   "source": [
    "#### Example of a probetask object\n",
    "\n",
    "The probetask `create_dataset` method samples from the subset of hidden states that are above tokens of a certain type. You can specify which types of nodes to collect with the `node_types` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41448d65-59db-4507-acb7-5c066bb91331",
   "metadata": {
    "id": "41448d65-59db-4507-acb7-5c066bb91331"
   },
   "outputs": [],
   "source": [
    "# Create a probetask object to sample from\n",
    "probetask_test = ProbeTask(stacked_hidden, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec052ad0-fb79-43a9-be24-dce40a61ccd7",
   "metadata": {
    "id": "ec052ad0-fb79-43a9-be24-dce40a61ccd7"
   },
   "outputs": [],
   "source": [
    "# Sample hidden states from layer 3 for 100 sentences per node_type per output_type\n",
    "# This may take a few seconds\n",
    "probe_dataset_test, idxs = probetask_test.create_dataset(layer_no=3, sents = 500, node_types=[\"a\",\"b\",\"c\",\"d\",\"e\"],\n",
    "                                                        output_types=[0,1,2], max_tokens_per_sent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38fd75-e30a-428f-bcbc-ac814184f0cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a38fd75-e30a-428f-bcbc-ac814184f0cf",
    "outputId": "0190e271-9211-42cf-9196-c302dea3a715"
   },
   "outputs": [],
   "source": [
    "# Check the output and verify it using the dataframe\n",
    "hidden_state, prediction_label = probe_dataset_test.hidden[0], probe_dataset_test.ys[0]\n",
    "sentence_idx, word_idx = idxs[0]\n",
    "print(\"dataset size\", len(probe_dataset_test))\n",
    "print(\"Hidden state size\", probe_dataset_test.hidden.shape)\n",
    "print(\"sentence idx\", sentence_idx, \"word idx\", word_idx)\n",
    "print(\"Token at position?\", df.iloc[sentence_idx].inputs.split()[word_idx])\n",
    "print(\"Valuation at position?\", df.iloc[sentence_idx].subformula_valuations[word_idx])\n",
    "print(\"Prediction label?\", prediction_label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfba98-797a-4a08-aa5d-5d17446fada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label distribution\")\n",
    "print(get_stats(probe_dataset_test.ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ff21f-bd25-49b5-8f5b-0b2f4ae1a1b9",
   "metadata": {
    "id": "c35ff21f-bd25-49b5-8f5b-0b2f4ae1a1b9"
   },
   "source": [
    "### Probe training\n",
    "Now that we know how to collect data for the probe, let's set up code to construct, train and evaluate a probing model.\n",
    "\n",
    "Instead of using a LogisticRegression from `sklearn`, I have provided a simple pytorch module called `Probe`, which is essentially just a wrapper around `nn.Linear`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d9be10-ef43-4ab4-b6b6-f8ff965cb742",
   "metadata": {},
   "source": [
    "#### ☑️ ToDo 1: Implement the training loop of the probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bb9b4-0d95-4828-a2de-11f756c101e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Probe(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A very simple linear model.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128, classes=3):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden_dim, classes)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        return self.linear(hidden_state)\n",
    "    \n",
    "def train_probe(probe, train_dataloader, epochs=10, lr=0.005, debug=False):\n",
    "    probe.train()\n",
    "    optim = torch.optim.Adam(probe.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        # TODO: implement a simple training loop given the dataloader, optimizer, probe and loss function\n",
    "        raise NotImplementedError()\n",
    "    probe.eval()\n",
    "    return probe\n",
    "    \n",
    "def eval_probe(probe, val_dataloader):\n",
    "    probe.eval()\n",
    "    ys_pred = np.zeros(len(val_dataloader.dataset))\n",
    "    ys_true = np.zeros(len(val_dataloader.dataset))\n",
    "    \n",
    "    i = 0\n",
    "    for inputs, labels in val_dataloader:\n",
    "        output = probe(inputs)\n",
    "        preds = torch.argmax(output, dim=-1)\n",
    "        ys_pred[i : i + len(labels)] = preds.detach().int().numpy()\n",
    "        ys_true[i : i + len(labels)] = labels.int().numpy()\n",
    "\n",
    "        i += len(labels)\n",
    "    return {\n",
    "            \"pred\": ys_pred,\n",
    "            \"true\": ys_true,\n",
    "            \"accuracy\": accuracy_score(ys_true, ys_pred),\n",
    "            \"f1_macro\": f1_score(ys_true, ys_pred, average=\"macro\"),\n",
    "            \"confusion_matrix\": confusion_matrix(ys_true, ys_pred, normalize='true')\n",
    "    }\n",
    "\n",
    "def random_split(dataset, train_size=.8):\n",
    "    \"\"\"Splits a ProbeDataSet into two datasets\"\"\"\n",
    "    train_size = int(len(dataset) * train_size)\n",
    "    test_size = len(dataset) - train_size\n",
    "    return torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "def split_dataset_into_dataloaders(probe_dataset, train_size=.8):\n",
    "    \"\"\"Splits a ProbeDataSet into two dataloaders\"\"\"\n",
    "    train_dataset, val_dataset = random_split(probe_dataset, train_size=train_size)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset , batch_size=256, shuffle=False)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced2d37-81f0-4b2b-a216-1ec7e941d760",
   "metadata": {
    "id": "5ced2d37-81f0-4b2b-a216-1ec7e941d760"
   },
   "source": [
    "### Training variable probes per layer\n",
    "\n",
    "We must first make some decisions about what data to use.:\n",
    "1. To remove noise, it is probably a good idea to only include datapoints where the model was correct.\n",
    "\n",
    "    * To think: what would happen if you included incorrect datapoints to the probe trainer? Could this also have benefits?\n",
    "\n",
    "2. As a first experiment, we can try to probe nodes that are variables. If a variable **must** be true, the model **must also output** that it is true: this information should therefore be kept track of for all sentences where the model is correct.\n",
    "\n",
    "3. Finally, we can choose which layer to probe. \n",
    "\n",
    "For evaluation of the probes: note that this is a 3-way classification task, so random accuracy is 33%. Some classes may have a higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2030ac-bbc7-45b0-ba97-48b3d93ed275",
   "metadata": {
    "id": "2f2030ac-bbc7-45b0-ba97-48b3d93ed275"
   },
   "outputs": [],
   "source": [
    "# Only include that part of the df where the model was correct\n",
    "p = ProbeTask(stacked_hidden, df[df.semantically_correct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ecd102-1024-4f08-bb56-559f85846822",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1ecd102-1024-4f08-bb56-559f85846822",
    "outputId": "69bce52a-e0d7-4a95-fc13-c6f056a670ae"
   },
   "outputs": [],
   "source": [
    "# Example code for training a probe:\n",
    "# Let's start with at least 1000 examples per node_type per truth type.\n",
    "# Print the distribution of labels to make sure the data is balanced\n",
    "layer_no = 4\n",
    "# Set seed for sampling data / training probe\n",
    "set_seed(1)\n",
    "probe_dataset, idxs = p.create_dataset(layer_no=layer_no, sents = 1000, node_types=[\"a\",\"b\",\"c\",\"d\",\"e\"])\n",
    "print(\"Label distribution\", get_stats(probe_dataset.ys))\n",
    "print(\"Total dataset size\", len(probe_dataset))\n",
    "train_dataloader, val_dataloader = split_dataset_into_dataloaders(probe_dataset)\n",
    "probe = train_probe(Probe(), train_dataloader)\n",
    "result = eval_probe(probe, val_dataloader)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a2f27-ba44-478d-bda4-bfa8000adff2",
   "metadata": {},
   "source": [
    "#### ☑️ ToDo 2 : probe variable nodes for all layers. \n",
    "\n",
    "Which layer gives the best performance and why do you think this is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa510b77-3f39-4613-99ae-945ab95b05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: train and evaluate variable-node-probes for each of the six layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8486f3-a9fb-438c-9a37-4d5b92f7457e",
   "metadata": {},
   "source": [
    "#### ✍️ ToSubmit 1: Visualize your probing results per layer.\n",
    "\n",
    "Give a brief description of the pattern you see, and why you might be seeing this.\n",
    "\n",
    "(It is enough to do this in the caption of the figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea4da49-2428-4567-b04f-c4f4b660eee7",
   "metadata": {},
   "source": [
    "#### 🧠 ToThink: \n",
    "- Given your results per layer, what are the limitations of probing for these `subformula_valuations` respective to the **entire** sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead449f-b7bd-47a0-aa82-fbc8c361b1fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Probing other types of nodes\n",
    "\n",
    "We have now only extracted hidden states of nodes at the positions of variables.\n",
    "\n",
    "However, other positions have the same property of always true/always false/contingent, as we have seen in exercise Pen & Paper 2 above.\n",
    "These nodes of other types can thus also be probed in the same way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d38f6e-c043-4fe5-b048-f18da285a5af",
   "metadata": {},
   "source": [
    "#### ☑️  ToDo 3: Run probes for non-variable/operator nodes.\n",
    "\n",
    "(I recommend training on **exactly one type of node** at the time, see next section. Be sure to increase the `sents` parameter to get a bigger dataset for just a single node type, but also pay attention to the label distribution and keep it balanced!)\n",
    "\n",
    "- Is the performance better or worse in general? \n",
    "\n",
    "- Is the pattern the same throughout the layers as with the variable nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa160b7f-55eb-482f-b7b8-747266bc7922",
   "metadata": {},
   "source": [
    "#### ✍️ ToSubmit 2: Add probing for your operator-node probe to your visualization for ToSubmit 1.\n",
    "\n",
    "Describe the pattern for these operator probes, and compare these results to your results in ToSubmit 1. Is there a difference, and what might be the reason for this difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c579ae7-23c8-4e38-b7cd-b66bd2df923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_no = 2\n",
    "set_seed(1)\n",
    "# 3000 sents gives a mostly balanced label distribution\n",
    "probe_dataset, idxs = p.create_dataset(layer_no=layer_no, sents = 3000, node_types=[\"&\"])\n",
    "print(\"Label distribution\",get_stats(probe_dataset.ys))\n",
    "print(\"Total dataset size\", len(probe_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c733da-3ea2-41eb-8fa9-2bb3a11c006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a06822-4069-4cc0-9bf6-086d3d92568a",
   "metadata": {},
   "source": [
    "### Do probes generalize?\n",
    "\n",
    "During our previous experiment we have assumed that data is encoded similarly for every node type in our dataset (every variable). \n",
    "However, it could be that our model has learned solutions that do not treat different variable types the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27819748-cbd6-44c9-9565-a443a9636701",
   "metadata": {},
   "source": [
    "#### ☑️ ToDo 4: train probes on one subset of node_types, then evaluate them on another subset as well as the original test set. \n",
    "E.g. train only on variable `a` (or `a` and `b`) and evaluate on all other variables. How does performance compare when testing on `a` versus other variables? \n",
    "Try multiple different subsets. Which nodes generalize to which other nodes?\n",
    "\n",
    "For this experiment, you can stick with a single layer (the best one from the previous exercise), or try multiple layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d92ef15-96b7-4952-b355-dee00a9c3fab",
   "metadata": {},
   "source": [
    "#### ✍️ ToSubmit 3: Include your findings in the report.\n",
    "Since you need to try multiple different subsets of train/test nodes, it is recommended to visualize this in a heatmap (similar to the cross-layer probing experiments from week 1).\n",
    "\n",
    "Briefly reflect on these results. Which probes generalize, which ones do not? Might this tell us anything about the way this Transformer is representing propositional logic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4298435-5d3f-4a12-9143-790d19a58092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to generate a dataset that is of another type of node:\n",
    "# probe_dataset_b_only, idxs = p.create_dataset(layer_no=5, sents = 5000, node_types=[\"b\"])\n",
    "# dataloader_b , _ = split_dataset_into_dataloaders(probe_dataset_b_only, 1.0) # Do not split\n",
    "# Now, you can pass dataloader_b to eval_probe with a probe that was trained on different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb96463-2378-4ea2-8b45-16f40bc4c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e4e75-b115-438a-b8e3-cc69769fb540",
   "metadata": {
    "id": "8f3e4e75-b115-438a-b8e3-cc69769fb540",
    "tags": []
   },
   "source": [
    "## Logical equivalences through the layers\n",
    "\n",
    "If the model truly understood the semantics of propositional logic, it should \"know about\" logical equivalences. For instance, '! xor' is equal in meaning to '<->'.\n",
    "\n",
    "We may be able to see this in the behaviour of the model, by comparing its outputs for sentences that are logically equivalent. Two sentences are logically equivalent if they have exactly the same possible worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec023a6-f63d-4703-99e1-22b83b88d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the full (non partial) possible worlds to a string so pandas can easily find all unique values\n",
    "df['fullworlds'] = df['possible_worlds'].apply(lambda x:' '.join(z for z in x if len(z)==max(map(len,x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807c9e7-2986-4297-ac5c-d01a0eae544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the maximum occuring equivalent sentences?\n",
    "df['fullworlds'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f8e96-513b-42a2-86f4-0f4e0511df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model outputs (but also the gold outputs) are not always the same for equivalent sentences\n",
    "df[df['fullworlds']=='b0c0 b0c1 b1c1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7a2da-69f2-4b44-8bc3-f3aeb0e52968",
   "metadata": {},
   "source": [
    "We see that the model does not always predict the same output for a logically equivalent sentence. Similarly, since there are only a relatively small number of possible outputs, the model will predict the same output for logically inequivalent sentences, as well. \n",
    "\n",
    "Maybe we can gain more insight into whether the model knows about logical equivalence by looking at its hidden representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22588d4-077c-4129-963e-98cb6b97a97b",
   "metadata": {},
   "source": [
    "### Creating our own small dataset\n",
    "\n",
    "The validation set contains mostly very long sentences, and not many sentences that are logically equivalent. \n",
    "\n",
    "Instead of sampling from the validation set, we can create our own small dataset to further inspect its behaviour on hand-constructed (non)equivalent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5b8c6-1b37-41da-8abe-a75080f82d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some pairs of logical equivalences for the input data\n",
    "# Student ToDo: add more inputs\n",
    "mini_data_inputs = [\"! <-> a b\", \"xor a b\"]\n",
    "# Corresponding correct outputs for completeness. Will not affect outputs\n",
    "mini_data_outputs = [\"a 1 b 0\", \"a 1 b 0\"]\n",
    "mini_dataset = LogicDataSet(\n",
    "    vocab_path=f\"transformer_logic_compact/vocabulary\",\n",
    "    data_inputs=(mini_data_inputs, mini_data_outputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d61f9e-81b0-40bd-9e34-773dbd7a750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the mini dataset in a dataloader and perform a forward pass\n",
    "mini_dataloader = DataLoader(mini_dataset, collate_fn=VarLengthCollate(batch_dim=1), batch_size=256)\n",
    "mini_states, mini_outputs = forward_all(model_interface, mini_dataloader, max_it=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ceb78-0cdf-4e8f-b85b-b5915a465e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705f2a4-1258-4ed6-8af9-ff90fdf375b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The root node is at position 1.\n",
    "position_to_compare = 1\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3,nrows=2,figsize=(16,6))\n",
    "axs = [ax for ax_array in axs for ax in ax_array]\n",
    "for ax, layer_no in zip(axs,range(6)):\n",
    "    cosines = sklearn.metrics.pairwise.cosine_similarity(mini_states[layer_no][:,position_to_compare,:])\n",
    "    sns.heatmap(cosines, annot=True, ax=ax)\n",
    "    ax.set_xticks(np.arange(len(mini_data_inputs))+0.5 ,mini_data_inputs,rotation=90)\n",
    "    ax.set_yticks(np.arange(len(mini_data_inputs))+0.5 ,mini_data_inputs,rotation=0)\n",
    "    ax.set_title(f'Layer {layer_no}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ab3ef-091d-469d-afd3-1848915e242e",
   "metadata": {},
   "source": [
    "#### ☑️ ToDo 5: Add at least 4 more inputs to the mini-dataset above and compare the similarity matrix through the layers\n",
    "\n",
    "Be sure to include equivalent inputs as well as non-equivalent inputs. \n",
    "\n",
    "It might be interesting to also include inputs that are 'almost equivalent'!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1777797-48a5-4e21-afee-7cbcfba5689d",
   "metadata": {},
   "source": [
    "#### 🧠 ToThink:\n",
    "I set `position_to_compare` to 1, meaning we compare the root node. Which side effects could this have? Can we see these effects through the layers?\n",
    "Could we also compare different positions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360ea2b-6fea-4d70-a06f-5938d64a5933",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### ✍️ ToSubmit 4: Include the plot above with (a subset of your) (non)-equivalent sentences.\n",
    "\n",
    "Reflect on at least these two questions:\n",
    "- What does similarity mean through the layers?\n",
    "- Which (non)equivalent sentences are more similar? Briefly describe at least two patterns you see.\n",
    "\n",
    "Note: Feel free to change the plotting code, or include multiple similarity plots, if this makes things clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da05304-d6b1-4a9f-9976-14ac18bd7eed",
   "metadata": {},
   "source": [
    "## ✅ <font color=green> End of notebook! </font>\n",
    "\n",
    "You have reached the end of the notebook - there are no more official ToDo's, but I have provided one more section below for interested students.\n",
    "\n",
    "I have spent a long time with these models during my thesis, so if you have (practical, existential, philosophical) questions about any of these experiments, you can always ask me (Anna) questions about it (for instance by [emailing me](mailto:annalangedijk@gmail.com)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c8025-a242-4067-8bae-cfea4a5d7d40",
   "metadata": {},
   "source": [
    "## Optional: Models trained on systematically different data\n",
    "\n",
    "I have also provided a model that is trained without the pattern `! xor`. Any `! xor` is replaced with `<->` during training, so the amount of training data stays (almost) equal.\n",
    "\n",
    "The model performs with equal performance on most sentences, including sentences with `xor` and ``<->`, but when `! xor` is present, the performance drops. The model seems to not be able to combine `!` with `xor`.\n",
    "\n",
    "Additionally, I have provided another model. This one is training without another pattern, namely `& xor`. Not only does `& xor` never appear (`xor` being the left subtree of the binary `&` node), but I have also filtered any occurences where `xor` is the right subtree of the `&` node, meaning the model has never seen a node `&` with a child `xor`, similar to the unary operator `!` in the previous model not having seen `xor` as a child.\n",
    "\n",
    "The model performs with almost equal performance to the original model, including on sentences containing the left-out pattern.\n",
    "\n",
    "How can we understand what this model is missing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f1bf70-b109-4904-a822-95eed22e68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the models to exactly the same validation set \n",
    "_, model_outputs_without_not_xor =  \\\n",
    "    load_hidden_and_outputs('model_without_not_xor_seed2', None, load_from_file=True, calculate_outputs = True, MAX_IT=100, \n",
    "                           calculate_hidden=False)\n",
    "\n",
    "_, model_outputs_without_and_xor =  \\\n",
    "    load_hidden_and_outputs('model_without_and_xor_seed1', None, load_from_file=True, calculate_outputs = True, MAX_IT=100,\n",
    "                           calculate_hidden=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce6728-fc49-430f-88b2-f13a675247c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_not_xor = calculate_df(model_outputs_without_not_xor, CUTOFF)\n",
    "df_without_and_xor = calculate_df(model_outputs_without_and_xor, CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372dd6c-3c8c-4371-a907-25dffdb1e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_and_xor['semantically_correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01629e80-27db-4ac9-9266-b6e5fc3ca571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_not_xor['semantically_correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62e081-c729-4f13-a910-cd125007a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_not_xor[~df_without_not_xor.semantically_correct].sort_values(by='input_size')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa77c9-9e34-4605-af25-49f9b8da5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_and_xor[df_without_and_xor['inputs'].str.contains('& xor')].sort_values(by='input_size',ascending=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c604a0cd-11c3-4739-b74e-188aa1cfe6f4",
   "metadata": {},
   "source": [
    "## Future questions / Ideas for projects\n",
    "\n",
    "- Are there differences in outputs/probing results/hidden state results for the models trained on systematically different data?\n",
    "- Can we probe for something else?\n",
    "- Can we compare this model to a (neuro)symbolic solver, or find an algorithm that the model is using internally?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "thesis2022",
   "language": "python",
   "name": "thesis2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
