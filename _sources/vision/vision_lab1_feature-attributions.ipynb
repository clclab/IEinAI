{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature attribution for explainable AI in vision <mark>Part 1</mark>\n",
        "\n",
        "In this tutorial, we will try to explain what is happening inside the CNN using feature attribution methods.\n",
        "\n",
        "**Feature attribution methods** highlight the features or pixels that were relevant for a certain image classification by a neural network.\n",
        "\n",
        "**Gradient-based:** Many methods compute the gradient of the prediction (or classification score) with respect to the input features. The gradient-based methods (of which there are many) mostly differ in how the gradient is computed.\n",
        "\n",
        "In this tutorial, you will apply gradient-based feature attribution:\n",
        "- **saliency mapping**: use gradients to understand what image pixels are most important for classification.\n",
        "- **GradCam** to understand what areas of the image are important for classification.\n",
        "- **integrated gradients** for the same purpose.\n",
        ":\n",
        "We are making use of the following PyTorch implementations:\n",
        "- Integrated Gradient blog: https://distill.pub/2020/attribution-baselines/\n",
        "- Axiomatic Attribution for Deep Networks: https://arxiv.org/pdf/1703.01365.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "AgRXFQex5XpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the environment\n",
        "\n",
        "We will first load the necessary python libraries.\n",
        "For google colab, we will connect to google drive. Please store the example files in the datafiles PATH."
      ],
      "metadata": {
        "id": "EDrvMrBS9tzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "datafiles = 'gdrive/MyDrive/Integrated gradient/' # The folder where the code/datasets and example images are located.\n",
        "print('data is in: ', datafiles)"
      ],
      "metadata": {
        "id": "A4fjsROu9Std"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set-up environment\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import os  # necessary\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow   # specific for colab"
      ],
      "metadata": {
        "id": "Dw1Oeb8A9pTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load a pretrained CNN model.\n",
        "\n",
        "We will make use of the VGG19 CNN network and ImageNet.\n",
        "\n",
        "- ImageNet is a large collection of images.\n",
        "- VGG19 is a convolutional neural network architecture.\n",
        "- We can load a version that is trained on ImageNet and that can detect objects in 1000 classes.\n",
        "\n",
        "- Read about and understand VGG ConvNet and Imagenet for background.\n",
        "\n",
        "The first step is that using the pytorch library, we load the pretrained version of VGG19.\n",
        "\n",
        "A Note on use of GPU. For more speed you may use some GPU, e.g. using Lisa. In order to make use to GPU you have to load your data in this specific mode.\n",
        "\n",
        "Since we will not train the model we set the model in evaluation mode.\n",
        "\n",
        "- Print and investigate the network structure of the VGG network.\n",
        "- Mention the three main architectural parts of this VGG network and describe short the purpose of these parts.\n",
        "- Describe shortly the required dimensions of the input (images) and the dimensions of the output (prediction)."
      ],
      "metadata": {
        "id": "GRRLxTme7YcH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2-l2E5hZqGR"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "# model_type = 'vgg19'\n",
        "model = models.vgg19(pretrained=True)\n",
        "\n",
        "# run it on a GPU if available:\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('cuda:', cuda, 'device:', device)\n",
        "model = model.to(device)\n",
        "\n",
        "# set model to evaluation\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load and preprocess the images\n",
        "\n",
        "Load an image. We have provided a few images on elephant and sharks, but please also use you own imagery.\n",
        "\n",
        "VGG-19 works best if image is normalised. Image should also be in the correct tensor format.\n",
        "\n",
        "--> ensure that input is in correct tensor format. Tip: you can use pytorch method like:\n",
        "\n",
        "input = torch.tensor(input, dtype=torch.float32, device=torch_device).\n",
        "\n",
        "(note: we will provide a few images)"
      ],
      "metadata": {
        "id": "T_X8F5iz_Mnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/gdrive/My Drive/IG/images/"
      ],
      "metadata": {
        "id": "gskF0w74wXmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing(obs, cuda):\n",
        "    # Students should transpose the image to the correct tensor format.\n",
        "    # Students should ensure that gradient for input is calculated\n",
        "    # set the GPU device\n",
        "    if cuda:\n",
        "        torch_device = torch.device('cuda:0')\n",
        "    else:\n",
        "        torch_device = torch.device('cpu')\n",
        "\n",
        "    # normalise\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n",
        "    obs = obs / 255\n",
        "    obs = (obs - mean) / std\n",
        "\n",
        "    # make tensor format that keeps track of gradient\n",
        "    # BEGIN for students to do\n",
        "    obs = np.transpose(obs, (2, 0, 1))\n",
        "    obs = np.expand_dims(obs, 0)\n",
        "    obs = np.array(obs)\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32, device=torch_device)\n",
        "    # END for students to do\n",
        "    return obs_tensor\n",
        "\n",
        "# read the image and convert it\n",
        "img = cv2.imread('elephant.jpg')\n",
        "\n",
        "# img = cv2.imread(datafiles+ 'shark/Shark1.jpeg')\n",
        "img = cv2.resize(img, (224, 224))\n",
        "img = img.astype(np.float32)\n",
        "img = img[:, :, (2, 1, 0)]\n",
        "print('img:', img.shape)"
      ],
      "metadata": {
        "id": "2IuO1GZAC3p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Predict class**\n",
        "\n",
        "We can now easily predict the class, and the softmax score of that prediction."
      ],
      "metadata": {
        "id": "Okakx_cZJy5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input, model, target_label_idx, cuda):\n",
        "    # Student should make prediction after preprocessing image\n",
        "    # Student should use softmax for getting predicion < 1.\n",
        "    # Note that output should be torch.tensor on cuda\n",
        "\n",
        "    output = model(input)\n",
        "    output = F.softmax(output, dim=1)            # calc output from model\n",
        "\n",
        "    if target_label_idx is None:\n",
        "      target_label_idx = torch.argmax(output, 1).item()\n",
        "    index = np.ones((output.size()[0], 1)) * target_label_idx\n",
        "    index = torch.tensor(index, dtype=torch.int64)\n",
        "    if cuda:\n",
        "      index = index.cuda()                     # calc prediction\n",
        "    output = output.gather(1, index)           # gather functionality of pytorch\n",
        "    return target_label_idx, output\n",
        "\n",
        "# test preprocessing\n",
        "input = pre_processing(img, cuda)          # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)\n",
        "output = predict(input, model, None, cuda)\n",
        "print('output:', output)"
      ],
      "metadata": {
        "id": "ijQSBDROKBFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 Visualise Saliency Map**.\n",
        "\n",
        "Now that we can predict the class of an object, we will try to understand which image pixels are the most important for the prediction using *feature Attribution XAI methods*. The first technique that we will make use of is the <mark>saliency maps</mark>. Briefly, this approach determines the gradient of the output with respect to the input.\n",
        "\n",
        "The idea of Saliency maps (called <em>Vanilla Gradient</em> as well), introduced by Simonyan et al. (https://arxiv.org/pdf/1312.6034.pdf) as one of the first pixel attribution approaches. The core idea is really simple and what needs to be done is to calculate the gradient of the loss function for the class we are interested in with respect to the input features. This gives us a saliency map of the same size with the input features with either negative to positive values.\n",
        "\n",
        "The recipe for this approach is as follows:\n",
        "- **Perform a forward pass** of the image ($\\mathbf{x}_0$) of interest using the network $\\mathcal{F}(\\mathbf{x}_0)$.\n",
        "- **Compute the gradient** of class score of interest with respect to the input pixels ($\\mathbf{x}_0$):\n",
        "\n",
        "$$g(x_o) = \\frac{\\partial \\mathcal{F}}{\\partial \\mathbf{x}_0} $$.\n",
        "- **Visualize the gradients**: You can either show the absolute values or highlight negative and positive contributions separately.\n",
        "\n",
        "**The intructions for the PyTorch code**:\n",
        "\n",
        "We have set the model in eval mode, but we can still catch the gradients of the input-image if we ask PyTorch to do this, and then do some backward calculation. So you need to perform the following steps:\n",
        "- Input should be preprocessed (and converted into a torch tensor).\n",
        "- Set the <mark>required_gradient=True</mark> on the input tensor.\n",
        "- Calculate the output (with previous method predict).\n",
        "- Set the gradient to zero and do a backward on the output.\n",
        "- Gradients with respect to the input can now be found under <mark>input.grad</mark>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vPfcweeA9Qc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTIVITY Fill the missing code"
      ],
      "metadata": {
        "id": "7VFIWm8AjGIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_outputs_and_gradients(inputs, model, target_label_idx, cuda=False):\n",
        "    # Students have to calculate the gradient of the output w.r.t. the input image\n",
        "    # The result should be a gradients numpy matrix of same dimensions as the inputs\n",
        "    predict_idx = None\n",
        "    gradients = []\n",
        "    for input in inputs:# for every image\n",
        "\n",
        "        input = pre_processing(input, cuda)\n",
        "        input.requires_grad=True\n",
        "        _, output = predict(input, model, target_label_idx, cuda)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # BEGIN students TODO\n",
        "        # Add the missing code for the backprop\n",
        "        # END students TODO\n",
        "\n",
        "        gradient = input.grad.detach().cpu().numpy()[0]  # do backward and gather gradients of input\n",
        "        gradients.append(gradient)\n",
        "    gradients = np.array(gradients)\n",
        "    return gradients, target_label_idx\n",
        "\n",
        "# calculate the gradient and the label index\n",
        "gradients, label_index = calculate_outputs_and_gradients([img], model, None, cuda)\n",
        "gradients = np.transpose(gradients[0], (1, 2, 0))\n",
        "\n",
        "print('gradients', gradients.shape)\n",
        "print(gradients[:3, :3, 0])"
      ],
      "metadata": {
        "id": "DKX_SN_oMIIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2 Visualise the saliency map (gradients)**\n",
        "\n",
        "Try to visualise the image and the saliency map.\n",
        "**Tip:** take absolute values of the gradients and maximize over all three channels."
      ],
      "metadata": {
        "id": "ymvb53obShXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the saliency map and also pick the maximum value from channels on each pixel.\n",
        "# In this case, we look at dim=2. Recall the shape of gradients (width, height, channel)\n",
        "\n",
        "def plot_gradients(img, gradients, title):\n",
        "  # plots image (dimensions: Width X Heigth X 3) and gradients (dimensions: Width X Heigh x 3) - both numpy arrays\n",
        "  saliency = np.max(np.abs(gradients), axis=2)       # takes maximum over 3 color channels\n",
        "  # Visualize the image and the saliency map\n",
        "  fig, ax = plt.subplots(1, 2)\n",
        "  ax[0].imshow(img/255)\n",
        "  ax[0].axis('off')\n",
        "  ax[1].imshow(saliency, cmap='hot')\n",
        "  ax[1].axis('off')\n",
        "  plt.tight_layout()\n",
        "  fig.suptitle(title)\n",
        "  plt.show()\n",
        "\n",
        "plot_gradients(img, gradients, 'The Image and Its Gradient Map')"
      ],
      "metadata": {
        "id": "lqdTojnn5qZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3 Problems with saliency maps and vanilla gradient (saturation issues)**.\n",
        "\n",
        "Vanilla Gradient methods, notoriously, are facing saturation problems, as explained in [Avanti et al. (2017)](https://arxiv.org/abs/1704.02685). When the ReLU is used, and when the activation goes below zero, then, the activation is limited at zero and does not change any more. Hence, the activation is saturated."
      ],
      "metadata": {
        "id": "lEJKvezoXleb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4 Grad Cam**\n",
        "\n",
        "Unlike saliency maps, in the **Grad-Cam** approach the gradient is not backpropagated all the way back to the image, but (usually) to the last convolutional layer in order to generate a visualization map that highlights important regions of the input.\n",
        "\n",
        "A naive visualization approach could be the following:\n",
        "\n",
        "- Simply employ the values for each feature map, (of the last convolutional layer),\n",
        "- Then, average these feature maps and overlay this over our image (by rescaling back to initial size).\n",
        "\n",
        "However, while simple, it is not really helpful approach, since these maps encode information for all classes, while we are interested in a specific class. **Grad-CAM** needs to figure out the importance for each of the $k$ feature map $A_k \\in \\mathbb{R}^{w \\times h}$ ($w$ the width and $h$ the height of the features maps) in respect to our class $c$ of interest.\n",
        "\n",
        "Therefore, we have to weight each pixel of each feature map with the gradient before averaging over the feature maps $A_k$. This calculated heatmap is send through the ReLU function which set all negative values to zero. The reason for that is that we are only interested in the parts that contribute to the selected class $c$ and not to other classes. The final feature map is rescaled back to the original image size. We then overlay it over the original image for producing the final visualization.\n",
        "\n",
        "**Grad Cam recipe:**\n",
        "\n",
        "- Forward-propagate the input image $\\mathbf{x}_0$ through the convolutional VGG19 network by calculating the $\\mathcal{F}(\\mathbf{x}_0)$.\n",
        "- Obtain the score for the class of interest, that means the activation before the softmax layer.\n",
        "- All the rest classes' activations should be set to zero.\n",
        "- Back-propagate the gradient of the class of interest to the last convolutional layer before the fully connected layers:\n",
        "\n",
        "$$\\frac{\\partial y_{c}}{\\partial A^k}$$\n",
        "\n",
        "- Weight each feature map <em>pixel</em> by the gradient for the class. Indices $i$ and $j$ refer to the width and height dimensions:\n",
        "\n",
        "$$\\alpha^{c}_{k} = \\overbrace{\\frac{1}{Z} \\sum_i \\sum_j}^{\\text{global averaging pooling}} \\underbrace{\\frac{\\partial y_{c}}{\\partial A^{k}_{ij}}}_{\\text{gradients of the backpropagation}}$$\n",
        "\n",
        "This means that the gradients are globally pooled.\n",
        "\n",
        "- Calculate an average of the feature maps, weighted per pixel by backpropagated gradient.\n",
        "- Apply ReLU to the averaged feature map.\n",
        "\n",
        "$$  L_{ij}^c = ReLU \\sum_k \\alpha^{c}_{k} A^{k}_{ij}$$\n",
        "\n",
        "We now have a heatmap $L^c$ for the class $c$.\n",
        "\n",
        "- Regarding the visualization: Scale values of the $L^c$ to the interval between 0 and 1. Upscale the image and overlay it over the original image.\n",
        "\n",
        "In our classification example this approach uses the activation map of the final convolutional layer (with VGG: the final features layer). Note that such an Activation Map can be a block of $14 \\times 14 \\times 512$, where the $14 \\times 14$ indicated a grid on the image (noted by subscripts i and j) and the 512 is the number of channels (features, noted by the letter k). **Grad Cam** pools the Activation Map over the channels, and it gives a weight equal to the contribution of each channel to the prediction. This contribution of each channel is calculated by taking the gradient of the output w.r.t. the Activation Map and then pool this over the spacial ($14\\times14$) dimensions.\n",
        "\n",
        "For the calculation of the gradient w.r.t the Activation Map we need a little PyTorch trick since this gradient cannot be accessed by default. The PyTorch trick is called a <em>hook</em>. We can register a hook on a tensor of the network. With a hook we can define a little program that is executed when the tensor is touched during a backward pass. In our case we register a hook on the Activation Map we want to study and that is the 36th layer of the VGG19 convolutional <em>features</em> layer. The hook needs to be registered during a forward pass, so we will redefine the forward pass for our model.\n",
        "\n",
        "\n",
        "There is a nice youtube tutorial on pytorch and hooks https://www.youtube.com/watch?v=syLFCVYua6Q. (22 minutes but I think it is worth it)\n"
      ],
      "metadata": {
        "id": "x-eGFLqiCBgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1  Define a new VGG() model including a hook**\n",
        "\n",
        "The VGG() class is based on the pretrained models.vgg19 that we have already load before.\n",
        "\n",
        "The <mark>Activation Map</mark> that we want to study should be defined in the init function. That is the output of the first 36 feature layers.\n",
        "\n",
        "In the <mark>activations_hook</mark> method we should define our hook that will store the gradient calculated on the tensor in <mark>self.gradients</mark>.\n",
        "\n",
        "In the forward pass, we should execute all VGG layers <em>by hand</em>. The hook is registered on the output of the first 36 feature layers. And then the remaining layers are defined.\n",
        "\n",
        "Fill the code procedures for accessing the activation maps and gradients w.r.t. these activation maps.\n",
        "\n",
        "When defined, we load this model, move it to our GPU if available and put the model in eval mode."
      ],
      "metadata": {
        "id": "dZ0icy7TZAe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTIVITY Fill the code"
      ],
      "metadata": {
        "id": "exquV6nQjbwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        # get the pretrained VGG19 network\n",
        "        self.vgg = models.vgg19(pretrained=True)\n",
        "        # self.vgg = model\n",
        "\n",
        "        # disect the network to access its last convolutional layer\n",
        "        self.features_conv = self.vgg.features[:36]\n",
        "\n",
        "        # get the max pool of the features stem\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "\n",
        "        # get the classifier of the vgg19\n",
        "        self.classifier = self.vgg.classifier\n",
        "\n",
        "        # placeholder for the gradients\n",
        "        self.gradients = None\n",
        "\n",
        "    # hook for the gradients of the activations: it stores the calculated grad (on our tensor) in self.gradients.\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward(self, x):\n",
        "        # gives the output of the first 36 'feature' layers\n",
        "        x = self.features_conv(x)\n",
        "\n",
        "        # register the hook (note: h is a handle, giving the hook a identifier, we do not use it here)\n",
        "        h = x.register_hook(self.activations_hook)\n",
        "\n",
        "        # apply the remaining pooling\n",
        "        x = self.max_pool(x)\n",
        "        x = x.view((1, -1))\n",
        "\n",
        "        # apply the remaining classifying\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # method for the gradient extraction\n",
        "    def get_activations_gradient(self):\n",
        "\n",
        "        # BEGIN students TODO\n",
        "        return NotImplemented\n",
        "        # END students TODO\n",
        "\n",
        "    # method for the activation exctraction\n",
        "    def get_activations(self, x):\n",
        "\n",
        "        # BEGIN students TODO\n",
        "        return NotImplemented\n",
        "        # END students TODO\n",
        "\n",
        "vgg = VGG(model)\n",
        "print('cuda:', cuda, 'device:', device)\n",
        "vgg = vgg.to(device)\n",
        "vgg.eval()"
      ],
      "metadata": {
        "id": "r20pxKijjGwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Now calculate the gradients of a prediction w.r.t. the activation map.\n",
        "\n",
        "We should perform a prediction with our newly defined model <mark>vgg</mark>, and perform a backward on the output (the logit of the prediction vector that is largest). After the backward, the gradients w.r.t the activation map are stored in self.gradient"
      ],
      "metadata": {
        "id": "ynn-DTnJbXLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the most likely prediction of the model\n",
        "\n",
        "input = pre_processing(img, cuda)          # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)\n",
        "print(input.shape)\n",
        "\n",
        "pred, output = predict(input, vgg, None, cuda)\n",
        "print(pred, output)                        # prediction gives class 2 = shark, or 386 = elephant\n",
        "                                           # maybe work with softmax here too"
      ],
      "metadata": {
        "id": "dMyU9-U_Bzij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the gradient of the output with respect to the parameters of the model\n",
        "output.backward()"
      ],
      "metadata": {
        "id": "e2LfLJcDLby8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTIVITY - calculate the pooled gradient for each channel and return it in a variable <mark>pooled_gradients</mark>"
      ],
      "metadata": {
        "id": "CGWgn9kDnDw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pull the gradients out of the model\n",
        "gradients = vgg.get_activations_gradient()\n",
        "print('gradients:', gradients.shape)\n",
        "\n",
        "# pool the gradients across the channels\n",
        "\n",
        "# BEGIN TODO\n",
        "# Calculate the gradient weint and return it in a variable pooled_gradients\n",
        "# END TODO\n",
        "\n",
        "print('pooled gradients:', pooled_gradients.shape)\n",
        "\n",
        "# get the activations of the last convolutional layer\n",
        "activations = vgg.get_activations(input).detach()\n",
        "\n",
        "# weight the channels by corresponding gradients\n",
        "for i in range(512):\n",
        "    activations[:, i, :, :] *= pooled_gradients[i]\n",
        "\n",
        "# average the channels of the activations\n",
        "heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "\n",
        "# relu on top of the heatmap\n",
        "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
        "# heatmap = np.maximum(heatmap, 0)\n",
        "heatmap = torch.maximum(heatmap, torch.tensor(0))\n",
        "\n",
        "# normalize the heatmap\n",
        "heatmap /= torch.max(heatmap)\n",
        "\n",
        "print('heatmap:', heatmap.shape)\n",
        "\n",
        "# draw the heatmap\n",
        "heatmap = heatmap.cpu().numpy().squeeze()\n",
        "plt.matshow(heatmap)"
      ],
      "metadata": {
        "id": "pXIWENPpCU3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw the image\n",
        "print('img:', img.shape)\n",
        "plt.matshow(img/255)\n"
      ],
      "metadata": {
        "id": "QMjahJa8NcC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(img.shape, heatmap.shape)\n",
        "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "print(img.shape, img.min(), img.max())\n",
        "print(heatmap.shape, heatmap.min(), heatmap.max())\n",
        "\n",
        "super_img = heatmap * 0.4 + img * 0.6\n",
        "super_img = np.uint8(super_img)\n",
        "plt.matshow(super_img)"
      ],
      "metadata": {
        "id": "KMZmffN7JcF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Integrated gradient**\n",
        "\n",
        "Path-attribution methods compare the current image to a reference image, which can be an artificial <em>zero</em> image such as a completely black image or random noise image.\n",
        "\n",
        "Let us consider the linear path between the baseline $\\mathbf{x}^{'}$ (black image or random noise) and our input image $\\mathbf{x}$ and compute all the points along this path using variable $a$. Integrated gradients (IG) are obtained by cumulating these gradients between the initial image and the gradual difference between the initial image with the baseline.\n",
        "\n",
        "Specifically, IG is defined as the path integral of the gradients along the straightline path from the baseline $\\mathbf{x}^{'}$ to the input $\\mathbf{x}$.\n",
        "\n",
        "The integral gradient for the $k^{\\text{th}}$ for an input $\\mathbf{x}$ and baseline $\\mathbf{x}^{'}$ is defined as:\n",
        "\n",
        "$$\n",
        "IG_k(\\mathbf{x}) =  \\underbrace{({x}_i - {x}^{'}_i)}_{\\text{Difference from baseline}} \\overbrace{\\int_{a=0}^{1} \\frac{\\partial \\mathcal{F}( {\\mathbf{x}}^{'} + a\\times (\\mathbf{x} -\\mathbf{x}^{'}) )}{\\partial x_i} \\,da}^{\\text{Accumulated  local gradients}}\n",
        "$$\n",
        "\n",
        "The baseline $\\mathbf{x}^{'}$  represents the <mark>absence</mark> of a feature input.\n",
        "\n",
        "If we have a closer look in the above equation, IG accumulates gradients on images interpolated between the baseline value and the current input.\n",
        "\n",
        "**But why would doing this make sense?** Recall that the gradient of a function represents the direction of maximum increase. The gradient is telling us which pixels have the steepest local slope with respect to the output. For this reason, the gradient of a network at the input was one of the earliest saliency methods.\n",
        "\n",
        "But using only the gradients for the visualization can lead to saturation problems. The gradients of input features may have small magnitudes around a sample even if the network depends heavily on those features. This can happen if the network function flattens after those features reach a certain magnitude. Intuitively, shifting the pixels in an image by a small amount typically doesnâ€™t change what the network sees in the image.\n",
        "\n",
        "Hence, in IG, we really want to know is how our network got from predicting essentially nothing at $\\mathbf{x}^{'}$ to being completely saturated towards the correct output class at $\\mathbf{x}$. Which pixels, when scaled along this path, most increased the network output for the correct class? This is exactly what the formula for integrated gradients gives us.\n",
        "\n",
        "By integrating over a path, integrated gradients avoids problems with local gradients being saturated. We can break the original equation down and visualize it in three separate parts:\n",
        "\n",
        "- the interpolated image between the baseline image and the target image,\n",
        "- the gradients at the interpolated image,\n",
        "- and accumulating many such gradients over $\\alpha$.\n",
        "\n",
        "Fortunately, instead of calculating the integral of integrated gradients we can approximate it via a summation. We can calculate it through the following equation:\n",
        "\n",
        "$$\n",
        "IG_k(\\mathbf{x}) =  ({x}_i - {x}^{'}_i) \\times  {\\sum_{K=1}^{m} \\frac{\\partial F( \\mathbf{x}^{'} + \\frac{k}{m}\\times (\\mathbf{x} - \\mathbf{x}^{'}) )}{\\partial x_i} \\times \\frac{1}{m}}\n",
        "$$"
      ],
      "metadata": {
        "id": "M1G9baHHQpnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.1 Calculate Integrated Gradients recipe**.\n",
        "\n",
        "Read first the literature on Integrated Gradients.\n",
        "\n",
        "Instructions for students:\n",
        "  - **Choose a baseline image**. For this purpose we choose a black image.\n",
        "  - **Build a series of inputs to IG**, each input consists of the baseline plus an additional fraction of the input image. The final IG input is the baseline plus the full image. Choose your fraction at 20.\n",
        "  - For each of these inputs, **calculate the gradients of the input** w.r.t. the prediction (using pre-defined methods). Take the average of all these gradients.\n",
        "  - **Calculate the difference of image and baseline**: I-B. And calculate Integrated Gradient = (I-B)*average of gradients.\n",
        "  - If you have chosen another baseline, e.g. <em>uniform random</em> generated baseline, then perform this procedure for multiple samples.\n",
        "\n"
      ],
      "metadata": {
        "id": "1rv0JvRFMD_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# integrated gradients\n",
        "def integrated_gradients(inputs, model, target_label_idx, baseline, steps=50, cuda=False):\n",
        "\n",
        "    # determine baseline\n",
        "    if baseline is None:\n",
        "        baseline = 0 * inputs\n",
        "    # scale inputs and compute gradients\n",
        "    scaled_inputs = [baseline + (float(i) / steps) * (inputs - baseline) for i in range(0, steps + 1)]\n",
        "    grads, _ = calculate_outputs_and_gradients(scaled_inputs, model, target_label_idx, cuda)\n",
        "    avg_grads = np.average(grads[:-1], axis=0)\n",
        "    avg_grads = np.transpose(avg_grads, (1, 2, 0))\n",
        "    delta_X = (pre_processing(inputs, cuda) - pre_processing(baseline, cuda)).detach().squeeze(0).cpu().numpy()\n",
        "    delta_X = np.transpose(delta_X, (1, 2, 0))\n",
        "    integrated_grad = delta_X * avg_grads\n",
        "    return integrated_grad\n",
        "\n",
        "def random_baseline_integrated_gradients(inputs, model, target_label_idx, steps, num_random_trials, cuda=False):\n",
        "    # when baseline randomly generated, take some samples and average result\n",
        "    all_intgrads = []\n",
        "    random_baseline = 255.0 * np.random.random(inputs.shape)\n",
        "    for i in range(num_random_trials):\n",
        "        integrated_grad = integrated_gradients(inputs, model, target_label_idx, baseline=random_baseline, steps=steps, cuda=cuda)\n",
        "        all_intgrads.append(integrated_grad)\n",
        "        print('the trial number is: {}'.format(i))\n",
        "    avg_intgrads = np.average(np.array(all_intgrads), axis=0)\n",
        "    return avg_intgrads\n",
        "\n",
        "# calculate the integrated gradients\n",
        "print('img:', img.shape, 'label_index', label_index)\n",
        "\n",
        "# for zero baseline\n",
        "int_gradients_zerobl = integrated_gradients(img, model, label_index, baseline=None, steps=50, cuda=cuda)\n",
        "print('DONE')\n",
        "# for random baselines, we average over number of trials\n",
        "int_gradients_randombl = random_baseline_integrated_gradients(img, model, label_index, steps=50, num_random_trials=5, cuda=cuda)\n",
        "print('DONE')"
      ],
      "metadata": {
        "id": "07yrdQeHyOWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Visualise the integrated gradients**"
      ],
      "metadata": {
        "id": "1fIjogsT22Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise overall\n",
        "\n",
        "# calculate gradients\n",
        "gradients, _ = calculate_outputs_and_gradients([img], model, None, cuda)\n",
        "gradients = np.transpose(gradients[0], (1, 2, 0))\n",
        "\n",
        "# combine it all in one image\n",
        "plot_gradients(img, gradients, 'The Image and Its Gradient Map')\n",
        "plot_gradients(img, int_gradients_zerobl, 'Image and Integrated Gradients')\n",
        "plot_gradients(img, int_gradients_randombl, 'Image and Integrated Gradients with Random Baseline')\n",
        "\n",
        "## By the way - I do not trust the results very much - integrated gradients seem to focus on pixels just above the elephants\n"
      ],
      "metadata": {
        "id": "tpbry09l-X9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework-part 1 <mark>TOSUBMIT</mark>**.\n",
        "\n",
        "Work for home:\n",
        "\n",
        "\n",
        "    1. Read the main article for IG\n",
        "    2. Why do integrated gradients give better results than the other saliency methods?\n",
        "    3. Think about the role of the baseline and how it negatively impacts the results.\n",
        "    4. Which alternative baselines are more efficient than the black or random baseline? Write down the reasons for that and show some visual examples (show the result from two methods in comparison with the random and black baseline).\n",
        "\n",
        "   <mark>TOSUBMIT</mark> At the first paragraph of your report you should analyze this and answer 2-4. Then, add the requested image!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y-1Cjm2QlofO"
      }
    }
  ]
}