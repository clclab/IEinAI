{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature attribution for explainable AI in vision (LIME) <mark>Part 2</mark>\n",
        "\n"
      ],
      "metadata": {
        "id": "M5PTVoUL72F5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Everything Is AWESOME](https://i3.ytimg.com/vi/6NPvTcJOGE0/maxresdefault.jpg)](https://www.youtube.com/watch?v=6NPvTcJOGE0&ab_channel=DeepLearningIIE \"How the XAI methods works\")\n",
        "\n",
        "In this post, we will study how\n",
        "<mark style=\"background-color:LavenderBlush;\">LIME</mark> (Local\n",
        "Interpretable Model-agnostic Explanations) (\\[1\\]) generates\n",
        "explanations for image classification tasks. The basic idea is to\n",
        "understand why a machine learning model predicts that a specific image\n",
        "belongs to a certain class (<em>Caretta-caretta</em> in our visual example).\n",
        "Briefly, this technique constructs a <em>new</em> simple model (for\n",
        "example a <mark style=\"background-color:LavenderBlush;\"><em>linear\n",
        "classifier</em></mark>) which is easy to be interpreted by humans and at the same time approximates\n",
        "the predictions of the <em>black-box</em> model in the neighborhood\n",
        "around the instance that needs to be explained (<em>local faithfulness\n",
        "</em>).\n",
        "\n",
        "The <mark style=\"background-color:LavenderBlush;\">LIME</mark> explainer is <em>model-agnostic</em> which means is not\n",
        "restricted to a specific model and can be used to explain any\n",
        "<mark>black-box</mark> classifier. So we donâ€™t need to have access to\n",
        "the details of our model (input, intermediate layers etc) to generate\n",
        "explanations. Moreover, the explainer is <em>local</em> meaning that it\n",
        "explains the prediction of the model in the neighborhood of the instance\n",
        "being explained. This technique lies in the PostHoc category of XAI\n",
        "methods, since it explains the behavior of the approach after the training process is performed.\n",
        "\n",
        "## Interpretable Representations\n",
        "\n",
        "An interpretable explanation in an image classifier explainer should use\n",
        "a representation that is understandable to humans, by explaining which\n",
        "parts of the input image influence the model decision. For instance, the\n",
        "pixel-based representations are not very informative especially when we\n",
        "deal with huge images and therefore a better way to explain the model\n",
        "decision is to use\n",
        "[super-pixels](https://infoscience.epfl.ch/record/149300) (\\[3\\]). Super-pixels\n",
        "are groups of pixels that share similar characteristics such as color\n",
        "and texture. Hence, a possible interpretable representation for image\n",
        "classification may be a binary vector indicating the <em>presence</em>\n",
        "or <em>absence</em> of a super-pixel.\n",
        "\n",
        "Thus, our explainer needs to find a way to attribute importance to each\n",
        "super-pixel in the initial input image. Itâ€™s important to note here,\n",
        "that the interpretable representations are meant to be just for the\n",
        "<mark style=\"background-color:LightCyan;\">LIME</mark> explainer while the <em>black-box</em> can still be trained using the\n",
        "original pixel-based representations.\n",
        "\n",
        "<mark style=\"background-color:LightCyan;\">LIME</mark> approach aims to\n",
        "just explain why the classifier took a specific decision upon a specific\n",
        "input image. It does not aim to explain the whole model. Authors, in the\n",
        "paper, proposed a mechanism called\n",
        "<mark style=\"background-color:LightCyan;\">SP-LIME</mark> that aims to\n",
        "explain the whole model. While we will not touch this method in this\n",
        "tutorial we encourage you to have a look at it in the original paper.\n",
        "\n",
        "## LIME approach details\n",
        "\n",
        "To explain how <mark style=\"background-color:LavenderBlush;\">LIME</mark> works in detail we should introduce some definitions and maths\n",
        "ðŸ˜Ž.\n",
        "\n",
        "Hence, let $\\mathbf{x} \\in R^{d}$ denote the original vector\n",
        "representation of an instance being explained (in our case a vector with\n",
        "all pixels in the image), and we use $\\mathbf{x}^{\\prime} \\in \\{0, 1\\}^d$ to denote a\n",
        "binary vector for its interpretable representation (super-pixels).\n",
        "\n",
        "The <mark style=\"background-color:LavenderBlush;\">LIME</mark> explainer is defined as (or explanation model) $g \\in G$, where $G$\n",
        "is a class of potentially interpretable models, such as <em>linear\n",
        "models</em>, <em>decision trees</em> etc. To keep things simple, in this\n",
        "tutorial, we will consider just the <em>linear classifier</em> case. As not\n",
        "every $g \\in G$ may be simple enough to be interpretable $\\Omega(g)$ is\n",
        "defined as a measure of complexity (in juxtaposition with the\n",
        "interpretability) of the explanation $g \\in G$. For example, for\n",
        "decision trees $\\Omega(g)$ may be the depth of the tree, while for\n",
        "linear models, $\\Omega(g)$ may be the number of non-zero weights. We\n",
        "define as $f: R^{d} \\to R$ the <em>black-box</em> model that we would\n",
        "like to explain. In classification, $f(\\mathbf{x})$ is the probability\n",
        "(or a binary indicator) that $\\mathbf{x}$ belongs to a certain class.\n",
        "\n",
        "We further use $\\pi_{\\mathbf{x}}(\\mathbf{z})$ as a proximity measure\n",
        "between an instance $\\mathbf{z}$ to $\\mathbf{x}$, so as to define\n",
        "locality around $\\mathbf{x}$. Finally, let\n",
        "$\\mathcal{L}(f, g, \\pi_{\\mathbf{x}})$ be a measure of how unfaithful $g$\n",
        "is in approximating $f$ in the locality defined by $\\pi_{\\mathbf{x}}$.\n",
        "To ensure both <em>interpretability</em> and <em>local fidelity</em>,\n",
        "they must minimize $L(f, g, \\pi_{x})$ while having $\\Omega(g)$ be as low\n",
        "as possible. This will keep the complexity of the explanation low while\n",
        "maintaining the fidelity of the explanation high.\n",
        "\n",
        "Hence, the loss function for the <mark style=\"background-color:LavenderBlush;\">LIME</mark> explaner is the following equation:\n",
        "\n",
        "$$\\xi(\\mathbf{x}) = \\mathcal{L}(f, g, \\pi_{\\mathbf{x}}) + \\Omega(g) $$\n",
        "\n",
        "The first term $\\mathcal{L}(f, g, \\pi_{\\mathbf{x}})$ in the equation is represented by the weighted square loss:\n",
        "\n",
        "$$\\mathcal{L}(f, g, \\pi_{\\mathbf{x}}) = \\sum_{\\mathbf{z}, \\mathbf{z}^{'}}\\pi_{\\mathbf{x}}(\\mathbf{z})(f(\\mathbf{z})- g(\\mathbf{z}^{'}))^{2} $$\n",
        "\n",
        "with $\\pi_{\\mathbf{x}}$ to be a kernel function that measures the proximity of $z$ to $x$:\n",
        "\n",
        "$$\\pi_{\\mathbf{x}} =  \\exp(-D(\\mathbf{x},\\mathbf{z})^{2}/\\sigma*{2})$$\n",
        "\n",
        "The idea is that by tuning the weights $\\mathbf{w}$ we can use them\n",
        "directly as a feature attribution to each super-pixel. The higher the\n",
        "weight that corresponds to a specific super-pixel the more important\n",
        "this super-pixel is for the prediction of the <em>black-box</em> model\n",
        "and vice-versa.\n",
        "\n",
        "Its important to remind you here the terms <em>faithfulness</em>\n",
        "and <em>local fidelity</em> which are about how well our explainer\n",
        "$g$ can approximate the decision of the <em>black-box</em> model $f$ in\n",
        "the locality defined by $\\pi_{\\mathbf{x}}$.\n",
        "\n",
        "The whole <mark style=\"background-color:LavenderBlush;\">LIME</mark> algorithm can be summarized as follows:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1_Ax1JnqFGF6mHh8buyHPvfG4CWm-XEzR)\n",
        "\n",
        "\n",
        "The <mark style=\"background-color:LavenderBlush;\">kernel</mark> shows the proximity between the instance that we desire to explain and the\n",
        "generated samples in the neighborhood. The neighborhood is created by\n",
        "<mark style=\"background-color:LightCyan;\">sampling</mark> instances\n",
        "around the initial image. The sampling is done by perturbing\n",
        "the instance being explained. For example, in the case of images, we can\n",
        "perturb the image by zeroing out some super-pixels. The perturbed\n",
        "instances are then fed to the <em>black-box</em> model and the output is\n",
        "used to train the explainer. The weights of the interpretable model are\n",
        "then used to explain the prediction of the <em>black-box</em> model.\n",
        "Finally, in the algorithm,\n",
        "<mark style=\"background-color:Lavender;\">K-lasso</mark> refers to the\n",
        "regulizarion that is introduced in a previous equation and relates with\n",
        "the term $\\Omega(g)$.\n",
        "\n",
        "<center>\n",
        "<video autoplay muted loop controls src=\"files/LIME3.mp4\" style=\"width:600px\" type=\"video/mp4\">\n",
        "</video>\n",
        "<figcaption>\n",
        "A demonstration of the whole <mark style=\"background-color:LavenderBlush;\">LIME</mark> algorithm.\n",
        "</figcaption>\n",
        "</center>\n",
        "\n",
        "[![Everything Is AWESOME](https://i3.ytimg.com/vi/eLsB-aMjqfA/maxresdefault.jpg)](https://youtu.be/eLsB-aMjqfA \"How the XAI methods works\")\n",
        "\n",
        "\n",
        "The above video explains the whole <mark style=\"background-color:LavenderBlush;\">LIME</mark> process. The initial surface\n",
        "represents the <mark style=\"background-color:Lavender;\">black-box</mark>\n",
        "classifier and the regions for the class of interest\n",
        "(e.g.Â Caretta-caretta with the light-pink color). The dark red-colored\n",
        "dot denotes the sample that we would like to explain and it is an image\n",
        "with the label <em> Caretta-caretta</em>. The first step is to sample\n",
        "the neighborhood of the point $\\mathbf{x}$ that we would like to\n",
        "explain. Several points are generated. The size of each generated sample\n",
        "and the transparency relates to the distance from the initial point\n",
        "$\\mathbf{x}$ which is calculated based using\n",
        "$\\pi_{\\mathbf{x}}(\\mathbf{z})$. The next step is to apply the\n",
        "<mark style=\"background-color:Lavender;\">black-box</mark> classifier\n",
        "$f()$ to find the label for each generated point. Samples with red\n",
        "represent the class caretta-caretta while samples with purple represent\n",
        "the adversary class (not Caretta-caretta ;). The next step is to train the\n",
        "interpretable model $g()$ using the generated samples. The weights of\n",
        "the interpretable model are used to explain the prediction of the\n",
        "<mark style=\"background-color:Lavender;\">black-box</mark> classifier.\n",
        "\n",
        "\n",
        "## Code implementation\n",
        "\n",
        "To begin with, we will need to import the required libraries. The code is\n",
        "written in Python 3.6.9 and PyTorch 1.7.0. Before importing the libraries we need to mount the Google drive folder using the following command:"
      ],
      "metadata": {
        "id": "gC5A4S1_8Cdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "FMSv-frWEGQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/My Drive/LIME/files/"
      ],
      "metadata": {
        "id": "UW-KoF7CNfWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "iOLIsPCENqWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os, json\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "SeMIRB788TVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization of a pre-trained VGG19 model\n",
        "\n",
        "The very first thing that we will do is load a pre-trained VGG19 model.\n",
        "This model will be used to classify images and we will try to explain\n",
        "its behavior. The output of the model is a vector of 1000 probabilities\n",
        "belonging to each class from the ImageNet dataset. The model is\n",
        "initialized and the weights are loaded. The model is set to evaluation\n",
        "mode. The model is set to run on GPU if available. You can do that on\n",
        "google colab by enabling the GPU option. The steps for that are the\n",
        "following: <mark>Edit -\\> Notebook settings -\\> Hardware accelerator -\\>\n",
        "GPU</mark>. The code for all these steps is the following:"
      ],
      "metadata": {
        "id": "5oEkE5CN92sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "# model_type = 'vgg19'\n",
        "model = models.vgg19(pretrained=True)\n",
        "\n",
        "# run it on a GPU if available:\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('cuda:', cuda, 'device:', device)\n",
        "model = model.to(device)\n",
        "# set model to evaluation\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "_SCK5Nnf938H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignore for now the warnings! This code should return the architecture of\n",
        "the VGG19 model. Of course, feel free to choose the model of your choice\n",
        "(the code should work with any model). Now letâ€™s load and process the\n",
        "image (for the VGG19 classifier) that we would like to test our\n",
        "<mark style=\"background-color:Lavender;\">LIME</mark> explainer. You can\n",
        "freely choose the image that you would like to explain."
      ],
      "metadata": {
        "id": "5ttHczzo-clN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imread_img(file_name):\n",
        "\n",
        "  # read the image and convert it - Set your pathto the image\n",
        "  img = cv2.imread(file_name)\n",
        "  if (type(img) is np.ndarray):\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img = img.astype(np.float32)\n",
        "    img = img[:, :, (2, 1, 0)]\n",
        "    print('img:', img.shape)\n",
        "  else:\n",
        "    print('image not found - set your path to the image')\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "T8srUy9N-dQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do make use of the\n",
        "<mark style=\"background-color:Lavender;\">OpenCV</mark> library to read\n",
        "the <mark style=\"background-color:Lavender;\">caretta.png</mark> image from <mark>Canvas files</mark>.\n",
        "You can check all the provided images or download one from the web.\n",
        "\n",
        "Note that you could do the same by using torchvision datasets and\n",
        "transforms. We will show an example of that when we will use the\n",
        "<mark style=\"background-color:Lavender;\">LIME</mark> explainer for our\n",
        "lab exercise at the end of this tutorial.\n",
        "\n",
        "### Image pre-processing\n",
        "\n",
        "As usual, we will need to normalize our input image. The normalization\n",
        "is done using the mean and standard deviation of the ImageNet dataset.\n",
        "The image is also transposed to the correct tensor format:"
      ],
      "metadata": {
        "id": "QGxzXWUP-iYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_processing(obs, cuda):\n",
        "    # Students should transpose the image to the correct tensor format.\n",
        "    # Students should ensure that gradient for input is calculated\n",
        "    # set the GPU device\n",
        "    if cuda:\n",
        "        torch_device = torch.device('cuda:0')\n",
        "    else:\n",
        "        torch_device = torch.device('cpu')\n",
        "\n",
        "    # normalise for ImageNet\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\n",
        "    obs = obs / 255\n",
        "    obs = (obs - mean) / std\n",
        "\n",
        "    # make tensor format that keeps track of gradient\n",
        "    obs = np.transpose(obs, (2, 0, 1))\n",
        "    obs = np.expand_dims(obs, 0)\n",
        "    obs = np.array(obs)\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32, device=torch_device)\n",
        "    return obs_tensor"
      ],
      "metadata": {
        "id": "96t40MbG-niz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do the same (resizing, normalization and conversion to tensor) by\n",
        "using the torchvision transform:\n",
        "\n",
        "\n",
        "``` python\n",
        "transform = transforms.Compose([transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224,0.225])])\n",
        "\n",
        "```\n",
        "\n",
        "Then, the next that we will do is load the image and preprocess it. We\n",
        "will also check the prediction of the VGG19 model. Note that the\n",
        "prediction is correct (E.g. 33 and 34 are â€˜caretta-carettaâ€™ and\n",
        "â€˜turtleâ€™). The code is the following:"
      ],
      "metadata": {
        "id": "nQ3p7Klt-rYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input, model, target_label_idx, cuda):\n",
        "    # Makes prediction after preprocessing image\n",
        "    # Note that output should be torch.tensor on cuda\n",
        "    output = model(input)\n",
        "    output = F.softmax(output, dim=1) # calc output from model\n",
        "    if target_label_idx is None:\n",
        "      target_label_idx = torch.argmax(output, 1).item()\n",
        "    index = np.ones((output.size()[0], 1)) * target_label_idx\n",
        "    index = torch.tensor(index, dtype=torch.int64)\n",
        "    if cuda:\n",
        "      index = index.cuda()                     # calc prediction\n",
        "    output = output.gather(1, index)           # gather functionality of pytorch\n",
        "    return target_label_idx, output\n",
        "\n",
        "# test preprocessing\n",
        "# you can check that the VGG network gives a correct prediction.\n",
        "img = imread_img(\"caretta_1.jpg\")\n",
        "input = pre_processing(img, cuda)          # preprocess: image (normalise, transpose, make tensor on cuda, requires_grad=True)\n",
        "print (input.shape)\n",
        "label, output = predict(input, model, None, cuda)\n",
        "\n",
        "print('output:', output)\n",
        "print('output label:', label)"
      ],
      "metadata": {
        "id": "GQAYR3FV-6Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code helps you to get the label of the prediction. The\n",
        "label is the index of the class in the ImageNet dataset. The index is\n",
        "used to get the class name from the <mark>.json</mark> file. The\n",
        "<mark>.json</mark> file is available in the repository.\n",
        "\n",
        "``` python\n",
        "idx2label, cls2label, cls2idx = [], {}, {}\n",
        "with open(os.path.abspath('imagenet_class_index.json'), 'r') as read_file:\n",
        "    class_idx = json.load(read_file)\n",
        "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
        "    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}\n",
        "    cls2idx = {class_idx[str(k)][0]: k for k in range(len(class_idx))}\n",
        "```"
      ],
      "metadata": {
        "id": "JP7PC0Z9--Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIME explanation\n",
        "\n",
        "The following figure illustrates the basic idea behind LIME. The figure\n",
        "shows light pink and blue areas which are the decision boundaries for\n",
        "the classifier (for the VGG19 pre-trained model on ImageNet for\n",
        "instance). <mark style=\"background-color:Lavender;\">LIME</mark> can\n",
        "provide explanations for the predictions of an individual instance (the\n",
        "one with the dark red dot). These explanations are created by generating\n",
        "a new dataset of perturbations around the instance to be explained (in\n",
        "our image are depicted with dot circles around the initial instance).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1iaIiUisB7vmyy6e_Jc2eAvXH7G5bgy1f)\n",
        "\n",
        "Then, we apply our\n",
        "<mark style=\"background-color:Lavender;\">black-box</mark> model\n",
        "<mark style=\"background-color:Lavender;\">$f()$</mark> and we can extract\n",
        "the label for all the perturbations (which can be seen with dark red and\n",
        "denotes turtle with purple to denote non-turtle). The\n",
        "<em>importance</em> of each perturbation is determined by measuring its\n",
        "distance from the original instance to be explained.\n",
        "<mark style=\"background-color:Lavender;\"> These distances are\n",
        "converted to weights by mapping the distances to a zero-one scale using\n",
        "a kernel function ($\\pi_{\\mathbf{x}}$ </mark>). All this information:\n",
        "the newly generated dataset, its class predictions and its weights are\n",
        "used to fit a simple model, such as a linear model (gray line), that can\n",
        "be interpreted. The coefficients for this model are extracted and used\n",
        "as the explanation for the prediction of the instance we want to\n",
        "explain. The higher the coefficient, the more important the feature is\n",
        "for the prediction.\n"
      ],
      "metadata": {
        "id": "8lCa0qAN--2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Perturbations of image\n",
        "\n",
        "In the case of image explanations, our perturbations will be generated\n",
        "by zeroing out some of the superpixels in the image.\n",
        "\n",
        "#### Extract super-pixels from the image\n",
        "\n",
        "The superpixels are generated using the\n",
        "<mark style=\"background-color:Lavender;\">quickshift</mark> segmentation\n",
        "algorithm. This algorithm is provided by the\n",
        "<mark style=\"background-color:Lavender;\">skimage.segmentation</mark>\n",
        "library. The amount of superpixesls depends on the\n",
        "<mark style=\"background-color:Lavender;\">quickshift</mark> parameters\n",
        "(<mark style=\"background-color:Lavender;\">kernel_size</mark> and\n",
        "<mark style=\"background-color:Lavender;\">max_dist</mark>). The code\n",
        "snippet can be found below:"
      ],
      "metadata": {
        "id": "pzJfFsZNPxDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage.io\n",
        "import skimage.segmentation\n",
        "\n",
        "superpixels = skimage.segmentation.quickshift(img/255, kernel_size=2, max_dist=200, ratio=0.2) #it returns an image with all the super-pixel regions\n",
        "num_superpixels = np.unique(superpixels).shape[0]\n",
        "num_superpixels\n",
        "\n",
        "imgplot = plt.imshow(skimage.segmentation.mark_boundaries(img/255, superpixels))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LqNJ0PRw_Kc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated superpixels for the input Caretta-caretta image are shown\n",
        "in the image above. Note that you can improve the result of the approach\n",
        "or use another method for creating the superpixels.\n",
        "\n",
        "### Creating random perturbations\n",
        "\n",
        "In this example, 14 super-pixels were used. However, for real-life\n",
        "applications, a larger number of super-pixels will produce more\n",
        "reliable explanations.\n",
        "\n",
        "Having extracted the super-pixels, the way that perturbations are calculated is the following: Random\n",
        "zeros and ones are generated and shaped as a matrix with perturbations\n",
        "as rows and superpixels as columns. An example of a perturbation (the\n",
        "first one) is shown below. Here, `1` represents that a superpixel is on\n",
        "and `0` represents it is off. Notice that the length of the shown vector\n",
        "corresponds to the number of superpixels in the image. By running this\n",
        "code:"
      ],
      "metadata": {
        "id": "jbDkmIsq_Np7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_perturb = 150\n",
        "perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n",
        "perturbations[0] #Show example of perturbation"
      ],
      "metadata": {
        "id": "0iT9u71H_RjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function `perturb_image` perturbs the given image (`img`)\n",
        "based on a perturbation vector (`perturbation`) and predefined\n",
        "superpixels (`segments`)."
      ],
      "metadata": {
        "id": "eslSFFt9_TVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTIVITY add your code here"
      ],
      "metadata": {
        "id": "NQUF-6TRmuy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perturb_image(img,perturbation,segments):\n",
        "  active_pixels = np.where(perturbation == 1)[0]\n",
        "  mask = np.zeros(segments.shape)\n",
        "  # BEGIN YOUR CODE\n",
        "  return NotImplemented\n",
        "  # END YOUR CODE"
      ],
      "metadata": {
        "id": "zDxlcYhS_XWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s use the previous function to see what a perturbed image would look like:"
      ],
      "metadata": {
        "id": "J9kRtpjQ_cbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skimage.io.imshow(perturb_image(img/255,perturbations[0],superpixels))"
      ],
      "metadata": {
        "id": "q0S6AlGJ_fLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Use ML classifier to predict classes of newly generated images\n",
        "\n",
        "This is the most computationally expensive step in LIME because a\n",
        "prediction for each perturbed image is computed. We aim to construct a label for each perturbation. From the shape of the\n",
        "predictions, we can see for each of the perturbations we have the output\n",
        "probability for each of the 1000 classes."
      ],
      "metadata": {
        "id": "y-oiSK4d_hkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "for pert in perturbations:\n",
        "  perturbed_img = perturb_image(img,pert,superpixels)\n",
        "  input = pre_processing(perturbed_img, cuda)\n",
        "  output, label = predict(input, model, None, cuda)\n",
        "\n",
        "  output = model(input)\n",
        "  output = F.softmax(output, dim=1)\n",
        "  print (output.shape)\n",
        "  target_label_idx = torch.argmax(output, 1).item()\n",
        "\n",
        "  predictions.append(output.detach().numpy())\n",
        "\n",
        "predictions = np.array(predictions)\n",
        "predictions.shape"
      ],
      "metadata": {
        "id": "k5NwLrGC_lxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we will need to calculate the distances between the perturbations\n",
        "and the original image. The distances are calculated using the cosine\n",
        "distance. The smaller the distance, the more similar the vectors are.\n",
        "The distances are then converted to weights using a kernel function."
      ],
      "metadata": {
        "id": "-W_4v_cS_q19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled\n",
        "\n",
        "print(original_image)\n",
        "distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n",
        "distances.shape"
      ],
      "metadata": {
        "id": "mx428xHK_s6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have shown before, after calculating the distances we need to apply\n",
        "kernels to use them in the loss function. The employed can is as\n",
        "follows:"
      ],
      "metadata": {
        "id": "qjj-XaKH_uiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_width = 0.25\n",
        "weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function, this is the author implementation and slightly different than the one in the paper\n",
        "weights.shape"
      ],
      "metadata": {
        "id": "5oqrTBJl_u-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last two cells actually are calculating the proximity of the pertubations with the original image given the following recipe:\n",
        "\n",
        "$$\\pi_{\\mathbf{x}} =  \\exp(-D(\\mathbf{x},\\mathbf{z})^{2}/\\sigma*{2})$$"
      ],
      "metadata": {
        "id": "0kXnpEwq_xK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end, what we would like to do is to train a linear classifier and use the calculated parameters of the linear classifier as a explanation of our image. Firstly, let's check the first five classes of the prediction once again!"
      ],
      "metadata": {
        "id": "7ApHeaEE4VAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(input)\n",
        "print (out.shape)\n",
        "out = F.softmax(out, dim=1)\n",
        "\n",
        "out, indices= torch.sort(out, descending=True)\n",
        "\n",
        "top_values = out[:, :5] # Keep the first 5 values from each row\n",
        "top_indices = indices[:, :5]   # Keep the corresponding indices\n",
        "\n",
        "top5 = torch.topk(out, k=5)\n",
        "\n",
        "topk_values = top_values.detach().numpy()\n",
        "topk_indices =  top_indices.detach().numpy()\n",
        "\n",
        "print(topk_values)\n",
        "print(topk_indices)"
      ],
      "metadata": {
        "id": "5Q7qDaFc_xhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now train the linear model using the perturbations and the\n",
        "predictions with the highest probability. The extracted coefficients (or\n",
        "the explainer model weights) of the linear model are the explanations\n",
        "for the predictions."
      ],
      "metadata": {
        "id": "88o0uhw7_1sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTIVITY add your code"
      ],
      "metadata": {
        "id": "futNupzAnGLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simpler_model = LinearRegression()\n",
        "\n",
        "# BEGIN YOUR CODE\n",
        "# add the parameters for the linear model simple_model.fit()\n",
        "#END YOUR CODE\n",
        "\n",
        "coeff = simpler_model.coef_[0]\n",
        "coeff"
      ],
      "metadata": {
        "id": "E590AWbD_3AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now visualise the top features that the classifier used to make\n",
        "its prediction. The top features are the superpixels that have the\n",
        "highest coefficients."
      ],
      "metadata": {
        "id": "kqVJFBlt_4Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_top_features = 2\n",
        "top_features = np.argsort(coeff)[-num_top_features:]\n",
        "top_features"
      ],
      "metadata": {
        "id": "Uuudnraz_5VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having calculated the top features, we can now create a mask that will\n",
        "highlight the top superpixels in the image."
      ],
      "metadata": {
        "id": "tg4ryDjU_618"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.zeros(num_superpixels)\n",
        "mask[top_features]= True #Activate top superpixels\n",
        "skimage.io.imshow(perturb_image(img/255,mask,superpixels) )"
      ],
      "metadata": {
        "id": "07xzZDyt_8jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab exercise <mark>TOSUBMIT</mark>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This part of the tutorial is a lab exercise where you will use LIME to\n",
        "debias a classifier. The goal is to understand how LIME can be used to\n",
        "explain the behavior of a classifier and how we can use this information\n",
        "to debias the classifier.\n",
        "\n",
        "The classifier $f()$ (you can find the trained model on Canvas zip folder) is trained to classify images of <em>huskies</em> and\n",
        "<em>wolves</em>. We have zero knowledge of the training dataset and training details.\n",
        "\n",
        "We hope that we will be able to identify the biases in the classifier and\n",
        "try to debias it using LIME explainer.\n",
        "\n",
        "We will need to explain the behavior of the classifier\n",
        "using images from huskies and wolves. You should make use also the\n",
        "images that you can find on the Canvas page of the course or from google\n",
        "colab page. If you want you can use also your own husky and wolf images.\n",
        "\n",
        "Firstly, we will load the classifier and the images we want to explain.\n",
        "The following code checks the performance of the classifier in a test\n",
        "set."
      ],
      "metadata": {
        "id": "36Z8d_YU_-6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model, device):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device)\n",
        "            y = y.to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "\n",
        "            print (\"prediction \"+ str(predictions) + \" label \" + str(y))\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')"
      ],
      "metadata": {
        "id": "d_aHx6T2AEEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will load the classifier, load the images and check the\n",
        "performance of the classifier."
      ],
      "metadata": {
        "id": "j6EFkKxIAHED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('hus_wolf_model.pth', map_location=device)"
      ],
      "metadata": {
        "id": "fIcgHg4mAHjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code loads the images and creates the dataloader. The\n",
        "dataloader is used to load the images in batches. The images are\n",
        "preprocessed using the same transformations that were used to train the\n",
        "classifier. Note that the following code is the same as the one used in\n",
        "the previous part of the tutorial for preprocessing images."
      ],
      "metadata": {
        "id": "3ibxFGMfAMLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the transformation for all the images\n",
        "transform = transforms.Compose([transforms.Resize(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224,0.225])])\n",
        "\n",
        "# Load the images\n",
        "dataset = datasets.ImageFolder(\"LIME_test_files/\", transform=transform) # check if this path is correct\n",
        "\n",
        "dataset.classes = ['husky','wolf']\n",
        "dataset.class_to_idx = {'husky':0,'wolf':1}\n",
        "dataset.samples = list(filter(lambda s: s[1] in [0,1], dataset.samples))\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
        "\n",
        "# Check the accuracy of the classifier\n",
        "check_accuracy(test_dataloader, model, device)"
      ],
      "metadata": {
        "id": "6uV9GyQeAOFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the accuracy of the method in the test set?\n",
        "\n",
        "By using the LIME algorithm established\n",
        "in previous cells, you should explain the behavior of the classifier.\n",
        "You should use the provided images from zip folder <mark>LIME_test_files</mark> folder.\n",
        "<mark style=\"background-color:Lavender;\"> Add your conclusions about the\n",
        "model $f()$</mark>. What are the main issues with this classifier?\n",
        "\n",
        "Add your conclusions as a second paragraph in your report. Moreocer, add also a grid-image with the result of the LIME explainer (applied to all the images in the <mark>LIME_test_files</mark> folder) together with the labels and the predictions."
      ],
      "metadata": {
        "id": "v23vDrKiAWM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You should not trust me! <mark>TOSUBMIT</mark>\n",
        "\n",
        "Now you feel like a <em>hacker</em> and you would like to fool LIME\n",
        "explanations by constructing adversarial classifiers. To do that we will\n",
        "follow the process that is described in the following paper \\[2\\]. You\n",
        "can find a detailed also explanation in the following video:\n",
        "\n",
        "[<span class=\"image placeholder\"\n",
        "original-image-src=\"../../../assets/img/2023-05-13-LIME/fooling_LIME.PNG\"\n",
        "original-image-title=\"\">You should not trust\n",
        "me!</span>](https://www.youtube.com/watch?v=qCYAKmFFpbs&ab_channel=MLExplained-AggregateIntellect-AI.SCIENCE \"You should not trust me!\")\n",
        "\n",
        "  This part of the tutorial is the third part of the assignment you need to\n",
        "  <mark>TOSUBMIT</mark>. The goal is to understand the limitations of LIME\n",
        "  and how we can take advantage of them to perform adversarial attacks.\n",
        "  The main focus is to understand the way that LIME generates samples (the\n",
        "  perturbations and the distribution of the perturbations in comparison\n",
        "  with the real data).\n",
        "\n",
        "  The setup is the following:\n",
        "\n",
        "  As an adversary hacker, you are intending to deploy the biased\n",
        "  classifier $f$ for making a <em>critical decision</em> (e.g., confusing\n",
        "  huskies with wolves ðŸ˜±) in the real world. The adversary must provide\n",
        "  black box access to customers and examiners, who may use post hoc\n",
        "  explanation techniques to better understand $f$ and determine if $f$ is\n",
        "  ready to be used in the real world. If customers and regulators detect\n",
        "  that $f$ is biased, they are not likely to approve it for deployment.\n",
        "  The goal of the adversary is to fool post hoc explanation techniques and\n",
        "  hide the underlying biases of $f$.\n",
        "\n",
        "  In this assignment, you need to illustrate the difference between the distribution of real samples and the perturbations. Find a way to illustrate that in an example of your choice. Find a way to plot that using the provided data! Think of way based on the provided paper!\n",
        "\n",
        "  <mark>Optionally</mark>, you should create a scaffolded classifier $e$ that\n",
        "  behaves exactly like $f$ when making predictions on instances sampled\n",
        "  from $X_{dist}$ (real samples) but will not reveal the underlying biases of $f$ when probed with leading post hoc explanation techniques such as LIME. That could give you some extra points in case of other mistakes in this assignment!\n",
        "\n",
        "  To do that you will need to follow the steps of the suggested paper. You\n",
        "  need to find ways to differentiate the real data from the generated data\n",
        "  and based on that create the scaffolded classifier $e$ that will fool\n",
        "  LIME.\n",
        "\n",
        "  For this assignment, you will need to read the proposed paper and think of ways to perform adversarial attacks using the provided model. Add in your report an explanation of the procedure and the image with the difference between the 2 distributions. Optionally you can add examples of how the classifier $e$ can work in our case.\n",
        "\n",
        "## Guidelines for the Canvas submissions:\n",
        "\n",
        "- You should prepare a report based on researching the <mark>TOSUBMIT</mark> from both notebooks.\n",
        "- This report should contain three paragraphs. One will be about Integrated gradient and the questions that can be found at the end of the first notebook, one for the LIME explanations using the provided model, and finally one about adversarial attacks (an image with the distributions).\n",
        "- Optionally, you should add one paragraph with the results of a scaffolding classifier that can fool LIME and some visual results with that."
      ],
      "metadata": {
        "id": "qY21juQTRH1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Journal club for weeks 2 and 3\n",
        "- [TCAV: Interpretability Beyond Feature Attribution](https://arxiv.org/pdf/1711.11279.pdf)\n",
        "- [Transformer_Interpretability_Beyond_Attention_Visualization](https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf)\n"
      ],
      "metadata": {
        "id": "utUZ0FUUsOv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-project ideas\n",
        "\n",
        "This part of the tutorial relates to ideas about extending parts of this tutorial. One point of criticism of the LIME technique is the way that the perturbations are taking place since the distributions between real and generated data are significantly different.\n",
        "- Think of ways to improve the gap (using alternative ways to perform perturbations).\n",
        "     - For instance, you can make use of generative models or counterfactual techniques.\n",
        "     - You can make use of evolution techniques.\n",
        "     - Perturbation methods: you can have a look at some perturbation methods in this [survey paper](https://www.sciencedirect.com/science/article/pii/S0167865521002440).\n",
        "     - Improving the stability of LIME using [this paper as inspiration](https://dl.acm.org/doi/pdf/10.1145/3447548.3467274).\n",
        "- Think of ways to improve the feature repressions (replacing super-pixels).\n",
        "- Propose an evaluation scheme and compare the results of different feature attribution methods for vision.\n",
        "- Think of alternative baselines for the Integrated methods."
      ],
      "metadata": {
        "id": "s6Nn-DPxsyks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "In this tutorial, we have analyzed LIME a posthoc XAI technique. An\n",
        "explanation of how this technique works but also step-by-step the code\n",
        "to implement it. We have also seen how we can use LIME to explain\n",
        "image-classifiers but also how to identify the bias in a classifier."
      ],
      "metadata": {
        "id": "oIfvQKoThpQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "[[1] Ribeuro, M.T. et al. Why Should I Trust You? Explaining the Predictions of Any Classifier, 2016. SIGKDD.](https://arxiv.org/pdf/1602.04938.pdf)\n",
        "\n",
        "\n",
        "[[2] Slack, D. et al. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods, 2020, AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society](https://arxiv.org/pdf/1911.02508.pdf)\n",
        "\n",
        "[[3] Vevaldi, A. et al. Quick Shift and Kernel Methods\n",
        "for Mode Seeking, 2008, ECCV](https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi08quick.pdf)"
      ],
      "metadata": {
        "id": "Y_zI9GpDrswZ"
      }
    }
  ]
}