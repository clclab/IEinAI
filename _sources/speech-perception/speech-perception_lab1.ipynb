{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop Week 1: Introduction to Posthoc Interpretability\n",
        "\n",
        "*Interpretability & Explainability in AI, MSc A.I., University of Amsterdam, June 2022*"
      ],
      "metadata": {
        "id": "3XLhXl3TFAYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab 2: Probing Audio Models\n",
        "\n",
        "In the previous notebook, you have trained probes on the hidden states of a fine-tuned RoBERTa model, to gain insights into how sentiment is represented in this model across layers. In this notebook, you will perform a similar analysis on an **audio-based language model: Wav2Vec2**. This self-supervised model learns powerful speech representations from raw audio data and can be applied to many downstream tasks, including **Automatic Speech Recognition (ASR)**.\n",
        "\n",
        "Before the rise of deep learning, ASR was performed using a pipeline of different components, each performing a subpart of the task. The ASR pipeline includes the following steps:\n",
        "\n",
        "1.   **Feature extraction**: extracting relevant time-frequency information from the raw audio signal.\n",
        "2.   **Acoustic modelling**: mapping the extracted audio features to a sequence of [phonemes](https://prowritingaid.com/phoneme), the smallest meaningful units in speech. Phonemes only refer to sounds and do not necessarily match with written letters. For example, consider the words *fit*, *phone*, and *laugh*. These words all contain the same phoneme: /f/. Since the same phoneme can be written in several different ways (*f*, *ph*, *gh*), it is useful for an ASR model to have an intermediate phoneme representation.\n",
        "3.   **Language modelling**: mapping the sequence of phonemes to a sequence of written words.\n",
        "\n",
        "Self-supervised speech models such as Wav2Vec2 are able to perform all of the above steps in an **end-to-end** fashion. In this assignment, we will try to find evidence for the implicit execution of the second subcomponent: acoustic modelling. We will therefore be probing the layers of the model for **phoneme information**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Ba5m7S8FVWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "ZoQY7_is6z0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't forget to enable the GPU runtime at the top! (Runtime -> Change runtime type)"
      ],
      "metadata": {
        "id": "Ut40VsH467Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary packages:"
      ],
      "metadata": {
        "id": "tJJsKUFx7FKk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdy8n02g6Ns6"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==1.18.3 # we need this specific version of datasets in order to load the TIMIT data\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to the GPU:"
      ],
      "metadata": {
        "id": "1cuttG2Z7V8j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1n7MwzG6aIk"
      },
      "outputs": [],
      "source": [
        "from tqdm import *\n",
        "import torch\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set a random seed for reproducibility of the experiments:"
      ],
      "metadata": {
        "id": "7bezz47I7Zo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set random seed.\"\"\"\n",
        "    if seed == -1:\n",
        "        seed = random.randint(0, 1000)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # if you are using GPU\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "5OyVKunvlp1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The TIMIT dataset"
      ],
      "metadata": {
        "id": "NZpSYALrovHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the [TIMIT Acoustic-Phonetic Continuous Speech Corpus](https://catalog.ldc.upenn.edu/LDC93S1), which contains sentence recordings of 630 speakers of eight major American-English dialects. Each speaker read aloud the same ten sentences, which were designed to elicit a wide variety of speech sounds. The corpus includes time-aligned transcriptions, as well as the raw waveform for each spoken sentence, sampled at a rate of 16 kHz."
      ],
      "metadata": {
        "id": "XgCQYE9d3xhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToDo1**</font>\n",
        "\n",
        "Load the TIMIT corpus by running the cell below."
      ],
      "metadata": {
        "id": "FQ1Ou6eIpMXa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3wJdbmK7AoM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "timit = load_dataset(\"timit_asr\", \"clean\").shuffle(seed=42)\n",
        "\n",
        "# Print number of train and test samples\n",
        "print(len(timit['train']), len(timit['test']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToThink1**</font>\n",
        "\n",
        "Look at some examples of the data and try to understand the annotations.\n",
        "\n",
        "What information is annotated in ```phonetic_detail?``` Tip: an explanation of the labels can be found [here](https://catalog.ldc.upenn.edu/docs/LDC93S1/PHONCODE.TXT).\n",
        "\n"
      ],
      "metadata": {
        "id": "N4gPLAk0oztQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "52sZ4RBZ9XPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Wav2Vec2 Model"
      ],
      "metadata": {
        "id": "SoJTlOqQ-b2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n",
        "\n",
        "The model takes raw audio waveforms as input and splits them into fixed-size frames of 25 milliseconds. During pre-training, some of these frames are masked and the model has to predict the correct speech unit for the masked position. In doing so, the model learns powerful speech representations in a self-supervised manner.\n",
        "\n",
        "After pre-training, the model can be fine-tuned for several downstream tasks. We will investigate a model version that is fine-tuned for **Automatic Speech Recognition**, i.e. predicting written transcriptions that correspond to the spoken input. More specifically, the model was fine-tuned to predict a character for each of the 25ms-frames that we discussed above. These characters are then collapsed into well-formed transcriptions using [Connectionist Temporal Classification](https://distill.pub/2017/ctc/).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bCsvzcySpRf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToThink2**</font>\n",
        "\n",
        "Read [this blogpost](https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html) about Wav2Vec2 and make sure you understand the different components inside the model.\n",
        "\n",
        "<font color='green'>**ToDo2**</font>\n",
        "\n",
        "Load the model and processor by running the cell below. Examine the architecture carefully."
      ],
      "metadata": {
        "id": "K6nlFuVYqg9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "\n",
        "# Load model and processor\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "model.to(DEVICE)"
      ],
      "metadata": {
        "id": "DAMMH7_o9fQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare model input and analyze transcriptions"
      ],
      "metadata": {
        "id": "RJ3-XJcLphC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToDo3**</font>\n",
        "\n",
        "The Wav2Vec 2.0 model takes raw waveforms as input. Select one waveform from the data, plot the signal and listen to the audio."
      ],
      "metadata": {
        "id": "WQB-7BI5po99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# Select a data item you want to examine\n",
        "item_index = 0\n",
        "\n",
        "# Retrieve the waveform from the data\n",
        "raw_waveform = # YOUR CODE HERE\n",
        "\n",
        "# Plot the raw waveform\n",
        "plt.figure(figsize=(15,5))\n",
        "librosa.display.waveshow(raw_waveform, sr=16000, alpha=0.4)\n",
        "\n",
        "# Print corresponding text\n",
        "text = # YOUR CODE HERE\n",
        "print(\"Text:\", text)\n",
        "\n",
        "# Listen to the audio\n",
        "ipd.Audio(data=np.asarray(raw_waveform), autoplay=False, rate=16000)"
      ],
      "metadata": {
        "id": "SM8rKVXT99uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToDo4**</font>\n",
        "\n",
        "Let the model generate a transcription for the waveform. Are there any mistakes?\n",
        "\n",
        "<font color='red'>**ToSubmit1**</font>\n",
        "\n",
        "Please submit your plot of the waveform signal, together with the input text and the transcription that Wav2Vec2 generated for that particular waveform."
      ],
      "metadata": {
        "id": "hnIoFVrnp27Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "input_values = processor(raw_waveform, sampling_rate=16000, return_tensors=\"pt\", padding=\"max_length\", max_length=1000).input_values  # Batch size 1\n",
        "print(\"Input shape:\", input_values.shape)\n",
        "\n",
        "# Generate transcription with the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "7xluOoes-iz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract hidden states"
      ],
      "metadata": {
        "id": "MWbCbDsup94s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now extract the hidden states from the model's Transformer layers. These hidden states will serve as the training and evaluation data for our probing classifiers.\n",
        "\n",
        "Concretely, we will perform the following steps to achieve this:\n",
        "\n",
        "1.  **Prepare input**: Retrieve the raw waveforms (i.e. audio arrays) from the TIMIT corpus and process them using the Wav2Vec2 processor.\n",
        "2.  **Forward pass**: Pass the waveforms through the model with ```output_hidden_states``` set to True.\n",
        "3. **Save hidden states**: Save the hidden states in a dictionary, which is organized by Transformer layer index and waveform index. Each waveform will have list of frame-level hidden states.\n",
        "4. **Sort hidden states per phoneme class**: Sort the hidden states by phoneme class using the time-aligned transcriptions from TIMIT.\n",
        "\n"
      ],
      "metadata": {
        "id": "5u-DFYJ9TXzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToDo5**</font>\n",
        "\n",
        "Finish the function below to extract the hidden states from the Transformer layers of the model. Hint: You can simply extract hidden states by calling ```.hidden_states``` on the model output.\n",
        "\n"
      ],
      "metadata": {
        "id": "h4H2C9U9rPFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hidden_states(model, processor, inputs, num_layers):\n",
        "    '''\n",
        "    Extract hidden states from Wav2Vec 2.0 transformer layers.\n",
        "    :param model: Wav2Vec 2.0 model\n",
        "    :param processor: Wav2Vec 2.0 processor\n",
        "    :param inputs: list of TIMIT instances (i.e. timit['train'] or timit['test'])\n",
        "    :return: dictionary containing frame-level hidden states saved per transformer layer and per waveform\n",
        "    '''\n",
        "\n",
        "    # Get waveforms\n",
        "    waveforms = [input[\"audio\"][\"array\"] for input in inputs]\n",
        "\n",
        "    # Here we will save all frame-level hidden states, sorted by layer and waveform\n",
        "    frame_states = {\n",
        "        layer_idx: {\n",
        "            waveform_idx: []\n",
        "            for waveform_idx in range(len(waveforms))\n",
        "        }\n",
        "        for layer_idx in range(num_layers)\n",
        "    }\n",
        "\n",
        "    for waveform_idx, waveform in enumerate(waveforms):\n",
        "\n",
        "        print(f'Extracting hidden states from waveform {waveform_idx} out of {len(waveforms)} waveforms...')\n",
        "\n",
        "        # Process waveform using the Wav2Vec2 processor\n",
        "        processed_input = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding='longest').input_values\n",
        "\n",
        "        with torch.no_grad():\n",
        "            input_tensor = torch.tensor(processed_input, device=DEVICE)\n",
        "\n",
        "            # forward pass\n",
        "            model_output = # YOUR CODE HERE\n",
        "\n",
        "            # get all hidden outputs\n",
        "            transformer_layers = # YOUR CODE HERE\n",
        "\n",
        "        # Save frame-level hidden states, organized by layer and waveform\n",
        "        for layer_idx, layer in enumerate(transformer_layers):\n",
        "            for waveform in layer:\n",
        "              for frame in waveform:\n",
        "                  frame_states[layer_idx][waveform_idx].append(frame.cpu())\n",
        "\n",
        "    return frame_states"
      ],
      "metadata": {
        "id": "K3xqk4fcAjUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to sort the hidden states by phoneme class using the time-aligned transcriptions from TIMIT:"
      ],
      "metadata": {
        "id": "_nz7bLSOrTJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_states_per_phoneme(data, frame_states, num_layers):\n",
        "\n",
        "    frame_states_per_phoneme = {\n",
        "        layer_idx: defaultdict(list)\n",
        "        for layer_idx in range(num_layers)\n",
        "    }\n",
        "\n",
        "    for layer_idx, layer in frame_states.items():\n",
        "\n",
        "        for waveform_idx, waveform in layer.items():\n",
        "\n",
        "            # retrieve phoneme annotation for the current waveform\n",
        "            phonemes = data[waveform_idx][\"phonetic_detail\"]\n",
        "            phoneme_indeces = defaultdict(list)\n",
        "\n",
        "            for start, stop, phoneme in zip(phonemes['start'], phonemes['stop'], phonemes['utterance']):\n",
        "\n",
        "                # divide start and stop point by sample rate (16000 hz) and frame length (0.020 sec)\n",
        "                start_index = math.floor((start / 16000) / 0.020)\n",
        "                stop_index = math.ceil((stop / 16000) / 0.020)\n",
        "                phoneme_indeces[phoneme].extend(range(start_index, stop_index))\n",
        "\n",
        "            # find hidden states corresponding to phoneme indeces and save them per layer and per phoneme\n",
        "            for phoneme, indeces in phoneme_indeces.items():\n",
        "                phoneme_states = [waveform[idx] for idx in indeces if idx < len(waveform)]\n",
        "                frame_states_per_phoneme[layer_idx][phoneme].extend(phoneme_states)\n",
        "\n",
        "    return frame_states_per_phoneme"
      ],
      "metadata": {
        "id": "ForBF5KrCHP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToDo6**</font>\n",
        "\n",
        "Extract the hidden states for training and testing our probes (this might take some time):"
      ],
      "metadata": {
        "id": "xIf23m5DraFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# We select a relatively small number of sentences since they will be split up in a large number of frames\n",
        "train_size = 800\n",
        "test_size = 100\n",
        "num_layers = 13 # input plus 12 transformer layers\n",
        "\n",
        "# Generate random indeces to select a subset of the train and test data\n",
        "train_indeces = random.sample(range(0, len(timit['train'])), train_size)\n",
        "test_indeces = random.sample(range(0, len(timit['test'])), test_size)\n",
        "\n",
        "train_subset = timit['train'].select(train_indeces)\n",
        "test_subset = timit['test'].select(test_indeces)\n",
        "print(len(train_subset), len(test_subset))\n",
        "\n",
        "# Extract frame-level hidden states for training and testing the probes (make sure you pass the data subset)\n",
        "frame_states_train = # YOUR CODE HERE\n",
        "frame_states_test = # YOUR CODE HERE\n",
        "\n",
        "# Sort the hidden states by phoneme (make sure you pass the data subset)\n",
        "phoneme_states_train = # YOUR CODE HERE\n",
        "phoneme_states_test = # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "l9YXMF1UCO6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map fine-grained phoneme labels to broader categories"
      ],
      "metadata": {
        "id": "SmIIghO1tYN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phonemes can have different realizations depending on the context in which they occur, or depending on the dialect of the speaker. TIMIT contains annotations for many of these realizations (61 in total). We will merge phonemes that sound very similar into a single category, such that we end up with 39 broader categories in total."
      ],
      "metadata": {
        "id": "bsKhSzKUI_u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The original labels\n",
        "print(phoneme_states_train[0].keys())"
      ],
      "metadata": {
        "id": "OgbKwdyUDGFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phoneme_mapping = {\n",
        "    'p': 'p',\n",
        "    'b': 'b',\n",
        "    't': 't',\n",
        "    'd': 'd',\n",
        "    'k': 'k',\n",
        "    'g': 'g',\n",
        "    'dx': 'dx',\n",
        "    'f': 'f',\n",
        "    'v': 'v',\n",
        "    'dh': 'dh',\n",
        "    'th': 'th',\n",
        "    's': 's',\n",
        "    'z': 'z',\n",
        "    'r': 'r',\n",
        "    'w': 'w',\n",
        "    'y': 'y',\n",
        "    'jh': 'jh',\n",
        "    'ch': 'ch',\n",
        "    'iy': 'iy',\n",
        "    'eh': 'eh',\n",
        "    'ey': 'ey',\n",
        "    'ae': 'ae',\n",
        "    'aw': 'aw',\n",
        "    'ay': 'ay',\n",
        "    'oy': 'oy',\n",
        "    'ow': 'ow',\n",
        "    'uh': 'uh',\n",
        "    'ah': 'ah',\n",
        "    'ax': 'ah',\n",
        "    'ax-h': 'ah',\n",
        "    'aa': 'aa',\n",
        "    'ao': 'aa',\n",
        "    'er': 'er',\n",
        "    'axr': 'er',\n",
        "    'hh': 'hh',\n",
        "    'hv': 'hh',\n",
        "    'ih': 'ih',\n",
        "    'ix': 'ih',\n",
        "    'l': 'l',\n",
        "    'el': 'l',\n",
        "    'm': 'm',\n",
        "    'em': 'm',\n",
        "    'n': 'n',\n",
        "    'en': 'n',\n",
        "    'nx': 'n',\n",
        "    'ng': 'ng',\n",
        "    'eng': 'ng',\n",
        "    'sh': 'sh',\n",
        "    'zh': 'sh',\n",
        "    'uw': 'uw',\n",
        "    'ux': 'uw',\n",
        "    'pcl': 'sil',\n",
        "    'bcl': 'sil',\n",
        "    'tcl': 'sil',\n",
        "    'dcl': 'sil',\n",
        "    'kcl': 'sil',\n",
        "    'gcl': 'sil',\n",
        "    'h#': 'sil',\n",
        "    'pau': 'sil',\n",
        "    'epi': 'sil'\n",
        "}"
      ],
      "metadata": {
        "id": "EZQaZhu8nU4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluate probing classifiers"
      ],
      "metadata": {
        "id": "ipzlv3Ywttk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is now time to train and evaluate our probes. We will define a probing classifier for each layer of the Wav2Vec2 model to see how phoneme information is represented across layers. We will be using Logistic Regression as our classification models (but feel free to experiment with other classifiers such as SVM)."
      ],
      "metadata": {
        "id": "Fb0EriXBZgyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Define layer-wise probes, to be trained and tested on frame-level Wav2Vec2 embeddings\n",
        "layer_probes = {\n",
        "    layer_idx: LogisticRegression(solver=\"liblinear\", penalty=\"l2\", max_iter=10)\n",
        "    for layer_idx in range(num_layers)\n",
        "}"
      ],
      "metadata": {
        "id": "DM5rTQO9Fm0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions for putting the data in the right format for Logistic Regression models, and for balancing the data:"
      ],
      "metadata": {
        "id": "qK4N3pCdt7Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(hidden_state_dict, layer_idx, target_phonemes=None):\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for phoneme, hidden_state_list in hidden_state_dict[layer_idx].items():\n",
        "        for i in hidden_state_list:\n",
        "            if i != None and phoneme != 'q':\n",
        "                if target_phonemes == None:\n",
        "                    X.append(np.array(i))\n",
        "                    y.append(phoneme_mapping[phoneme])\n",
        "                else:\n",
        "                    if phoneme in target_phonemes:\n",
        "                        X.append(np.array(i))\n",
        "                        y.append(phoneme_mapping[phoneme])\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def balance_classes(X_instances, y_labels):\n",
        "\n",
        "    balanced_data_X = []\n",
        "    balanced_data_y = []\n",
        "\n",
        "    class_distribution = Counter(y_labels)\n",
        "    num_instances_per_class = min(class_distribution.values())\n",
        "\n",
        "    for label in class_distribution.keys():\n",
        "\n",
        "        i = 1\n",
        "        instances = []\n",
        "        labels = []\n",
        "\n",
        "        for x, y in zip(X_instances, y_labels):\n",
        "            if y == label:\n",
        "                instances.append(x)\n",
        "                labels.append(y)\n",
        "                if i == num_instances_per_class:\n",
        "                    balanced_data_X.extend(instances)\n",
        "                    balanced_data_y.extend(labels)\n",
        "                    break\n",
        "                i += 1\n",
        "\n",
        "    return balanced_data_X, balanced_data_y"
      ],
      "metadata": {
        "id": "liLJkqgIGQXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToDo7**</font>\n",
        "\n",
        "Train and evaluate the probing classifier for each layer (this will take some time). Examine the difference in probing accuracy when predicting different categories of phonemes (i.e. stops, fricatives, nasals, glides, vowels). The different categories are explained [here](http://www.ello.uos.de/field.php/PhoneticsandPhonology/MannerOfArticulation)."
      ],
      "metadata": {
        "id": "fpSpAfmRe0hF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "phoneme_dict = {\n",
        "    'stops': ['p', 't', 'k', 'b', 'd', 'g'],\n",
        "    'fricatives': ['f', 'v', 'th', 'dh', 's', 'z', 'sh'],\n",
        "    'nasals': ['m', 'n', 'ng'],\n",
        "    'approximants': ['l', 'w', 'y', 'hh'],\n",
        "    'vowels': ['aa', 'ow', 'iy', 'eh', 'uh']\n",
        "}\n",
        "\n",
        "# Save layer-wise accuracies for each phoneme category\n",
        "accs_per_category = []\n",
        "\n",
        "for phoneme_category, target_phonemes in phoneme_dict.items():\n",
        "\n",
        "  print(f\"Training layer-wise probes to predict {phoneme_category}...\")\n",
        "  accs = []\n",
        "\n",
        "  # Train and test an individual probe for each layer\n",
        "  for layer_idx in range(num_layers):\n",
        "\n",
        "    # Put train data in the right format for our probing classifier + balance classes\n",
        "    train_X, train_y = data_loader(phoneme_states_train, layer_idx, target_phonemes=target_phonemes)\n",
        "    train_X, train_y = balance_classes(train_X, train_y)\n",
        "\n",
        "    if layer_idx == 0:\n",
        "      print(\"Balanced class distribution TRAIN:\", Counter(train_y))\n",
        "\n",
        "    # Train model\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Put test data in the right format for our probing classifier + balance classes\n",
        "    test_X, test_y = data_loader(phoneme_states_test, layer_idx, target_phonemes=target_phonemes)\n",
        "    test_X, test_y = balance_classes(test_X, test_y)\n",
        "\n",
        "    if layer_idx == 0:\n",
        "      print(\"Balanced class distribution TEST:\", Counter(test_y))\n",
        "\n",
        "    # Predict\n",
        "    test_pred = # YOUR CODE HERE\n",
        "\n",
        "    # Calculate accuracy\n",
        "    test_acc = accuracy_score(test_y, test_pred)\n",
        "    print(f'Accuracy for layer {layer_idx}:', test_acc)\n",
        "    accs.append(test_acc)\n",
        "\n",
        "  accs_per_category.append(accs)"
      ],
      "metadata": {
        "id": "wMNM-oG2F1ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'>**ToDo8**</font>\n",
        "\n",
        "Plot the layer-wise accuracies per phoneme category.\n",
        "\n",
        "<font color='green'>**ToThink3**</font>\n",
        "\n",
        "Does the layer-wise evolution of phoneme information match with the subcomponents in the traditional ASR pipeline?\n",
        "\n",
        "<font color='red'>**ToSubmit2**</font>\n",
        "\n",
        "Please submit your plot of the probing results. Make sure to add a caption in which you briefly explain the observed pattern of the probing accuracies over layers and how this relates to the traditional ASR pipeline."
      ],
      "metadata": {
        "id": "WoXyDSK9aeHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for accs in accs_per_category:\n",
        "\n",
        "  # Plot layer-wise probing accuracy per phoneme category\n",
        "  plt.plot(range(num_layers), accs, marker='.')\n",
        "  plt.title(f'Representation of phonemes in Wav2Vec 2.0', fontsize=16)\n",
        "  plt.xlabel('Wav2Vec 2.0 layer', fontsize=12)\n",
        "  plt.xticks(range(num_layers), range(1, 14), fontsize=12)\n",
        "  plt.ylabel('Probing accuracy', fontsize=12)\n",
        "  plt.legend(list(phoneme_dict.keys()))\n",
        "  plt.ylim((0.5, 1.0))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pL9PEEPRnkdX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}