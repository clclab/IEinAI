{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFrxjD-neDcj"
      },
      "source": [
        "# Workshop on Speech Perception, Part 2: Comparing Audio Transformer representations with Centered Kernel Alignment\n",
        "\n",
        "*Interpretability & Explainability in AI, MSc A.I., University of Amsterdam, June 2024*\n",
        "\n",
        "This notebook includes contributions by: Marianne de Heer Kloots & Marta Grasa.\n",
        "It makes use of the [reference code](https://colab.research.google.com/github/google-research/google-research/blob/master/representation_similarity/Demo.ipynb) provided by Simon Kornblith and colleagues for implementing CKA computations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZMxxInMgoow"
      },
      "source": [
        "## Comparing representations with similarity-based interpretability\n",
        "\n",
        "In the first notebook for this workshop we made our first steps towards understanding where different types of information are represented in Wav2Vec2, by evaluating how well we can decode interpretable features (like amplitude envelopes and phonemes) across the model's hidden layers.  \n",
        "\n",
        "In this notebook we experiment with a different technique for analyzing model internals, by computing similarities between different representation spaces. Such representational similarity analyses even allow us to compare representational spaces with very different formats, such model embeddings and syntactic trees (as in [Shen et al., 2023](https://www.isca-archive.org/interspeech_2023/shen23_interspeech.html)), or model embeddings and human brain recordings (as in [Abnar et al., 2019](https://aclanthology.org/W19-4820/)) â€” as long as we are able to define a similarity measure over a common set of stimuli within each representational space. It also allows us to examine for example how representations evolve across different layers of the same model, or how representations generated by different models compare.\n",
        "\n",
        "For this notebook, we will focus on a method called Centered Kernel Alignment (CKA), introduced by [Kornblith et al. (2019)](https://proceedings.mlr.press/v97/kornblith19a.html). We will use it to compare between a range of different Wav2Vec2 models (untrained, pretrained, and finetuned versions of the base and large architectures), and also to compare the internal representations of these models to more interpretable feature spaces like MFCC and GloVe vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEz0K8GOQDJE"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='blue'><b>ToThink 5</b></font>: Understanding similarity-based interpretability techniques\n",
        "\n",
        "Familiarize yourself with similarity-based interpretability techniques by watching [this video](https://youtu.be/u7Dvb_a1D-0). If you're interested, a more in-depth explanation of CKA specifically is also covered in [this talk](https://youtu.be/TBjdvjdS2KM). What is an advantage of CKA as compared to other similarity-based interpretability methods, like CCA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v5KvDggef1b6"
      },
      "outputs": [],
      "source": [
        "# @title CKA functions\n",
        "## taken from Kornblith et al. (2019) - https://cka-similarity.github.io/\n",
        "import numpy as np\n",
        "\n",
        "def gram_linear(x):\n",
        "  \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
        "\n",
        "  Args:\n",
        "    x: A num_examples x num_features matrix of features.\n",
        "\n",
        "  Returns:\n",
        "    A num_examples x num_examples Gram matrix of examples.\n",
        "  \"\"\"\n",
        "  return x.dot(x.T)\n",
        "\n",
        "\n",
        "def gram_rbf(x, threshold=1.0):\n",
        "  \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
        "\n",
        "  Args:\n",
        "    x: A num_examples x num_features matrix of features.\n",
        "    threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
        "      bandwidth. (This is the heuristic we use in the paper. There are other\n",
        "      possible ways to set the bandwidth; we didn't try them.)\n",
        "\n",
        "  Returns:\n",
        "    A num_examples x num_examples Gram matrix of examples.\n",
        "  \"\"\"\n",
        "  dot_products = x.dot(x.T)\n",
        "  sq_norms = np.diag(dot_products)\n",
        "  sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
        "  sq_median_distance = np.median(sq_distances)\n",
        "  return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
        "\n",
        "\n",
        "def center_gram(gram, unbiased=False):\n",
        "  \"\"\"Center a symmetric Gram matrix.\n",
        "\n",
        "  This is equvialent to centering the (possibly infinite-dimensional) features\n",
        "  induced by the kernel before computing the Gram matrix.\n",
        "\n",
        "  Args:\n",
        "    gram: A num_examples x num_examples symmetric matrix.\n",
        "    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
        "      estimate of HSIC. Note that this estimator may be negative.\n",
        "\n",
        "  Returns:\n",
        "    A symmetric matrix with centered columns and rows.\n",
        "  \"\"\"\n",
        "  if not np.allclose(gram, gram.T):\n",
        "    raise ValueError('Input must be a symmetric matrix.')\n",
        "  gram = gram.copy()\n",
        "\n",
        "  if unbiased:\n",
        "    # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
        "    # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
        "    # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
        "    # stable than the alternative from Song et al. (2007).\n",
        "    n = gram.shape[0]\n",
        "    np.fill_diagonal(gram, 0)\n",
        "    means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
        "    means -= np.sum(means) / (2 * (n - 1))\n",
        "    gram -= means[:, None]\n",
        "    gram -= means[None, :]\n",
        "    np.fill_diagonal(gram, 0)\n",
        "  else:\n",
        "    means = np.mean(gram, 0, dtype=np.float64)\n",
        "    means -= np.mean(means) / 2\n",
        "    gram -= means[:, None]\n",
        "    gram -= means[None, :]\n",
        "\n",
        "  return gram\n",
        "\n",
        "\n",
        "def cka(gram_x, gram_y, debiased=False):\n",
        "  \"\"\"Compute CKA.\n",
        "\n",
        "  Args:\n",
        "    gram_x: A num_examples x num_examples Gram matrix.\n",
        "    gram_y: A num_examples x num_examples Gram matrix.\n",
        "    debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
        "\n",
        "  Returns:\n",
        "    The value of CKA between X and Y.\n",
        "  \"\"\"\n",
        "  gram_x = center_gram(gram_x, unbiased=debiased)\n",
        "  gram_y = center_gram(gram_y, unbiased=debiased)\n",
        "\n",
        "  # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
        "  # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
        "  scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
        "\n",
        "  normalization_x = np.linalg.norm(gram_x)\n",
        "  normalization_y = np.linalg.norm(gram_y)\n",
        "  return scaled_hsic / (normalization_x * normalization_y)\n",
        "\n",
        "\n",
        "def _debiased_dot_product_similarity_helper(\n",
        "    xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y,\n",
        "    n):\n",
        "  \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
        "  # This formula can be derived by manipulating the unbiased estimator from\n",
        "  # Song et al. (2007).\n",
        "  return (\n",
        "      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
        "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
        "\n",
        "\n",
        "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
        "  \"\"\"Compute CKA with a linear kernel, in feature space.\n",
        "\n",
        "  This is typically faster than computing the Gram matrix when there are fewer\n",
        "  features than examples.\n",
        "\n",
        "  Args:\n",
        "    features_x: A num_examples x num_features matrix of features.\n",
        "    features_y: A num_examples x num_features matrix of features.\n",
        "    debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
        "      biased. Note that this estimator may be negative.\n",
        "\n",
        "  Returns:\n",
        "    The value of CKA between X and Y.\n",
        "  \"\"\"\n",
        "  features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
        "  features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
        "\n",
        "  dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
        "  normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
        "  normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
        "\n",
        "  if debiased:\n",
        "    n = features_x.shape[0]\n",
        "    # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
        "    sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
        "    sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
        "    squared_norm_x = np.sum(sum_squared_rows_x)\n",
        "    squared_norm_y = np.sum(sum_squared_rows_y)\n",
        "\n",
        "    dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
        "        dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
        "        squared_norm_x, squared_norm_y, n)\n",
        "    normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
        "        normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
        "        squared_norm_x, squared_norm_x, n))\n",
        "    normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
        "        normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
        "        squared_norm_y, squared_norm_y, n))\n",
        "\n",
        "  return dot_product_similarity / (normalization_x * normalization_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6-8kEtvzG_sa"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions\n",
        "def set_seed(seed):\n",
        "    \"\"\"Set random seed.\"\"\"\n",
        "    if seed == -1:\n",
        "        seed = random.randint(0, 1000)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # if you are using GPU\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def plot_gram_matrix(feature_list, subset_ids, ax, gram_fun=gram_linear):\n",
        "    N_subsets = len(subset_ids)\n",
        "    im = ax.imshow(gram_fun(np.array(feature_list)))\n",
        "    divider_lines = [(len(feature_list)/N_subsets)*i for i in range(1,N_subsets)]\n",
        "    ax.vlines(divider_lines, 0, len(feature_list), color='w')\n",
        "    ax.hlines(divider_lines, 0, len(feature_list), color='w')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlim(0, len(feature_list))\n",
        "    ax.set_ylim(0, len(feature_list))\n",
        "    text_positions = [(divider_lines[0])/2] +\\\n",
        "    [(divider_lines[i]+divider_lines[i+1])/2 for i in range(len(divider_lines)-1)] +\\\n",
        "    [(divider_lines[-1]+len(feature_list))/2]\n",
        "    for i, tp in enumerate(text_positions):\n",
        "        ax.text(tp, -10, subset_ids[i], ha='center')\n",
        "        ax.text(-10, tp, subset_ids[i], va='center', rotation=90)\n",
        "    return im\n",
        "\n",
        "# plotting aesthetics\n",
        "linestyles = {\n",
        "    'base_unt': 'dotted',\n",
        "    'base_pret': 'dashed',\n",
        "    'base_ft': 'solid',\n",
        "    'large_unt': 'dotted',\n",
        "    'large_pret': 'dashed',\n",
        "    'large_ft': 'solid'\n",
        "}\n",
        "\n",
        "colors = {\n",
        "    'base_unt': 'silver',\n",
        "    'base_pret': 'mediumpurple',\n",
        "    'base_ft': 'indigo',\n",
        "    'large_unt': 'silver',\n",
        "    'large_pret': 'mediumpurple',\n",
        "    'large_ft': 'indigo'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ9s7Jx-gjQv"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY-OI3rBHL8T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xECseAkqBEdI"
      },
      "outputs": [],
      "source": [
        "import tgt\n",
        "import torch\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "from transformers import Wav2Vec2Config, Wav2Vec2FeatureExtractor, Wav2Vec2Model, Wav2Vec2ForCTC\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTTQir1N9xzb"
      },
      "outputs": [],
      "source": [
        "# download a subset of the speech accent archive with force-aligned transcriptions\n",
        "# ('please_call_stella' folder)\n",
        "!gdown 1rH-EWbtJBp0teUFXCjVHE_u2icvNRNNT\n",
        "!unzip please_call_stella.zip\n",
        "\n",
        "# GloVe embeddings for all words in the please_call_stella fragment\n",
        "!gdown 1tx7F6QLwjaWpg_sZY3mel8a13q0-CME0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzzMpg9yTpvT"
      },
      "outputs": [],
      "source": [
        "# setup variables\n",
        "STELLA_DIR = '/content/please_call_stella'\n",
        "SAMP_FREQ = 16000\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12k2cKSth_k1"
      },
      "outputs": [],
      "source": [
        "## load all Wav2Vec2 models we will compare\n",
        "base_pret_ckpt = \"facebook/wav2vec2-base\"\n",
        "base_ft_ckpt = \"facebook/wav2vec2-base-960h\"\n",
        "large_pret_ckpt = \"facebook/wav2vec2-large\"\n",
        "large_ft_ckpt = \"facebook/wav2vec2-large-960h\"\n",
        "base_config = Wav2Vec2Config.from_pretrained(base_pret_ckpt)\n",
        "large_config = Wav2Vec2Config.from_pretrained(large_pret_ckpt)\n",
        "\n",
        "base_models = ['base_unt', 'base_pret', 'base_ft']\n",
        "large_models = ['large_unt', 'large_pret', 'large_ft']\n",
        "models = {\n",
        "    # untrained base model\n",
        "    \"base_unt\": Wav2Vec2Model(base_config),\n",
        "    # pretrained (only) base model\n",
        "    \"base_pret\": Wav2Vec2Model.from_pretrained(base_pret_ckpt),\n",
        "    # pretrained + finetuned base model\n",
        "    \"base_ft\": Wav2Vec2Model.from_pretrained(base_ft_ckpt),\n",
        "    # untrained large model\n",
        "    \"large_unt\": Wav2Vec2Model(large_config),\n",
        "    # pretrained (only) large model\n",
        "    \"large_pret\": Wav2Vec2Model.from_pretrained(large_pret_ckpt),\n",
        "    # pretrained + finetuned large model\n",
        "    \"large_ft\": Wav2Vec2Model.from_pretrained(large_ft_ckpt),\n",
        "}\n",
        "\n",
        "# feature extractor is the same for all models\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(base_ft_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgOrmSMtiXeJ"
      },
      "source": [
        "## Speech Accent Archive recordings and word segments\n",
        "\n",
        "To construct our stimulus set for representational similarity measures, we will use a subset of recordings from the [Speech Accent Archive](https://accent.gmu.edu/). The Speech Accent Archive contains recordings of many different speakers reading the same _elicitation paragraph_ which we refer to as 'Please call Stella'. This paragraph was constructed to contain a large variety of difficult English sounds and sound sequences, and read by speakers of English with a variety of native and non-native accents.  \n",
        "\n",
        "In this notebook, we will be analyzing model representations on the level of _words_. To this end, we provide word-aligned transcriptions of a subset of the Speech Accent Archive recordings, obtained automatically using the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/latest/) tool and provided as .TextGrid files in the `please_call_stella` folder (downloaded above).\n",
        "\n",
        "For demonstration, we here only use a small subset of 3 male North-American native English speakers. However, you can find more recordings with aligned transcripts in the same folder, in case you are interested in studying speaker or accent effects for your mini-project. Have a look at the `speaker_info` dataframe loaded below for details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehmz0-XJWyJ0"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='green'>**ToDo 13**</font>: Inspecting Speech Accent Archive data\n",
        "\n",
        "Load our `annotated_speaker_subset` from the Speech Accent Archive by running the cells below. Listen to an example recording and some extracted word segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9luPAQ3QjTo7"
      },
      "outputs": [],
      "source": [
        "# functions for reading in wav files and TextGrid annotations from our please_call_stella folder\n",
        "def get_word_annotations(annotations_file):\n",
        "  word_annotation_tier = tgt.io.read_textgrid(annotations_file).get_tier_by_name('words')\n",
        "  annotations_dict = {\n",
        "      'start_times': [intv.start_time for intv in word_annotation_tier],\n",
        "      'end_times': [intv.end_time for intv in word_annotation_tier],\n",
        "      'words': [intv.text for intv in word_annotation_tier]\n",
        "  }\n",
        "  return annotations_dict\n",
        "\n",
        "def get_word_audios(full_audio, sr, words, word_start_times, word_end_times):\n",
        "  word_audios = [\n",
        "      full_audio[int(wst*sr):int(wet*sr)]\n",
        "      for wst, wet in zip(word_start_times, word_end_times)\n",
        "  ]\n",
        "  return word_audios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REgbdT_5V0vq"
      },
      "outputs": [],
      "source": [
        "# information about all speakers in this speech accent archive subset (e.g. age, gender and native language)\n",
        "speaker_info = pd.read_csv(f'{STELLA_DIR}/speakers.csv', sep=';', index_col=0)\n",
        "\n",
        "# we will create a subset containing only recordings by these speakers\n",
        "speaker_subset_ids = ['english1', 'english51', 'english81']\n",
        "\n",
        "# load the recordings and annotations for these speakers into a dict\n",
        "annotated_speaker_subset = {\n",
        "    speaker_id: {\n",
        "        'audio': librosa.load(f'{STELLA_DIR}/{speaker_id}.wav', sr=SAMP_FREQ)[0]\n",
        "    } | get_word_annotations(f'{STELLA_DIR}/{speaker_id}.TextGrid')\n",
        "    for speaker_id in speaker_subset_ids\n",
        "}\n",
        "\n",
        "# list of id strings describing all audio segments (including the speaker and the word corresponding to each segment)\n",
        "audio_ids = [f'{speaker_id}_{w}' for speaker_id in speaker_subset_ids for w in annotated_speaker_subset[speaker_id]['words']]\n",
        "\n",
        "# raw audio signals (waveforms) for each word segment\n",
        "word_audios = [wa\n",
        "    for speaker_id in speaker_subset_ids\n",
        "    for wa in get_word_audios(annotated_speaker_subset[speaker_id]['audio'],\n",
        "                              SAMP_FREQ,\n",
        "                              annotated_speaker_subset[speaker_id]['words'],\n",
        "                              annotated_speaker_subset[speaker_id]['start_times'],\n",
        "                              annotated_speaker_subset[speaker_id]['end_times'])\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uQR09iAV05W"
      },
      "outputs": [],
      "source": [
        "speaker_id = 'english1'\n",
        "print(f'Full recording for {speaker_id}:')\n",
        "ipd.display(Audio(annotated_speaker_subset['english1']['audio'], rate=SAMP_FREQ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUbMxH_9WZAF"
      },
      "outputs": [],
      "source": [
        "print(f'A few word segments:')\n",
        "for audio_id, word_audio in list(zip(audio_ids, word_audios))[:3]:\n",
        "  print(audio_id)\n",
        "  ipd.display(Audio(word_audio, rate=SAMP_FREQ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ijbpQK3l8Jx"
      },
      "source": [
        "## Gram matrices for MFCCs and GloVe embeddings\n",
        "\n",
        "The first step in our similarity analyses is to compute _gram matrices_ which capture the _representational similarity_ between each example in our dataset as computed using some kernel function over our chosen set of features (see a helpful visualization [here](https://youtu.be/TBjdvjdS2KM&t=219)).  \n",
        "\n",
        "We will soon be computing these matrices for model internal layers, but we'll first compute them over two sets of somewhat more interpretable features: the average [Mel-frequency cepstral coefficients (MFCCs)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) over word segments (capturing the word's acoustics) and the [GloVe vectors](https://nlp.stanford.edu/projects/glove/) for each word in the paragraph (capturing word-level distributional information)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvVhMvgMbCBa"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='green'>**ToDo 14**</font>: Inspecting Gram matrices with Linear and RBF kernels\n",
        "\n",
        "Run the code cells below to plot the Gram matrices for the MFCC and GloVe features over all word segments in our subset of recordings. We can compute similarity between examples using a simple linear kernel (the dot product), but we can also use more complex non-linear kernels like the RBF kernel. What differences do you observe in the Gram matrices of these two feature spaces? What effect does the choice of kernel function have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64e5zrTmgHe1"
      },
      "outputs": [],
      "source": [
        "# glove embeddings for all words (3 times the same paragraph)\n",
        "glove_emb_dict = pickle.load(open('stella_glove_embs.pkl', 'rb'))\n",
        "glove_embs = np.vstack([glove_emb_dict[audio_id.split('_')[1]] for audio_id in audio_ids])\n",
        "\n",
        "# MFCC features for all words (3 times the same paragraph, read by each speaker)\n",
        "avg_MFCCs = np.vstack([librosa.feature.mfcc(y=wa, sr=SAMP_FREQ, n_mels=18, n_fft=256).mean(axis=1) for wa in word_audios])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9xFipZPjsJI"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10,10))\n",
        "axs[0,0].set_title('MFCCs, Linear kernel')\n",
        "im = plot_gram_matrix(avg_MFCCs, speaker_subset_ids, axs[0,0], gram_fun=gram_linear)\n",
        "\n",
        "axs[0,1].set_title('GloVe embeddings, Linear kernel')\n",
        "im = plot_gram_matrix(glove_embs, speaker_subset_ids, axs[0,1], gram_fun=gram_linear)\n",
        "\n",
        "axs[1,0].set_title('MFCCs, RBF kernel')\n",
        "im = plot_gram_matrix(avg_MFCCs, speaker_subset_ids, axs[1,0], gram_fun=gram_rbf)\n",
        "\n",
        "axs[1,1].set_title('GloVe embeddings, RBF kernel')\n",
        "im = plot_gram_matrix(glove_embs, speaker_subset_ids, axs[1,1], gram_fun=gram_rbf)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwhoRkwzqEQO"
      },
      "source": [
        "## Gram matrices for Wav2Vec2 representations\n",
        "We'll now proceed to use the same technique to analyze inter-example similarities at different layers of the Wav2Vec2 model. To do that, we first need to extract the audio frame representations for each of the word segments in our dataset. Running the code cells below will do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smYvPoYMnFUK"
      },
      "source": [
        "### Extracting model hidden states for word segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi9kCIPS7nQ5"
      },
      "outputs": [],
      "source": [
        "class SaveOutput:\n",
        "    def __init__(self):\n",
        "        self.outputs = defaultdict()\n",
        "\n",
        "    def __call__(self, name):\n",
        "        def hook(module, module_in, module_out):\n",
        "            self.outputs[name] = module_out.detach()\n",
        "        return hook\n",
        "\n",
        "    def clear(self):\n",
        "        self.outputs = defaultdict()\n",
        "\n",
        "def get_frame_states_for_segments(model, feature_extractor, full_waveform, segment_start_times, segment_end_times):\n",
        "  \"\"\"\n",
        "  Extract layerwise audio frame representations for segments out of a single full waveform,\n",
        "  indicated by the provided start & end times.\n",
        "  \"\"\"\n",
        "  ## convert segment start & end times to model frame indices\n",
        "  time_offset = model.config.inputs_to_logits_ratio / feature_extractor.sampling_rate\n",
        "  segment_start_frames = [int(np.floor(start_time / time_offset)) for start_time in segment_start_times]\n",
        "  segment_end_frames = [int(np.ceil(end_time / time_offset)) for end_time in segment_end_times]\n",
        "\n",
        "  ## register hooks at CNN output and Transformer embeds + layers\n",
        "  save_output = SaveOutput()\n",
        "  # for the ASR-finetuned model\n",
        "  if type(model) == Wav2Vec2ForCTC:\n",
        "      last_conv_layer = model.wav2vec2.feature_extractor.conv_layers[-1]\n",
        "      last_conv_layer.activation.register_forward_hook(save_output('CNN'))\n",
        "      model.wav2vec2.encoder.layer_norm.register_forward_hook(save_output('embeds'))\n",
        "      for i, enc_layer in enumerate(model.wav2vec2.encoder.layers):\n",
        "          enc_layer.final_layer_norm.register_forward_hook(save_output(f'T{i+1}'))\n",
        "  # for the pretrained & untrained model\n",
        "  elif type(model) == Wav2Vec2Model:\n",
        "      last_conv_layer = model.feature_extractor.conv_layers[-1]\n",
        "      last_conv_layer.activation.register_forward_hook(save_output('CNN'))\n",
        "      model.encoder.layer_norm.register_forward_hook(save_output('embeds'))\n",
        "      for i, enc_layer in enumerate(model.encoder.layers):\n",
        "          enc_layer.final_layer_norm.register_forward_hook(save_output(f'T{i+1}'))\n",
        "\n",
        "  # prepare inputs\n",
        "  inputs = feature_extractor(full_waveform,\n",
        "                             sampling_rate=SAMP_FREQ,\n",
        "                             return_tensors=\"pt\",\n",
        "                             padding='longest').input_values.to(DEVICE)\n",
        "  # forward pass\n",
        "  model.eval()\n",
        "  model.to(DEVICE)\n",
        "  with torch.no_grad():\n",
        "    model(inputs, output_hidden_states=True, output_attentions=False)\n",
        "\n",
        "  ## store only the frame states between the segment starts & ends\n",
        "  layer_segment_states = {layer: [] for layer in save_output.outputs.keys()}\n",
        "  for layer in save_output.outputs.keys():\n",
        "    # for the CNN output\n",
        "    if layer.startswith('C'):\n",
        "      for start_frame, end_frame in zip(segment_start_frames, segment_end_frames):\n",
        "        layer_segment_states[layer].append(save_output.outputs[layer][:, :, start_frame:end_frame].swapaxes(1, 2).detach().cpu().numpy())\n",
        "    # for the Transformer embeds + layer representations\n",
        "    else:\n",
        "      for start_frame, end_frame in zip(segment_start_frames, segment_end_frames):\n",
        "        layer_segment_states[layer].append(save_output.outputs[layer][:, start_frame:end_frame, :].detach().cpu().numpy())\n",
        "\n",
        "  del save_output\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  return layer_segment_states\n",
        "\n",
        "def frame_states_over_dataset(models, feature_extractor, dataset):\n",
        "  \"\"\"\n",
        "  Extract frame states for a range of models (provided in the `models` dict, which all work with feature_extractor),\n",
        "  over all annotated word segments in the provided dataset (dict organized by speaker_id with audio, start_times, and end_times for\n",
        "  each speaker, as in annotated_speaker_subset)\n",
        "  \"\"\"\n",
        "  model_frame_states = {}\n",
        "  N_speakers = len(dataset)\n",
        "  for model_id, model in models.items():\n",
        "    model_name = model.config._name_or_path if model.config._name_or_path else 'randomly initialized'\n",
        "    print(f'Extracting states from {model_id} ({model_name})...')\n",
        "    layer_segment_states = []\n",
        "    for i, (speaker_id, speaker_data) in enumerate(dataset.items()):\n",
        "      print(f'\\tSpeaker {i+1}/{N_speakers} ({speaker_id})...')\n",
        "      full_waveform = speaker_data['audio']\n",
        "      word_start_times = speaker_data['start_times']\n",
        "      word_end_times = speaker_data['end_times']\n",
        "      layer_segment_states.append(get_frame_states_for_segments(model, feature_extractor, full_waveform, word_start_times, word_end_times))\n",
        "    model_frame_states[model_id] = {\n",
        "        layer: [segst for i in range(len(layer_segment_states)) for segst in layer_segment_states[i][layer]]\n",
        "        for layer in layer_segment_states[0].keys()\n",
        "    }\n",
        "    del layer_segment_states\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "  return model_frame_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJaZftKK6f_o"
      },
      "outputs": [],
      "source": [
        "# extract frame states for all models in our comparison set and all speakers in our speaker subset\n",
        "model_frame_states = frame_states_over_dataset(models, feature_extractor, annotated_speaker_subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRcotoewpV3W"
      },
      "source": [
        "### Computing mean-pooled word representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We9I42WHeFy9"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='green'>**ToDo 15**</font>: Computing mean-pooled representations over word segments\n",
        "\n",
        "To be able to use our kernel functions for computing similarities between words, we first need to create word representations out of the sets of frame states we have for each word segment. For now, we will simply take the mean over all frames within a word segment to create equally shaped vectors for each word segment in our dataset.  \n",
        "Complete the function below to return a matrix with the mean frame state vector for each word segment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ9tX-0apOdl"
      },
      "outputs": [],
      "source": [
        "def get_mean_frame_embs(frame_states):\n",
        "  \"\"\"\n",
        "  :param frame_states:        list of frame states for audio segments, extracted from a single\n",
        "                              model component; i.e. list of N_segments elements, each a np.array\n",
        "                              of shape (1, N_frames, D_component), where D_component is 512 for\n",
        "                              CNN output and 768 for Transformer representations\n",
        "\n",
        "  :return mean_frame_embs:    np.array of shape (N_segments, D_component), containing the mean-\n",
        "                              pooled frame embedding for every segment (i.e. the mean over frame\n",
        "                              states across the segment)\n",
        "  \"\"\"\n",
        "  raise NotImplementedError\n",
        "  mean_frame_embs = # YOUR CODE HERE\n",
        "  return mean_frame_embs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsESoI9IgKpH"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='green'>**ToDo 16**</font>: Inspecting Gram matrices of Wav2Vec2 embeddings\n",
        "\n",
        "Now we are able to construct between-word similarity matrices for all models and layers of interest. Use the code cell below to inspect the Gram matrices for a few different models and layers. Also feel free to experiment with linear and RBF kernels again!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KljpON1pqfy"
      },
      "outputs": [],
      "source": [
        "# choose from our set of loaded models from which we have extracted frame states\n",
        "# ('base_unt', 'base_pret', 'base_ft', 'large_unt', 'large_pret', 'large_ft')\n",
        "model_id = 'base_ft'\n",
        "\n",
        "# choose from 'CNN', 'embeds', 'T{i}' (i = 1..12 for base and 1..24 for large)\n",
        "model_component = 'CNN'\n",
        "\n",
        "# choose between gram_linear and gram_rbf\n",
        "gram_fun = gram_linear\n",
        "\n",
        "# get word representations\n",
        "model_embs = get_mean_frame_embs(model_frame_states[model_id][model_component])\n",
        "\n",
        "# plot gram matrix for these word representations\n",
        "fig, ax = plt.subplots()\n",
        "im = plot_gram_matrix(model_embs, speaker_subset_ids, ax, gram_fun=gram_linear)\n",
        "cbar_ax = fig.add_axes([0.85, 0.15, 0.025, 0.7])\n",
        "fig.colorbar(im, cax=cbar_ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzCAHOTkqI2T"
      },
      "source": [
        "## CKA similarity between model layers\n",
        "\n",
        "Now that we have between-word similarity matrices for all models and components, we can compute the similarities between all those matrices with CKA! This allows us for example to compare different layers of the same architecture, and possibly detect apparent differences in processing between models, as done by [Kornblith et al. (2019)](https://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px_Umjv3j1to"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='green'>**ToDo 17**</font>: Inspecting between-layer similarities across models\n",
        "\n",
        "Run the cells below to plot the between-layer similarities for each of the models in our comparison set. What differences stand out between the untrained, pretrained and finetuned versions, and between the base and large models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raBcbSrjv4Y_"
      },
      "outputs": [],
      "source": [
        "def compute_model_layer_cka(first_model_id, second_model_id, gram_fun=gram_linear):\n",
        "  first_model_components = list(model_frame_states[first_model_id].keys())\n",
        "  second_model_components = list(model_frame_states[second_model_id].keys())\n",
        "\n",
        "  cka_df = pd.DataFrame(index=first_model_components, columns=second_model_components)\n",
        "  for comp1 in first_model_components:\n",
        "    for comp2 in second_model_components:\n",
        "      comp1_embs = get_mean_frame_embs(model_frame_states[first_model_id][comp1])\n",
        "      comp2_embs = get_mean_frame_embs(model_frame_states[second_model_id][comp2])\n",
        "      cka_df.loc[comp1, comp2] = cka(gram_fun(comp1_embs), gram_fun(comp2_embs))\n",
        "\n",
        "  return cka_df\n",
        "\n",
        "def plot_model_layer_cka(cka_df, ax):\n",
        "  first_model_components = list(cka_df.index)\n",
        "  second_model_components = list(cka_df.columns)\n",
        "\n",
        "  sns.heatmap(cka_df.values.astype(float), ax=ax)\n",
        "  ax.set_yticks(np.arange(len(first_model_components))+0.5, first_model_components, rotation=0)\n",
        "  ax.set_xticks(np.arange(len(second_model_components))+0.5, second_model_components, rotation=90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83RM6KIUyiLo"
      },
      "outputs": [],
      "source": [
        "cka_base_models = {\n",
        "    'base_unt': compute_model_layer_cka('base_unt', 'base_unt'),\n",
        "    'base_pret': compute_model_layer_cka('base_pret', 'base_pret'),\n",
        "    'base_ft': compute_model_layer_cka('base_ft', 'base_ft')\n",
        "}\n",
        "cka_large_models = {\n",
        "    'large_unt': compute_model_layer_cka('large_unt', 'large_unt'),\n",
        "    'large_pret': compute_model_layer_cka('large_pret', 'large_pret'),\n",
        "    'large_ft': compute_model_layer_cka('large_ft', 'large_ft')\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxp8MHwEy0YK"
      },
      "outputs": [],
      "source": [
        "# plot base models\n",
        "fig, axs = plt.subplots(1,3, figsize=(16,4))\n",
        "for i, (model_id, cka_df) in enumerate(cka_base_models.items()):\n",
        "  plot_model_layer_cka(cka_df, axs[i])\n",
        "  axs[i].set_title(model_id)\n",
        "plt.show()\n",
        "\n",
        "# plot large models\n",
        "fig, axs = plt.subplots(1,3, figsize=(16,4))\n",
        "for i, (model_id, cka_df) in enumerate(cka_large_models.items()):\n",
        "  plot_model_layer_cka(cka_df, axs[i])\n",
        "  axs[i].set_title(model_id)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js0tHPGCqWCf"
      },
      "source": [
        "## CKA similarity of Wav2Vec2 to MFCCs and GloVe\n",
        "\n",
        "Next to computing similarities within and across model representations, we can also use CKA to compute similarity between model embeddings and more interpretable features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHm4Wfg_5E9c"
      },
      "source": [
        "### Layerwise CKA similarity to interpretable features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch7DF3hqklgy"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='green'>**ToDo 18**</font>: Inspecting layerwise similarities to MFCCs and GloVe across models\n",
        "\n",
        "Run the code cells below to obtain layerwise CKA similarities between Wav2Vec2 representations and MFCC vs. GloVe features. How do these layerwise patterns differ between models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnZT5JEb5Ki8"
      },
      "outputs": [],
      "source": [
        "def compute_layerwise_feature_cka(model_id, features, frame_agg_fun=get_mean_frame_embs, gram_fun=gram_linear):\n",
        "  feature_cka = {\n",
        "      model_component: cka(gram_fun(frame_agg_fun(model_frame_states[model_id][model_component])),\n",
        "                           gram_fun(features))\n",
        "      for model_component in model_frame_states[model_id].keys()\n",
        "  }\n",
        "  return feature_cka\n",
        "\n",
        "def plot_layerwise_feature_cka(model_ids, layerwise_cka_sims, ax):\n",
        "  for model_id, layerwise_cka in zip(model_ids, layerwise_cka_sims):\n",
        "    ax.plot(list(layerwise_cka.values()), color=colors[model_id], linestyle=linestyles[model_id], label=model_id)\n",
        "  ax.set_xticks(range(len(layerwise_cka.keys())), list(layerwise_cka.keys()), rotation=90)\n",
        "  ax.legend(frameon=False)\n",
        "  ax.set_ylabel('CKA similarity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uBaxZtCE4on"
      },
      "outputs": [],
      "source": [
        "base_layerwise_MFCC_sims = [\n",
        "    compute_layerwise_feature_cka(model_id, avg_MFCCs)\n",
        "    for model_id in base_models\n",
        "    ]\n",
        "large_layerwise_MFCC_sims = [\n",
        "    compute_layerwise_feature_cka(model_id, avg_MFCCs)\n",
        "    for model_id in large_models\n",
        "]\n",
        "base_layerwise_GloVe_sims = [\n",
        "    compute_layerwise_feature_cka(model_id, glove_embs)\n",
        "    for model_id in base_models\n",
        "    ]\n",
        "large_layerwise_GloVe_sims = [\n",
        "    compute_layerwise_feature_cka(model_id, glove_embs)\n",
        "    for model_id in large_models\n",
        "]\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20,4))\n",
        "axs[0].set_title('Layerwise similarity to MFCCs (base models)')\n",
        "plot_layerwise_feature_cka(base_models, base_layerwise_MFCC_sims, ax=axs[0])\n",
        "axs[1].set_title('Layerwise similarity to MFCCs (large models)')\n",
        "plot_layerwise_feature_cka(large_models, large_layerwise_MFCC_sims, ax=axs[1])\n",
        "axs[2].set_title('Layerwise similarity to GloVe (base models)')\n",
        "plot_layerwise_feature_cka(base_models, base_layerwise_GloVe_sims, ax=axs[2])\n",
        "axs[3].set_title('Layerwise similarity to GloVe (large models)')\n",
        "plot_layerwise_feature_cka(large_models, large_layerwise_GloVe_sims, ax=axs[3])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSU8B4oskGsK"
      },
      "source": [
        "### Comparing mean and middle frame embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxlb4oKHlKWy"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='green'>**ToDo 19**</font>: Computing middle frame embeddings\n",
        "\n",
        "In their paper about word-level information encoded in self-supervised speech model representations, [Pasad et al. (2024)](https://doi.org/10.1162/tacl_a_00656) investigate, amongst other things, which frames within a word segment seem most informative. One remarkable result from this analysis is that the middle frame of the word segment often seems as informative on its own as the mean-pooled word representation averaging all frames within a word segment (see e.g. Figure 4). We will here try to replicate this analysis by comparing similarity to GloVe embeddings between the mean-pooled representations we used so far and the middle frame embeddings.  \n",
        "Complete the function below to return a matrix with the middle frame state vector for each word segment. Then examine the layerwise patterns of similarities to GloVe for mean and middle frame embeddings. Are they in line with the findings in Pasad et al.?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih1hn_g-F00r"
      },
      "outputs": [],
      "source": [
        "def get_middle_frame_embs(frame_states):\n",
        "  \"\"\"\n",
        "  :param frame_states:        list of frame states for audio segments, extracted from a single\n",
        "                              model component; i.e. list of N_segments elements, each a np.array\n",
        "                              of shape (1, N_frames, D_component), where D_component is 512 for\n",
        "                              CNN output and 768 for Transformer representations\n",
        "\n",
        "  :return middle_frame_embs:  np.array of shape (N_segments, D_component), containing the middle\n",
        "                              frame state for every segment\n",
        "  \"\"\"\n",
        "  raise NotImplementedError\n",
        "  middle_frame_indices = # YOUR CODE HERE\n",
        "  middle_frame_embs = # YOUR CODE HERE\n",
        "  return middle_frame_embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xubh2-keE2h-"
      },
      "outputs": [],
      "source": [
        "def plot_emb_comparison(model_id, features, ax):\n",
        "  mean_emb_cka_sims = compute_layerwise_feature_cka(model_id, features, frame_agg_fun=get_mean_frame_embs)\n",
        "  middle_emb_cka_sims = compute_layerwise_feature_cka(model_id, features, frame_agg_fun=get_middle_frame_embs)\n",
        "  ax.plot(list(mean_emb_cka_sims.values()), linestyle='solid', color='black', label='mean-pooled')\n",
        "  ax.plot(list(middle_emb_cka_sims.values()), linestyle='dashed', color='black', label='middle frame')\n",
        "  ax.set_ylabel('CKA similarity')\n",
        "  ax.set_xticks(range(len(mean_emb_cka_sims.keys())), list(mean_emb_cka_sims.keys()), rotation=90)\n",
        "\n",
        "fig, axs = plt.subplots(1, len(models), figsize=(28,4), sharey=True)\n",
        "for i, model_id in enumerate(models.keys()):\n",
        "  plot_emb_comparison(model_id, glove_embs, axs[i])\n",
        "  axs[i].set_title(model_id)\n",
        "  if i == 0:\n",
        "    axs[i].legend(frameon=False, loc='upper left')\n",
        "plt.suptitle('Layerwise similarity to GloVe embeddings', y=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw0P95rTE8Sy"
      },
      "source": [
        "##### <input type=\"checkbox\"/> <font color='red'>**ToSubmit 5**</font>: Interpreting differences between models with CKA\n",
        "\n",
        "Choose your favourite model comparison plot out of the ones generated above to include in your report.  \n",
        "Add a brief (max. 2 sentences) caption explaining what insights this analysis provides on differences in speech sound processing between the untrained, pretrained, and finetuned versions of the base and large Wav2Vec2 models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bkOaii7nSEj"
      },
      "source": [
        "# ðŸš€ Mini-project starting points\n",
        "\n",
        "This week you have learned about several techniques for analyzing the internals of Audio Transformer models. We hope you are excited to apply these and/or other techniques to further study speech and sound processing models for your mini-project! Below, we provide a few possible starting points for inspiration. Of course, you are also free to come up with your own ideas. But do confirm with your TA that they are realistic to pursue within the mini-project timeframe!\n",
        "\n",
        "**1. Temporal generalization and context-invariant phonetic encoding**  \n",
        "In week 1, you applied Temporal Generalization Matrices to compute the crossaccuracy of probes across different model layers. Temporal Generalization analysis has also been used to study the encoding of phoneme sequences across time in neural speech models and the human brain â€” see [Oli et al. (2024)](https://arxiv.org/pdf/2405.08237) for a study replicating neuroscience findings in a self-supervised RNN model. What is the window of phoneme decodability in Wav2Vec2, and how context-invariant are its phonetic encodings?\n",
        "\n",
        "**2. Effects of noise or speaker accent on model representations and transcription accuracy**  \n",
        "In the first notebook of this workshop, we saw that adding noise to the audio signal causes a decrease in Wav2Vec2's transcription accuracy (as measured by CER and WER). Similar performance decreases can be observed for other types of audio that the model wasn't exposed to during training, such as non-native accented speech or speech from other speaker minorities in the training dataset (e.g. women, children, elderly). It would be great if we could identify the Transformer components responsible for such biases, so that we might be able to mitigate them using targeted finetuning (as has been done for text models; e.g. [Chintam et al., 2023](https://aclanthology.org/2023.blackboxnlp-1.29/)). Possible steps towards this:\n",
        "- Can you localize which model components show most representational sensitivity to changes in noise / speaker identity / speaker accent using similarity-based interpretability techniques? (e.g. using differences in similarities between word representations for native- and non-native accented speech)\n",
        "- Can you localize which model components are causally involved in increases to WER/CER metrics observed for noisy audio / accented speakers etc.? (e.g. using causal interventions)\n",
        "\n",
        "**3. Language-specific phonology and perceptual narrowing**  \n",
        "Human speech perception is known to become tuned to language-specific experience over development: we start out being able to distinguish speech sounds from many different languages, but we become worse at distinguishing speech sounds from languages other than our native language over time. This phenomenon is known as [perceptual narrowing](https://en.wikipedia.org/wiki/Perceptual_narrowing#Phoneme_distinction).  \n",
        "Can you find evidence of language-specific speech perception in Wav2Vec2 models trained on different languages?  \n",
        "Relevant resources:\n",
        "- French [LeBenchmark](https://huggingface.co/LeBenchmark) set of Wav2Vec2 models trained on different amounts of French speech data\n",
        "- English [set of Wav2Vec2 checkpoints](https://huggingface.co/techsword/wav2vec2-base-english-librispeech730h-checkpoints/tree/main) over training released by [Shen et al. (2024)](https://aclanthology.org/2024.naacl-long.239/)\n",
        "- The [VoxAngeles](https://github.com/pacscilab/voxangeles) Corpus with phonetic transcriptions for 95 different languages, and the [CommonPhone](https://zenodo.org/records/5846137) dataset with phonetic transcriptions of a few more common languages\n",
        "- ABX tests of phone discrimination ability as used by [Millet & Dunbar (2022)](https://aclanthology.org/2022.acl-long.523/) and their [collection of stimuli](https://docs.cognitive-ml.fr/perceptimatic/perceptimatic_description.html) targeted at French vs. English speech sound contrasts"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
