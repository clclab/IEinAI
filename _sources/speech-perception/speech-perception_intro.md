# Speech Perception

Welcome to the Speech Perception workshop!

In the journal club, we will discuss the following papers:

Key Paper: 
- Pasad, A., Chien, C. M., Settle, S., & Livescu, K. (2024). What Do Self-Supervised Speech Models Know About Words? _Transactions of the Association for Computational Linguistics, 12_, 372-391. https://doi.org/10.1162/tacl_a_00656
  
For presentations, choose from:
- Mohebbi, H., Chrupała, G., Zuidema, W., & Alishahi, A. (2023). Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_ (pp. 8249-8260). https://doi.org/10.18653/v1/2023.emnlp-main.513
  
- Both of:
    - Shen, G., Alishahi, A., Bisazza, A., Chrupała, G. (2023) Wave to Syntax: Probing spoken language models for syntax. _Proc. INTERSPEECH 2023_, 1259-1263. https://doi.org/10.21437/Interspeech.2023-679
    - de Heer Kloots, M. & Zuidema, W. (2024). Human-like Linguistic Biases in Neural Speech Models: Phonetic Categorization and Phonotactic Constraints in Wav2Vec2.0. _Accepted to Proc. INTERSPEECH 2024_. 
    https://doi.org/10.48550/arXiv.2407.03005

During the computer labs, we will work through two notebooks — one on [probing acoustic, phonemic and orthographic information in Wav2Vec2](https://clclab.github.io/IEinAI/speech-perception/speech_perception_lab1_probing.html), and one on [comparing Audio Transformer representations with CKA](https://clclab.github.io/IEinAI/speech-perception/speech_perception_lab2_CKA.html).